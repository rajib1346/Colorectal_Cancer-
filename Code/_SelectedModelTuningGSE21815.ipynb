{"cells":[{"cell_type":"markdown","metadata":{"id":"b9Wed-y4skm5"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"J_lgF0vQzS1D","outputId":"bcb309da-5337-4606-8528-db524e73805e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-f3e4e18a-976e-459b-a0fa-cee14c3498d9\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f3e4e18a-976e-459b-a0fa-cee14c3498d9\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ML_DEG_dataset.csv to ML_DEG_dataset.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # Manually upload the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"r2OFHcYKBIrH","outputId":"0a17937d-4e88-4b9c-8a44-0cfd1571b0e2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-33a4867c-6718-4c62-bd2b-c8bd8cc7b559\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-33a4867c-6718-4c62-bd2b-c8bd8cc7b559\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving GSE21815_gene_expression_renamed.csv to GSE21815_gene_expression_renamed.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # Manually upload the file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eL6e3aFO13-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aacjzmgj7eez","outputId":"ab1a7c55-babb-4e43-e759-0e72b3bff753"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"markdown","metadata":{"id":"__TcBLoYhShL"},"source":["# Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"0_TYrCmtoqQm"},"source":["# MI Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"GGMZm0mpBymN"},"source":["## Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpYoE9zJA1HI","outputId":"f5fed2ed-2516-466c-e47e-b33990aa9ed1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 300}\n","\n","Extra Trees Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 0.1, 'n_estimators': 200}\n","Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 200}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 300}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['ADH1B', 'CDH3', 'GUCA2B', 'PDE9A', 'ABCA8', 'GUCA2A', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"o47_gTDJCIe3"},"source":["# Random Search CV\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZwC5eOuCF7d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"45bdf115-e613-4b67-bd8b-c70e7185513d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","\n","Logistic Regression Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n","\n","Random Forest Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 7, 'learning_rate': np.float64(0.105)}\n","\n","Gradient Boosting Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(0.2575)}\n","\n","AdaBoost Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 150, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.75), 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 7, 'learning_rate': np.float64(0.105)}\n","AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(0.2575)}\n","Bagging: {'n_estimators': 150, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.75), 'bootstrap': True}\n","Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['ADH1B', 'CDH3', 'GUCA2B', 'PDE9A', 'ABCA8', 'GUCA2A', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"t3mUPawDtFh-"},"source":["# Tuning for Hub genes"]},{"cell_type":"markdown","metadata":{"id":"On-eVOfNH2Mq"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3x_YW_pdH1Zz"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n","                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n","                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"iSMjPZQEIE8Y"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llUEZucpIIPY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n","                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n","                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"p2z4CNu14LfE"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC8LQrRG8AdB"},"outputs":[],"source":["['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esk6-wyL8UJX"},"outputs":[],"source":["# Grid Search CV Tuning for Pearson Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeLialgiJQfK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"19geX5E_fES3"},"source":["# PearsonCorrelation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKwT7Z0hJiGg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3eeb996-0e80-4ae5-df57-0c97ba9b94c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 95.04%\n","Precision: 0.97\n","Recall: 0.95\n","F1-Score: 0.96\n","MCC: 0.73\n","Confusion Matrix:\n","[[  9   0]\n"," [  7 125]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.56      1.00      0.72         9\n","           1       1.00      0.95      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.78      0.97      0.85       141\n","weighted avg       0.97      0.95      0.96       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 53.90%\n","Precision: 0.94\n","Recall: 0.54\n","F1-Score: 0.64\n","MCC: 0.25\n","Confusion Matrix:\n","[[ 9  0]\n"," [65 67]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.12      1.00      0.22         9\n","           1       1.00      0.51      0.67       132\n","\n","    accuracy                           0.54       141\n","   macro avg       0.56      0.75      0.45       141\n","weighted avg       0.94      0.54      0.64       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 93.62%\n","Precision: 0.93\n","Recall: 0.94\n","F1-Score: 0.93\n","MCC: 0.38\n","Confusion Matrix:\n","[[  3   6]\n"," [  3 129]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.33      0.40         9\n","           1       0.96      0.98      0.97       132\n","\n","    accuracy                           0.94       141\n","   macro avg       0.73      0.66      0.68       141\n","weighted avg       0.93      0.94      0.93       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n","\n","AdaBoost Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.76\n","Confusion Matrix:\n","[[  7   2]\n"," [  2 130]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.78      0.78         9\n","           1       0.98      0.98      0.98       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.88      0.88      0.88       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\n","\n","Bagging Testing Accuracy: 95.04%\n","Precision: 0.94\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.48\n","Confusion Matrix:\n","[[  3   6]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.33      0.46         9\n","           1       0.96      0.99      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.85      0.66      0.72       141\n","weighted avg       0.94      0.95      0.94       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 99.29%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.94\n","Confusion Matrix:\n","[[  8   1]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.89      0.94         9\n","           1       0.99      1.00      1.00       132\n","\n","    accuracy                           0.99       141\n","   macro avg       1.00      0.94      0.97       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n","Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n","Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\n","Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"5A0a9nKfJuOh"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocACFm9GJxiw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"32eff069-825d-4993-ebeb-13626240dbfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","\n","Logistic Regression Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.97\n","MCC: 0.75\n","Confusion Matrix:\n","[[  8   1]\n"," [  4 128]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.89      0.76         9\n","           1       0.99      0.97      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.83      0.93      0.87       141\n","weighted avg       0.97      0.96      0.97       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 4, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 53.19%\n","Precision: 0.94\n","Recall: 0.53\n","F1-Score: 0.64\n","MCC: 0.24\n","Confusion Matrix:\n","[[ 9  0]\n"," [66 66]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.12      1.00      0.21         9\n","           1       1.00      0.50      0.67       132\n","\n","    accuracy                           0.53       141\n","   macro avg       0.56      0.75      0.44       141\n","weighted avg       0.94      0.53      0.64       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n","\n","Gradient Boosting Testing Accuracy: 93.62%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.49\n","Confusion Matrix:\n","[[  5   4]\n"," [  5 127]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.56      0.53         9\n","           1       0.97      0.96      0.97       132\n","\n","    accuracy                           0.94       141\n","   macro avg       0.73      0.76      0.75       141\n","weighted avg       0.94      0.94      0.94       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(0.505)}\n","\n","AdaBoost Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.68\n","Confusion Matrix:\n","[[  7   2]\n"," [  4 128]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.64      0.78      0.70         9\n","           1       0.98      0.97      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.81      0.87      0.84       141\n","weighted avg       0.96      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 150, 'max_samples': np.float64(0.75), 'max_features': np.float64(0.875), 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 95.04%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.46\n","Confusion Matrix:\n","[[  2   7]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36         9\n","           1       0.95      1.00      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.97      0.61      0.67       141\n","weighted avg       0.95      0.95      0.94       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 4, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n","AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(0.505)}\n","Bagging: {'n_estimators': 150, 'max_samples': np.float64(0.75), 'max_features': np.float64(0.875), 'bootstrap': False}\n","Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9ZofIrx9od7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wvh8hLzfLpAN"},"source":["# Lasso Regression Tuning"]},{"cell_type":"markdown","metadata":{"id":"msVjPMdWKHvh"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5s0I3FFKKhi","outputId":"8641893e-e445-41b3-9f88-be8df018f037"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  7   2]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.78      0.82         9\n","           1       0.98      0.99      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.93      0.89      0.91       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 73.76%\n","Precision: 0.95\n","Recall: 0.74\n","F1-Score: 0.80\n","MCC: 0.38\n","Confusion Matrix:\n","[[ 9  0]\n"," [37 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.20      1.00      0.33         9\n","           1       1.00      0.72      0.84       132\n","\n","    accuracy                           0.74       141\n","   macro avg       0.60      0.86      0.58       141\n","weighted avg       0.95      0.74      0.80       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.74\n","Confusion Matrix:\n","[[  6   3]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.67      0.75         9\n","           1       0.98      0.99      0.98       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.92      0.83      0.87       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n","\n","Extra Trees Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n","Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 200}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['NR3C2', 'BRCA2', 'AFF3', 'LGALS2', 'TMEM100', 'CDH3', 'ABCG2', 'STMN2', 'ABCA8', 'AZGP1', 'TSPAN7', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"9PR0uZu1KdTM"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTE_l8AqKgQI","outputId":"3922169c-cb7f-4422-9cc5-cb32a29f865e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.01)}\n","\n","Logistic Regression Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  7   2]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.78      0.82         9\n","           1       0.98      0.99      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.93      0.89      0.91       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 73.76%\n","Precision: 0.95\n","Recall: 0.74\n","F1-Score: 0.80\n","MCC: 0.38\n","Confusion Matrix:\n","[[ 9  0]\n"," [37 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.20      1.00      0.33         9\n","           1       1.00      0.72      0.84       132\n","\n","    accuracy                           0.74       141\n","   macro avg       0.60      0.86      0.58       141\n","weighted avg       0.95      0.74      0.80       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n","\n","Random Forest Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.1}\n","\n","Gradient Boosting Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': 1}\n","\n","AdaBoost Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.74\n","Confusion Matrix:\n","[[  6   3]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.67      0.75         9\n","           1       0.98      0.99      0.98       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.92      0.83      0.87       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 100, 'max_samples': 0.75, 'max_features': 0.5, 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}\n","\n","Extra Trees Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.01)}\n","K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.1}\n","AdaBoost: {'n_estimators': 200, 'learning_rate': 1}\n","Bagging: {'n_estimators': 100, 'max_samples': 0.75, 'max_features': 0.5, 'bootstrap': True}\n","Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest =['NR3C2', 'BRCA2', 'AFF3', 'LGALS2', 'TMEM100', 'CDH3', 'ABCG2', 'STMN2', 'ABCA8', 'AZGP1', 'TSPAN7', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy')\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"-mNcsm1UOmIy"},"source":["# mRMR Feature Selection Tuning\n"]},{"cell_type":"markdown","metadata":{"id":"4Llii6ZcK7pw"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rsm2PWBgKyH-","outputId":"e2d2e65a-47f7-4dba-f959-7bee5fab3cbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.86\n","Confusion Matrix:\n","[[  9   0]\n"," [  3 129]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      1.00      0.86         9\n","           1       1.00      0.98      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.88      0.99      0.92       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.88\n","Confusion Matrix:\n","[[  8   1]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.89      0.89         9\n","           1       0.99      0.99      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.94      0.94      0.94       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 95.04%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.46\n","Confusion Matrix:\n","[[  2   7]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36         9\n","           1       0.95      1.00      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.97      0.61      0.67       141\n","weighted avg       0.95      0.95      0.94       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 97.16%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.73\n","Confusion Matrix:\n","[[  5   4]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.56      0.71         9\n","           1       0.97      1.00      0.99       132\n","\n","    accuracy                           0.97       141\n","   macro avg       0.99      0.78      0.85       141\n","weighted avg       0.97      0.97      0.97       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n","\n","AdaBoost Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\n","\n","Bagging Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n","\n","Extra Trees Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n","Random Forest: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n","Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n","XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['CDH3', 'GNG7', 'GDPD3', 'MMP1', 'ABCA8', 'VSNL1', 'ADH1B', 'AZGP1', 'LGALS2', 'TMEM100', 'DPEP1',\n","                       'AFF3', 'BRCA2', 'MS4A12', 'CENPA', 'LIFR', 'CEP72', 'ABCG2', 'ABI3BP', 'COL10A1', 'MT1M', 'MAD2L1',\n","                       'STMN2', 'NFE2L3', 'SLC4A4', 'KRT23', 'PRR7', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"fbobSA-eLGwl"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grBfkHRHLKAY","outputId":"1aa85f0d-56f4-4124-bf61-0643feaaf9ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.027825594022071243)}\n","\n","Logistic Regression Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.86\n","Confusion Matrix:\n","[[  9   0]\n"," [  3 129]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      1.00      0.86         9\n","           1       1.00      0.98      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.88      0.99      0.92       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.88\n","Confusion Matrix:\n","[[  8   1]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.89      0.89         9\n","           1       0.99      0.99      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.94      0.94      0.94       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n","\n","Random Forest Testing Accuracy: 95.04%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.46\n","Confusion Matrix:\n","[[  2   7]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36         9\n","           1       0.95      1.00      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.97      0.61      0.67       141\n","weighted avg       0.95      0.95      0.94       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.2}\n","\n","Gradient Boosting Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 100, 'learning_rate': 0.1}\n","\n","AdaBoost Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.98\n","MCC: 0.88\n","Confusion Matrix:\n","[[  7   2]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.78      0.88         9\n","           1       0.99      1.00      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.99      0.89      0.93       141\n","weighted avg       0.99      0.99      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 50, 'max_samples': 0.75, 'max_features': 0.75, 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Extra Trees Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n","\n","XGBoost Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.027825594022071243)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.2}\n","AdaBoost: {'n_estimators': 100, 'learning_rate': 0.1}\n","Bagging: {'n_estimators': 50, 'max_samples': 0.75, 'max_features': 0.75, 'bootstrap': False}\n","Extra Trees: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['CDH3', 'GNG7', 'GDPD3', 'MMP1', 'ABCA8', 'VSNL1', 'ADH1B', 'AZGP1', 'LGALS2', 'TMEM100', 'DPEP1',\n","                       'AFF3', 'BRCA2', 'MS4A12', 'CENPA', 'LIFR', 'CEP72', 'ABCG2', 'ABI3BP', 'COL10A1', 'MT1M', 'MAD2L1',\n","                       'STMN2', 'NFE2L3', 'SLC4A4', 'KRT23', 'PRR7', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'CatBoost': {\n","        'iterations': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'depth': [4, 6, 8],\n","        'l2_leaf_reg': [3, 5, 7]\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy')\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix2WJa7FfAgG"},"outputs":[],"source":["# GRid Search CV for 15 Dataset"]},{"cell_type":"markdown","metadata":{"id":"ZACfMYOlj2Pr"},"source":["# Elastic Net Feature Selection Tuning"]},{"cell_type":"markdown","metadata":{"id":"p6PWrX_bMkJY"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVGl5Ew1MoIr","outputId":"446da112-3aa7-4a09-d1cd-fd8fc0f8b8c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.88\n","Confusion Matrix:\n","[[  8   1]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.89      0.89         9\n","           1       0.99      0.99      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.94      0.94      0.94       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.90\n","Confusion Matrix:\n","[[  9   0]\n"," [  2 130]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      1.00      0.90         9\n","           1       1.00      0.98      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.91      0.99      0.95       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 95.04%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.46\n","Confusion Matrix:\n","[[  2   7]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36         9\n","           1       0.95      1.00      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.97      0.61      0.67       141\n","weighted avg       0.95      0.95      0.94       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 50}\n","\n","AdaBoost Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 100}\n","\n","Bagging Testing Accuracy: 95.04%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.46\n","Confusion Matrix:\n","[[  2   7]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36         9\n","           1       0.95      1.00      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.97      0.61      0.67       141\n","weighted avg       0.95      0.95      0.94       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.88\n","Confusion Matrix:\n","[[  8   1]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.89      0.89         9\n","           1       0.99      0.99      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.94      0.94      0.94       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n","Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 50}\n","Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 100}\n","Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = [ 'NEBL.1', 'CDH3', 'LGI1', 'BRCA2', 'PTGDS', 'ASPA', 'ABCG2', 'SST', 'TNFRSF11B', 'SLC4A4', 'TAF1A', 'SPINK2', 'PTGDR', 'AFF3', 'DHRS9', 'TG', 'MMP12', 'ADH1B', 'IL23A', 'ACSL6', 'TSPAN7', 'MMP1', 'PDE9A', 'SPINK5', 'CDKN2A','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"N0FQoca8MxEl"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGHnKrB2L5A5","outputId":"beeef356-212e-414f-ee88-c2195b3b2514"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewqHYkoBMzyj","outputId":"8a80605a-1f6d-4df6-bb95-9bd9557c2097"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(0.01)}\n","\n","Logistic Regression Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.88\n","Confusion Matrix:\n","[[  8   1]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.89      0.89         9\n","           1       0.99      0.99      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.94      0.94      0.94       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  7   2]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.78      0.82         9\n","           1       0.98      0.99      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.93      0.89      0.91       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 96.45%\n","Precision: 0.97\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.65\n","Confusion Matrix:\n","[[  4   5]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.44      0.62         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.72      0.80       141\n","weighted avg       0.97      0.96      0.96       141\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n","\n","Gradient Boosting Testing Accuracy: 95.74%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.95\n","MCC: 0.56\n","Confusion Matrix:\n","[[  3   6]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.33      0.50         9\n","           1       0.96      1.00      0.98       132\n","\n","    accuracy                           0.96       141\n","   macro avg       0.98      0.67      0.74       141\n","weighted avg       0.96      0.96      0.95       141\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 50, 'learning_rate': 1}\n","\n","AdaBoost Testing Accuracy: 97.87%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.81\n","Confusion Matrix:\n","[[  6   3]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.67      0.80         9\n","           1       0.98      1.00      0.99       132\n","\n","    accuracy                           0.98       141\n","   macro avg       0.99      0.83      0.89       141\n","weighted avg       0.98      0.98      0.98       141\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 50, 'max_samples': 0.75, 'max_features': 1.0, 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 95.04%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.94\n","MCC: 0.46\n","Confusion Matrix:\n","[[  2   7]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36         9\n","           1       0.95      1.00      0.97       132\n","\n","    accuracy                           0.95       141\n","   macro avg       0.97      0.61      0.67       141\n","weighted avg       0.95      0.95      0.94       141\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 98.58%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.88\n","Confusion Matrix:\n","[[  8   1]\n"," [  1 131]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.89      0.89         9\n","           1       0.99      0.99      0.99       132\n","\n","    accuracy                           0.99       141\n","   macro avg       0.94      0.94      0.94       141\n","weighted avg       0.99      0.99      0.99       141\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 94.33%\n","Precision: 0.95\n","Recall: 0.94\n","F1-Score: 0.92\n","MCC: 0.32\n","Confusion Matrix:\n","[[  1   8]\n"," [  0 132]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.11      0.20         9\n","           1       0.94      1.00      0.97       132\n","\n","    accuracy                           0.94       141\n","   macro avg       0.97      0.56      0.59       141\n","weighted avg       0.95      0.94      0.92       141\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(0.01)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n","AdaBoost: {'n_estimators': 50, 'learning_rate': 1}\n","Bagging: {'n_estimators': 50, 'max_samples': 0.75, 'max_features': 1.0, 'bootstrap': True}\n","Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n","from xgboost import XGBClassifier\n","\n","\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = [ 'NEBL.1', 'CDH3', 'LGI1', 'BRCA2', 'PTGDS', 'ASPA', 'ABCG2', 'SST', 'TNFRSF11B', 'SLC4A4', 'TAF1A', 'SPINK2', 'PTGDR', 'AFF3', 'DHRS9', 'TG', 'MMP12', 'ADH1B', 'IL23A', 'ACSL6', 'TSPAN7', 'MMP1', 'PDE9A', 'SPINK5', 'CDKN2A','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy')\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"5rxf_fX2mOoT"},"source":["# Grid Search CV for 15 Dataset"]},{"cell_type":"markdown","metadata":{"id":"k8f8C5LkmVZE"},"source":["# Random Search CV for 15 Dataset"]},{"cell_type":"markdown","metadata":{"id":"DPxy7NcGmkp8"},"source":["# Grid Search CV for 82 Dataset"]},{"cell_type":"markdown","metadata":{"id":"oCQ2JEhjmx0H"},"source":["# Random Search CV for 82 Dataset"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}