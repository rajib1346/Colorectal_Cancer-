{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GSE106582"
      ],
      "metadata": {
        "id": "IeAhQKN7YdAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815"
      ],
      "metadata": {
        "id": "grjcAmeFY83U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076"
      ],
      "metadata": {
        "id": "wgRzmHjlZrAD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipA6kDmcZbxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671"
      ],
      "metadata": {
        "id": "l_JPA_FHZceG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Tuning"
      ],
      "metadata": {
        "id": "mv1tPRdxZ3Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Serach CV GSE106582"
      ],
      "metadata": {
        "id": "n61h2KX1aff8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2',\n",
        "                       'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2', 'PPP3CB', 'PTRF', 'ITPR1', 'PALLD',\n",
        "                       'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zLpq9kuZ53D",
        "outputId": "b67068eb-079c-4f44-c3e2-bd19bad4db44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 62.89%\n",
            "Precision: 0.77\n",
            "Recall: 0.63\n",
            "F1-Score: 0.51\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[117   0]\n",
            " [ 72   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.76       117\n",
            "           1       1.00      0.06      0.12        77\n",
            "\n",
            "    accuracy                           0.63       194\n",
            "   macro avg       0.81      0.53      0.44       194\n",
            "weighted avg       0.77      0.63      0.51       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 48.97%\n",
            "Precision: 0.68\n",
            "Recall: 0.49\n",
            "F1-Score: 0.42\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[22 95]\n",
            " [ 4 73]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.19      0.31       117\n",
            "           1       0.43      0.95      0.60        77\n",
            "\n",
            "    accuracy                           0.49       194\n",
            "   macro avg       0.64      0.57      0.45       194\n",
            "weighted avg       0.68      0.49      0.42       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 82.47%\n",
            "Precision: 0.84\n",
            "Recall: 0.82\n",
            "F1-Score: 0.82\n",
            "MCC: 0.64\n",
            "Confusion Matrix:\n",
            "[[113   4]\n",
            " [ 30  47]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.97      0.87       117\n",
            "           1       0.92      0.61      0.73        77\n",
            "\n",
            "    accuracy                           0.82       194\n",
            "   macro avg       0.86      0.79      0.80       194\n",
            "weighted avg       0.84      0.82      0.82       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 75.77%\n",
            "Precision: 0.82\n",
            "Recall: 0.76\n",
            "F1-Score: 0.76\n",
            "MCC: 0.57\n",
            "Confusion Matrix:\n",
            "[[75 42]\n",
            " [ 5 72]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.64      0.76       117\n",
            "           1       0.63      0.94      0.75        77\n",
            "\n",
            "    accuracy                           0.76       194\n",
            "   macro avg       0.78      0.79      0.76       194\n",
            "weighted avg       0.82      0.76      0.76       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 78.87%\n",
            "Precision: 0.83\n",
            "Recall: 0.79\n",
            "F1-Score: 0.77\n",
            "MCC: 0.58\n",
            "Confusion Matrix:\n",
            "[[115   2]\n",
            " [ 39  38]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.98      0.85       117\n",
            "           1       0.95      0.49      0.65        77\n",
            "\n",
            "    accuracy                           0.79       194\n",
            "   macro avg       0.85      0.74      0.75       194\n",
            "weighted avg       0.83      0.79      0.77       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 50}\n",
            "\n",
            "Bagging Testing Accuracy: 70.10%\n",
            "Precision: 0.70\n",
            "Recall: 0.70\n",
            "F1-Score: 0.70\n",
            "MCC: 0.38\n",
            "Confusion Matrix:\n",
            "[[86 31]\n",
            " [27 50]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75       117\n",
            "           1       0.62      0.65      0.63        77\n",
            "\n",
            "    accuracy                           0.70       194\n",
            "   macro avg       0.69      0.69      0.69       194\n",
            "weighted avg       0.70      0.70      0.70       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Extra Trees Testing Accuracy: 53.61%\n",
            "Precision: 0.75\n",
            "Recall: 0.54\n",
            "F1-Score: 0.48\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[29 88]\n",
            " [ 2 75]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.25      0.39       117\n",
            "           1       0.46      0.97      0.62        77\n",
            "\n",
            "    accuracy                           0.54       194\n",
            "   macro avg       0.70      0.61      0.51       194\n",
            "weighted avg       0.75      0.54      0.48       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 74.23%\n",
            "Precision: 0.78\n",
            "Recall: 0.74\n",
            "F1-Score: 0.74\n",
            "MCC: 0.51\n",
            "Confusion Matrix:\n",
            "[[78 39]\n",
            " [11 66]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.67      0.76       117\n",
            "           1       0.63      0.86      0.73        77\n",
            "\n",
            "    accuracy                           0.74       194\n",
            "   macro avg       0.75      0.76      0.74       194\n",
            "weighted avg       0.78      0.74      0.74       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 50}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Search CV GSE106582"
      ],
      "metadata": {
        "id": "sihKR_2TaZTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2',\n",
        "                       'PPP3CB', 'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1',\n",
        "                       'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_fQ-5TjaSX-",
        "outputId": "ec0d6674-9fc3-48b4-de25-56c9e068659f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 62.89%\n",
            "Precision: 0.77\n",
            "Recall: 0.63\n",
            "F1-Score: 0.51\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[117   0]\n",
            " [ 72   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.76       117\n",
            "           1       1.00      0.06      0.12        77\n",
            "\n",
            "    accuracy                           0.63       194\n",
            "   macro avg       0.81      0.53      0.44       194\n",
            "weighted avg       0.77      0.63      0.51       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 48.97%\n",
            "Precision: 0.68\n",
            "Recall: 0.49\n",
            "F1-Score: 0.42\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[22 95]\n",
            " [ 4 73]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.19      0.31       117\n",
            "           1       0.43      0.95      0.60        77\n",
            "\n",
            "    accuracy                           0.49       194\n",
            "   macro avg       0.64      0.57      0.45       194\n",
            "weighted avg       0.68      0.49      0.42       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
            "\n",
            "Random Forest Testing Accuracy: 77.32%\n",
            "Precision: 0.78\n",
            "Recall: 0.77\n",
            "F1-Score: 0.78\n",
            "MCC: 0.54\n",
            "Confusion Matrix:\n",
            "[[89 28]\n",
            " [16 61]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.76      0.80       117\n",
            "           1       0.69      0.79      0.73        77\n",
            "\n",
            "    accuracy                           0.77       194\n",
            "   macro avg       0.77      0.78      0.77       194\n",
            "weighted avg       0.78      0.77      0.78       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 88.66%\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.76\n",
            "Confusion Matrix:\n",
            "[[108   9]\n",
            " [ 13  64]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.91       117\n",
            "           1       0.88      0.83      0.85        77\n",
            "\n",
            "    accuracy                           0.89       194\n",
            "   macro avg       0.88      0.88      0.88       194\n",
            "weighted avg       0.89      0.89      0.89       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 150, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 77.84%\n",
            "Precision: 0.82\n",
            "Recall: 0.78\n",
            "F1-Score: 0.76\n",
            "MCC: 0.56\n",
            "Confusion Matrix:\n",
            "[[115   2]\n",
            " [ 41  36]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.98      0.84       117\n",
            "           1       0.95      0.47      0.63        77\n",
            "\n",
            "    accuracy                           0.78       194\n",
            "   macro avg       0.84      0.73      0.73       194\n",
            "weighted avg       0.82      0.78      0.76       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.875), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 63.92%\n",
            "Precision: 0.74\n",
            "Recall: 0.64\n",
            "F1-Score: 0.63\n",
            "MCC: 0.39\n",
            "Confusion Matrix:\n",
            "[[54 63]\n",
            " [ 7 70]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.46      0.61       117\n",
            "           1       0.53      0.91      0.67        77\n",
            "\n",
            "    accuracy                           0.64       194\n",
            "   macro avg       0.71      0.69      0.64       194\n",
            "weighted avg       0.74      0.64      0.63       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Extra Trees Testing Accuracy: 52.06%\n",
            "Precision: 0.72\n",
            "Recall: 0.52\n",
            "F1-Score: 0.47\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[27 90]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.23      0.37       117\n",
            "           1       0.45      0.96      0.61        77\n",
            "\n",
            "    accuracy                           0.52       194\n",
            "   macro avg       0.68      0.60      0.49       194\n",
            "weighted avg       0.72      0.52      0.47       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 59.79%\n",
            "Precision: 0.70\n",
            "Recall: 0.60\n",
            "F1-Score: 0.59\n",
            "MCC: 0.31\n",
            "Confusion Matrix:\n",
            "[[49 68]\n",
            " [10 67]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.42      0.56       117\n",
            "           1       0.50      0.87      0.63        77\n",
            "\n",
            "    accuracy                           0.60       194\n",
            "   macro avg       0.66      0.64      0.59       194\n",
            "weighted avg       0.70      0.60      0.59       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 150, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 50, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.875), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Grid Search CV"
      ],
      "metadata": {
        "id": "xHOEnOf3aznk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2', 'PPP3CB',\n",
        "                       'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k3URklZan5c",
        "outputId": "a84672c1-39a3-4d3e-e64d-738e44f280bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 96.45%\n",
            "Precision: 0.96\n",
            "Recall: 0.96\n",
            "F1-Score: 0.96\n",
            "MCC: 0.66\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [  1 131]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.56      0.67         9\n",
            "           1       0.97      0.99      0.98       132\n",
            "\n",
            "    accuracy                           0.96       141\n",
            "   macro avg       0.90      0.77      0.82       141\n",
            "weighted avg       0.96      0.96      0.96       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 63.83%\n",
            "Precision: 0.91\n",
            "Recall: 0.64\n",
            "F1-Score: 0.73\n",
            "MCC: 0.15\n",
            "Confusion Matrix:\n",
            "[[ 6  3]\n",
            " [48 84]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.67      0.19         9\n",
            "           1       0.97      0.64      0.77       132\n",
            "\n",
            "    accuracy                           0.64       141\n",
            "   macro avg       0.54      0.65      0.48       141\n",
            "weighted avg       0.91      0.64      0.73       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Random Forest Testing Accuracy: 57.45%\n",
            "Precision: 0.91\n",
            "Recall: 0.57\n",
            "F1-Score: 0.68\n",
            "MCC: 0.12\n",
            "Confusion Matrix:\n",
            "[[ 6  3]\n",
            " [57 75]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.67      0.17         9\n",
            "           1       0.96      0.57      0.71       132\n",
            "\n",
            "    accuracy                           0.57       141\n",
            "   macro avg       0.53      0.62      0.44       141\n",
            "weighted avg       0.91      0.57      0.68       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 87.23%\n",
            "Precision: 0.93\n",
            "Recall: 0.87\n",
            "F1-Score: 0.89\n",
            "MCC: 0.38\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 15 117]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.67      0.40         9\n",
            "           1       0.97      0.89      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.63      0.78      0.66       141\n",
            "weighted avg       0.93      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 75.18%\n",
            "Precision: 0.92\n",
            "Recall: 0.75\n",
            "F1-Score: 0.81\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 32 100]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      0.67      0.26         9\n",
            "           1       0.97      0.76      0.85       132\n",
            "\n",
            "    accuracy                           0.75       141\n",
            "   macro avg       0.56      0.71      0.55       141\n",
            "weighted avg       0.92      0.75      0.81       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 91.49%\n",
            "Precision: 0.94\n",
            "Recall: 0.91\n",
            "F1-Score: 0.92\n",
            "MCC: 0.47\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [  9 123]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.67      0.50         9\n",
            "           1       0.98      0.93      0.95       132\n",
            "\n",
            "    accuracy                           0.91       141\n",
            "   macro avg       0.69      0.80      0.73       141\n",
            "weighted avg       0.94      0.91      0.92       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Extra Trees Testing Accuracy: 89.36%\n",
            "Precision: 0.93\n",
            "Recall: 0.89\n",
            "F1-Score: 0.91\n",
            "MCC: 0.42\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 12 120]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.67      0.44         9\n",
            "           1       0.98      0.91      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.65      0.79      0.69       141\n",
            "weighted avg       0.93      0.89      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 78.01%\n",
            "Precision: 0.92\n",
            "Recall: 0.78\n",
            "F1-Score: 0.83\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 28 104]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.67      0.28         9\n",
            "           1       0.97      0.79      0.87       132\n",
            "\n",
            "    accuracy                           0.78       141\n",
            "   macro avg       0.57      0.73      0.57       141\n",
            "weighted avg       0.92      0.78      0.83       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Random SearchCV"
      ],
      "metadata": {
        "id": "SG4ZHqbubCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2', 'PPP3CB',\n",
        "                       'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "IMDhs1_9aoDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32281010-7e2e-4ff8-9b01-7cada982782b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 96.45%\n",
            "Precision: 0.96\n",
            "Recall: 0.96\n",
            "F1-Score: 0.96\n",
            "MCC: 0.66\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [  1 131]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.56      0.67         9\n",
            "           1       0.97      0.99      0.98       132\n",
            "\n",
            "    accuracy                           0.96       141\n",
            "   macro avg       0.90      0.77      0.82       141\n",
            "weighted avg       0.96      0.96      0.96       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 82.98%\n",
            "Precision: 0.92\n",
            "Recall: 0.83\n",
            "F1-Score: 0.86\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 20 112]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.56      0.29         9\n",
            "           1       0.97      0.85      0.90       132\n",
            "\n",
            "    accuracy                           0.83       141\n",
            "   macro avg       0.58      0.70      0.60       141\n",
            "weighted avg       0.92      0.83      0.86       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Random Forest Testing Accuracy: 85.82%\n",
            "Precision: 0.93\n",
            "Recall: 0.86\n",
            "F1-Score: 0.89\n",
            "MCC: 0.36\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.67      0.38         9\n",
            "           1       0.97      0.87      0.92       132\n",
            "\n",
            "    accuracy                           0.86       141\n",
            "   macro avg       0.62      0.77      0.65       141\n",
            "weighted avg       0.93      0.86      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 81.56%\n",
            "Precision: 0.92\n",
            "Recall: 0.82\n",
            "F1-Score: 0.86\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 23 109]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.67      0.32         9\n",
            "           1       0.97      0.83      0.89       132\n",
            "\n",
            "    accuracy                           0.82       141\n",
            "   macro avg       0.59      0.75      0.60       141\n",
            "weighted avg       0.92      0.82      0.86       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 150, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 72.34%\n",
            "Precision: 0.92\n",
            "Recall: 0.72\n",
            "F1-Score: 0.79\n",
            "MCC: 0.21\n",
            "Confusion Matrix:\n",
            "[[ 6  3]\n",
            " [36 96]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.67      0.24         9\n",
            "           1       0.97      0.73      0.83       132\n",
            "\n",
            "    accuracy                           0.72       141\n",
            "   macro avg       0.56      0.70      0.53       141\n",
            "weighted avg       0.92      0.72      0.79       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 100, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.625), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 81.56%\n",
            "Precision: 0.92\n",
            "Recall: 0.82\n",
            "F1-Score: 0.86\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 23 109]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.67      0.32         9\n",
            "           1       0.97      0.83      0.89       132\n",
            "\n",
            "    accuracy                           0.82       141\n",
            "   macro avg       0.59      0.75      0.60       141\n",
            "weighted avg       0.92      0.82      0.86       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 91.49%\n",
            "Precision: 0.94\n",
            "Recall: 0.91\n",
            "F1-Score: 0.92\n",
            "MCC: 0.47\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [  9 123]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.67      0.50         9\n",
            "           1       0.98      0.93      0.95       132\n",
            "\n",
            "    accuracy                           0.91       141\n",
            "   macro avg       0.69      0.80      0.73       141\n",
            "weighted avg       0.94      0.91      0.92       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 70.92%\n",
            "Precision: 0.92\n",
            "Recall: 0.71\n",
            "F1-Score: 0.78\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[ 6  3]\n",
            " [38 94]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.67      0.23         9\n",
            "           1       0.97      0.71      0.82       132\n",
            "\n",
            "    accuracy                           0.71       141\n",
            "   macro avg       0.55      0.69      0.52       141\n",
            "weighted avg       0.92      0.71      0.78       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "AdaBoost: {'n_estimators': 150, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 100, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.625), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Grid Serach CV"
      ],
      "metadata": {
        "id": "xUgGf7Ebc0Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2', 'PPP3CB',\n",
        "                       'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "wk70MoTBaoL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69543ff-f798-4da9-fb8b-4f5b8825ce7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 79.05%\n",
            "Precision: 0.79\n",
            "Recall: 0.79\n",
            "F1-Score: 0.78\n",
            "MCC: 0.52\n",
            "Confusion Matrix:\n",
            "[[30 20]\n",
            " [11 87]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.60      0.66        50\n",
            "           1       0.81      0.89      0.85        98\n",
            "\n",
            "    accuracy                           0.79       148\n",
            "   macro avg       0.77      0.74      0.75       148\n",
            "weighted avg       0.79      0.79      0.78       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 66.22%\n",
            "Precision: 0.44\n",
            "Recall: 0.66\n",
            "F1-Score: 0.53\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[ 0 50]\n",
            " [ 0 98]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        50\n",
            "           1       0.66      1.00      0.80        98\n",
            "\n",
            "    accuracy                           0.66       148\n",
            "   macro avg       0.33      0.50      0.40       148\n",
            "weighted avg       0.44      0.66      0.53       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 35.81%\n",
            "Precision: 0.78\n",
            "Recall: 0.36\n",
            "F1-Score: 0.21\n",
            "MCC: 0.10\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [95  3]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       1.00      0.03      0.06        98\n",
            "\n",
            "    accuracy                           0.36       148\n",
            "   macro avg       0.67      0.52      0.29       148\n",
            "weighted avg       0.78      0.36      0.21       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 44.59%\n",
            "Precision: 0.79\n",
            "Recall: 0.45\n",
            "F1-Score: 0.37\n",
            "MCC: 0.25\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [82 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      1.00      0.55        50\n",
            "           1       1.00      0.16      0.28        98\n",
            "\n",
            "    accuracy                           0.45       148\n",
            "   macro avg       0.69      0.58      0.42       148\n",
            "weighted avg       0.79      0.45      0.37       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n",
            "\n",
            "AdaBoost Testing Accuracy: 39.19%\n",
            "Precision: 0.78\n",
            "Recall: 0.39\n",
            "F1-Score: 0.28\n",
            "MCC: 0.17\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [90  8]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.08      0.15        98\n",
            "\n",
            "    accuracy                           0.39       148\n",
            "   macro avg       0.68      0.54      0.34       148\n",
            "weighted avg       0.78      0.39      0.28       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 50}\n",
            "\n",
            "Bagging Testing Accuracy: 37.84%\n",
            "Precision: 0.78\n",
            "Recall: 0.38\n",
            "F1-Score: 0.25\n",
            "MCC: 0.15\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [92  6]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.06      0.12        98\n",
            "\n",
            "    accuracy                           0.38       148\n",
            "   macro avg       0.68      0.53      0.32       148\n",
            "weighted avg       0.78      0.38      0.25       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "\n",
            "Extra Trees Testing Accuracy: 41.89%\n",
            "Precision: 0.79\n",
            "Recall: 0.42\n",
            "F1-Score: 0.33\n",
            "MCC: 0.21\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [86 12]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.12      0.22        98\n",
            "\n",
            "    accuracy                           0.42       148\n",
            "   macro avg       0.68      0.56      0.38       148\n",
            "weighted avg       0.79      0.42      0.33       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 41.89%\n",
            "Precision: 0.79\n",
            "Recall: 0.42\n",
            "F1-Score: 0.33\n",
            "MCC: 0.21\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [86 12]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.12      0.22        98\n",
            "\n",
            "    accuracy                           0.42       148\n",
            "   macro avg       0.68      0.56      0.38       148\n",
            "weighted avg       0.79      0.42      0.33       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n",
            "Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 50}\n",
            "Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Random Search CV"
      ],
      "metadata": {
        "id": "k4wmwmK6dYQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2', 'PPP3CB',\n",
        "                       'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "E6ZKORzjaoQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03387dcd-a6a3-416a-e48d-e6a72712b4c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 78.38%\n",
            "Precision: 0.78\n",
            "Recall: 0.78\n",
            "F1-Score: 0.78\n",
            "MCC: 0.50\n",
            "Confusion Matrix:\n",
            "[[29 21]\n",
            " [11 87]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.58      0.64        50\n",
            "           1       0.81      0.89      0.84        98\n",
            "\n",
            "    accuracy                           0.78       148\n",
            "   macro avg       0.77      0.73      0.74       148\n",
            "weighted avg       0.78      0.78      0.78       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 58.78%\n",
            "Precision: 0.42\n",
            "Recall: 0.59\n",
            "F1-Score: 0.49\n",
            "MCC: -0.20\n",
            "Confusion Matrix:\n",
            "[[ 0 50]\n",
            " [11 87]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        50\n",
            "           1       0.64      0.89      0.74        98\n",
            "\n",
            "    accuracy                           0.59       148\n",
            "   macro avg       0.32      0.44      0.37       148\n",
            "weighted avg       0.42      0.59      0.49       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Random Forest Testing Accuracy: 38.51%\n",
            "Precision: 0.78\n",
            "Recall: 0.39\n",
            "F1-Score: 0.27\n",
            "MCC: 0.16\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [91  7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.07      0.13        98\n",
            "\n",
            "    accuracy                           0.39       148\n",
            "   macro avg       0.68      0.54      0.33       148\n",
            "weighted avg       0.78      0.39      0.27       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 43.24%\n",
            "Precision: 0.79\n",
            "Recall: 0.43\n",
            "F1-Score: 0.35\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [84 14]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.14      0.25        98\n",
            "\n",
            "    accuracy                           0.43       148\n",
            "   macro avg       0.69      0.57      0.40       148\n",
            "weighted avg       0.79      0.43      0.35       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 41.22%\n",
            "Precision: 0.79\n",
            "Recall: 0.41\n",
            "F1-Score: 0.31\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [87 11]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.11      0.20        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.68      0.56      0.37       148\n",
            "weighted avg       0.79      0.41      0.31       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 35.14%\n",
            "Precision: 0.78\n",
            "Recall: 0.35\n",
            "F1-Score: 0.20\n",
            "MCC: 0.08\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [96  2]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       1.00      0.02      0.04        98\n",
            "\n",
            "    accuracy                           0.35       148\n",
            "   macro avg       0.67      0.51      0.28       148\n",
            "weighted avg       0.78      0.35      0.20       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 41.22%\n",
            "Precision: 0.79\n",
            "Recall: 0.41\n",
            "F1-Score: 0.31\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [87 11]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.11      0.20        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.68      0.56      0.37       148\n",
            "weighted avg       0.79      0.41      0.31       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 42.57%\n",
            "Precision: 0.79\n",
            "Recall: 0.43\n",
            "F1-Score: 0.34\n",
            "MCC: 0.22\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [85 13]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.13      0.23        98\n",
            "\n",
            "    accuracy                           0.43       148\n",
            "   macro avg       0.69      0.57      0.39       148\n",
            "weighted avg       0.79      0.43      0.34       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Grid Search CV"
      ],
      "metadata": {
        "id": "ySVQ3aO5do3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2', 'PPP3CB',\n",
        "                       'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2', 'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "86yX9vwpaoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9182cd-58f2-4d98-edfa-0caf015b4f79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 84.38%\n",
            "Precision: 0.88\n",
            "Recall: 0.84\n",
            "F1-Score: 0.84\n",
            "MCC: 0.72\n",
            "Confusion Matrix:\n",
            "[[22 10]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.69      0.81        32\n",
            "           1       0.76      1.00      0.86        32\n",
            "\n",
            "    accuracy                           0.84        64\n",
            "   macro avg       0.88      0.84      0.84        64\n",
            "weighted avg       0.88      0.84      0.84        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 100.00%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "MCC: 1.00\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        32\n",
            "           1       1.00      1.00      1.00        32\n",
            "\n",
            "    accuracy                           1.00        64\n",
            "   macro avg       1.00      1.00      1.00        64\n",
            "weighted avg       1.00      1.00      1.00        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 95.31%\n",
            "Precision: 0.95\n",
            "Recall: 0.95\n",
            "F1-Score: 0.95\n",
            "MCC: 0.91\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 2 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95        32\n",
            "           1       0.97      0.94      0.95        32\n",
            "\n",
            "    accuracy                           0.95        64\n",
            "   macro avg       0.95      0.95      0.95        64\n",
            "weighted avg       0.95      0.95      0.95        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 96.88%\n",
            "Precision: 0.97\n",
            "Recall: 0.97\n",
            "F1-Score: 0.97\n",
            "MCC: 0.94\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 1 31]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.97      0.97      0.97        32\n",
            "\n",
            "    accuracy                           0.97        64\n",
            "   macro avg       0.97      0.97      0.97        64\n",
            "weighted avg       0.97      0.97      0.97        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n",
            "\n",
            "AdaBoost Testing Accuracy: 85.94%\n",
            "Precision: 0.88\n",
            "Recall: 0.86\n",
            "F1-Score: 0.86\n",
            "MCC: 0.74\n",
            "Confusion Matrix:\n",
            "[[24  8]\n",
            " [ 1 31]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.75      0.84        32\n",
            "           1       0.79      0.97      0.87        32\n",
            "\n",
            "    accuracy                           0.86        64\n",
            "   macro avg       0.88      0.86      0.86        64\n",
            "weighted avg       0.88      0.86      0.86        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.75, 'n_estimators': 50}\n",
            "\n",
            "Bagging Testing Accuracy: 95.31%\n",
            "Precision: 0.96\n",
            "Recall: 0.95\n",
            "F1-Score: 0.95\n",
            "MCC: 0.91\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 3 29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.96        32\n",
            "           1       1.00      0.91      0.95        32\n",
            "\n",
            "    accuracy                           0.95        64\n",
            "   macro avg       0.96      0.95      0.95        64\n",
            "weighted avg       0.96      0.95      0.95        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "\n",
            "Extra Trees Testing Accuracy: 95.31%\n",
            "Precision: 0.96\n",
            "Recall: 0.95\n",
            "F1-Score: 0.95\n",
            "MCC: 0.91\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 3 29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.96        32\n",
            "           1       1.00      0.91      0.95        32\n",
            "\n",
            "    accuracy                           0.95        64\n",
            "   macro avg       0.96      0.95      0.95        64\n",
            "weighted avg       0.96      0.95      0.95        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 93.75%\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "F1-Score: 0.94\n",
            "MCC: 0.88\n",
            "Confusion Matrix:\n",
            "[[30  2]\n",
            " [ 2 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        32\n",
            "           1       0.94      0.94      0.94        32\n",
            "\n",
            "    accuracy                           0.94        64\n",
            "   macro avg       0.94      0.94      0.94        64\n",
            "weighted avg       0.94      0.94      0.94        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n",
            "Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.75, 'n_estimators': 50}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Random Search CV"
      ],
      "metadata": {
        "id": "wMfqpONYd9ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PRNP', 'EHD2', 'GABARAPL1', 'CORO1C', 'DPT', 'SNCA', 'TMEM47', 'LAMB2', 'TIMP2', 'JAM3', 'MEF2C', 'FNBP1', 'RHOB', 'PRKD1', 'FLNA', 'TGFB1I1', 'RHOQ', 'CAV2',\n",
        "                       'PPP3CB', 'PTRF', 'ITPR1', 'PALLD', 'CSRP1', 'TNS1', 'ILK', 'ACTN1', 'SPARCL1', 'RASL12', 'DPYSL3', 'TUBB', 'SLIT2',\n",
        "                       'EMILIN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "UTkDQ_0DaoZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930e0f07-735e-4882-9308-2e36564bf56a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 84.38%\n",
            "Precision: 0.88\n",
            "Recall: 0.84\n",
            "F1-Score: 0.84\n",
            "MCC: 0.72\n",
            "Confusion Matrix:\n",
            "[[22 10]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.69      0.81        32\n",
            "           1       0.76      1.00      0.86        32\n",
            "\n",
            "    accuracy                           0.84        64\n",
            "   macro avg       0.88      0.84      0.84        64\n",
            "weighted avg       0.88      0.84      0.84        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 51.56%\n",
            "Precision: 0.75\n",
            "Recall: 0.52\n",
            "F1-Score: 0.37\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [31  1]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      1.00      0.67        32\n",
            "           1       1.00      0.03      0.06        32\n",
            "\n",
            "    accuracy                           0.52        64\n",
            "   macro avg       0.75      0.52      0.37        64\n",
            "weighted avg       0.75      0.52      0.37        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
            "\n",
            "Random Forest Testing Accuracy: 92.19%\n",
            "Precision: 0.93\n",
            "Recall: 0.92\n",
            "F1-Score: 0.92\n",
            "MCC: 0.85\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 4 28]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93        32\n",
            "           1       0.97      0.88      0.92        32\n",
            "\n",
            "    accuracy                           0.92        64\n",
            "   macro avg       0.93      0.92      0.92        64\n",
            "weighted avg       0.93      0.92      0.92        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 93.75%\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "F1-Score: 0.94\n",
            "MCC: 0.88\n",
            "Confusion Matrix:\n",
            "[[30  2]\n",
            " [ 2 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        32\n",
            "           1       0.94      0.94      0.94        32\n",
            "\n",
            "    accuracy                           0.94        64\n",
            "   macro avg       0.94      0.94      0.94        64\n",
            "weighted avg       0.94      0.94      0.94        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 150, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 93.75%\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "F1-Score: 0.94\n",
            "MCC: 0.88\n",
            "Confusion Matrix:\n",
            "[[30  2]\n",
            " [ 2 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        32\n",
            "           1       0.94      0.94      0.94        32\n",
            "\n",
            "    accuracy                           0.94        64\n",
            "   macro avg       0.94      0.94      0.94        64\n",
            "weighted avg       0.94      0.94      0.94        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': False}\n",
            "\n",
            "Bagging Testing Accuracy: 96.88%\n",
            "Precision: 0.97\n",
            "Recall: 0.97\n",
            "F1-Score: 0.97\n",
            "MCC: 0.94\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 2 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97        32\n",
            "           1       1.00      0.94      0.97        32\n",
            "\n",
            "    accuracy                           0.97        64\n",
            "   macro avg       0.97      0.97      0.97        64\n",
            "weighted avg       0.97      0.97      0.97        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
            "\n",
            "Extra Trees Testing Accuracy: 95.31%\n",
            "Precision: 0.95\n",
            "Recall: 0.95\n",
            "F1-Score: 0.95\n",
            "MCC: 0.91\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 2 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95        32\n",
            "           1       0.97      0.94      0.95        32\n",
            "\n",
            "    accuracy                           0.95        64\n",
            "   macro avg       0.95      0.95      0.95        64\n",
            "weighted avg       0.95      0.95      0.95        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 90.62%\n",
            "Precision: 0.91\n",
            "Recall: 0.91\n",
            "F1-Score: 0.91\n",
            "MCC: 0.81\n",
            "Confusion Matrix:\n",
            "[[29  3]\n",
            " [ 3 29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91        32\n",
            "           1       0.91      0.91      0.91        32\n",
            "\n",
            "    accuracy                           0.91        64\n",
            "   macro avg       0.91      0.91      0.91        64\n",
            "weighted avg       0.91      0.91      0.91        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.003593813663804626)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 150, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': False}\n",
            "Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    }
  ]
}