{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GSE106582"
      ],
      "metadata": {
        "id": "IeAhQKN7YdAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815"
      ],
      "metadata": {
        "id": "grjcAmeFY83U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076"
      ],
      "metadata": {
        "id": "wgRzmHjlZrAD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipA6kDmcZbxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671"
      ],
      "metadata": {
        "id": "l_JPA_FHZceG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Tuning"
      ],
      "metadata": {
        "id": "mv1tPRdxZ3Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Serach CV GSE106582"
      ],
      "metadata": {
        "id": "n61h2KX1aff8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA',\n",
        "                       'CSRP1', 'TNS1', 'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zLpq9kuZ53D",
        "outputId": "6c1759ae-28dc-44bf-f827-c75a1ae0f6d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 52.58%\n",
            "Precision: 0.76\n",
            "Recall: 0.53\n",
            "F1-Score: 0.47\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[26 91]\n",
            " [ 1 76]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.22      0.36       117\n",
            "           1       0.46      0.99      0.62        77\n",
            "\n",
            "    accuracy                           0.53       194\n",
            "   macro avg       0.71      0.60      0.49       194\n",
            "weighted avg       0.76      0.53      0.47       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 47.94%\n",
            "Precision: 0.64\n",
            "Recall: 0.48\n",
            "F1-Score: 0.41\n",
            "MCC: 0.15\n",
            "Confusion Matrix:\n",
            "[[22 95]\n",
            " [ 6 71]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.19      0.30       117\n",
            "           1       0.43      0.92      0.58        77\n",
            "\n",
            "    accuracy                           0.48       194\n",
            "   macro avg       0.61      0.56      0.44       194\n",
            "weighted avg       0.64      0.48      0.41       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
            "\n",
            "Random Forest Testing Accuracy: 59.28%\n",
            "Precision: 0.69\n",
            "Recall: 0.59\n",
            "F1-Score: 0.58\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[48 69]\n",
            " [10 67]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.41      0.55       117\n",
            "           1       0.49      0.87      0.63        77\n",
            "\n",
            "    accuracy                           0.59       194\n",
            "   macro avg       0.66      0.64      0.59       194\n",
            "weighted avg       0.69      0.59      0.58       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 64.95%\n",
            "Precision: 0.76\n",
            "Recall: 0.65\n",
            "F1-Score: 0.64\n",
            "MCC: 0.42\n",
            "Confusion Matrix:\n",
            "[[54 63]\n",
            " [ 5 72]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.46      0.61       117\n",
            "           1       0.53      0.94      0.68        77\n",
            "\n",
            "    accuracy                           0.65       194\n",
            "   macro avg       0.72      0.70      0.65       194\n",
            "weighted avg       0.76      0.65      0.64       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 61.86%\n",
            "Precision: 0.72\n",
            "Recall: 0.62\n",
            "F1-Score: 0.61\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[51 66]\n",
            " [ 8 69]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.44      0.58       117\n",
            "           1       0.51      0.90      0.65        77\n",
            "\n",
            "    accuracy                           0.62       194\n",
            "   macro avg       0.69      0.67      0.62       194\n",
            "weighted avg       0.72      0.62      0.61       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 56.19%\n",
            "Precision: 0.69\n",
            "Recall: 0.56\n",
            "F1-Score: 0.54\n",
            "MCC: 0.27\n",
            "Confusion Matrix:\n",
            "[[40 77]\n",
            " [ 8 69]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.34      0.48       117\n",
            "           1       0.47      0.90      0.62        77\n",
            "\n",
            "    accuracy                           0.56       194\n",
            "   macro avg       0.65      0.62      0.55       194\n",
            "weighted avg       0.69      0.56      0.54       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "\n",
            "Extra Trees Testing Accuracy: 49.48%\n",
            "Precision: 0.70\n",
            "Recall: 0.49\n",
            "F1-Score: 0.43\n",
            "MCC: 0.22\n",
            "Confusion Matrix:\n",
            "[[22 95]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.19      0.31       117\n",
            "           1       0.44      0.96      0.60        77\n",
            "\n",
            "    accuracy                           0.49       194\n",
            "   macro avg       0.66      0.57      0.46       194\n",
            "weighted avg       0.70      0.49      0.43       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 62.37%\n",
            "Precision: 0.71\n",
            "Recall: 0.62\n",
            "F1-Score: 0.62\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[54 63]\n",
            " [10 67]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.46      0.60       117\n",
            "           1       0.52      0.87      0.65        77\n",
            "\n",
            "    accuracy                           0.62       194\n",
            "   macro avg       0.68      0.67      0.62       194\n",
            "weighted avg       0.71      0.62      0.62       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "W4F2VAIedlkG",
        "outputId": "a320496f-f8f8-4613-f76a-331333d0511e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      PTRF   PRNP   FLNA  SPARCL1   RHOB  PALLD   SNCA  CSRP1   TNS1  DPYSL3  \\\n",
              "0    False  False  False    False  False  False  False  False  False   False   \n",
              "1    False  False  False    False  False  False  False  False  False   False   \n",
              "2    False  False  False    False  False  False  False  False  False   False   \n",
              "3    False  False  False    False  False  False  False  False  False   False   \n",
              "4    False  False  False    False  False  False  False  False  False   False   \n",
              "..     ...    ...    ...      ...    ...    ...    ...    ...    ...     ...   \n",
              "340  False  False  False    False  False  False  False  False  False   False   \n",
              "341  False  False  False    False  False  False  False  False  False   False   \n",
              "342  False  False  False    False  False  False  False  False  False   False   \n",
              "343  False  False  False    False  False  False  False  False  False   False   \n",
              "344  False  False  False    False  False  False  False  False  False   False   \n",
              "\n",
              "     TIMP2  MEF2C    ILK  ACTN1  TGFB1I1   RHOQ   CAV2  target  \n",
              "0    False  False  False  False    False  False  False   False  \n",
              "1    False  False  False  False    False  False  False   False  \n",
              "2    False  False  False  False    False  False  False   False  \n",
              "3    False  False  False  False    False  False  False   False  \n",
              "4    False  False  False  False    False  False  False   False  \n",
              "..     ...    ...    ...    ...      ...    ...    ...     ...  \n",
              "340  False  False  False  False    False  False  False   False  \n",
              "341  False  False  False  False    False  False  False   False  \n",
              "342  False  False  False  False    False  False  False   False  \n",
              "343  False  False  False  False    False  False  False   False  \n",
              "344  False  False  False  False    False  False  False   False  \n",
              "\n",
              "[345 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8094ada-d430-4218-aa77-8ca9a539d423\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PTRF</th>\n",
              "      <th>PRNP</th>\n",
              "      <th>FLNA</th>\n",
              "      <th>SPARCL1</th>\n",
              "      <th>RHOB</th>\n",
              "      <th>PALLD</th>\n",
              "      <th>SNCA</th>\n",
              "      <th>CSRP1</th>\n",
              "      <th>TNS1</th>\n",
              "      <th>DPYSL3</th>\n",
              "      <th>TIMP2</th>\n",
              "      <th>MEF2C</th>\n",
              "      <th>ILK</th>\n",
              "      <th>ACTN1</th>\n",
              "      <th>TGFB1I1</th>\n",
              "      <th>RHOQ</th>\n",
              "      <th>CAV2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>345 rows Ã— 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8094ada-d430-4218-aa77-8ca9a539d423')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b8094ada-d430-4218-aa77-8ca9a539d423 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b8094ada-d430-4218-aa77-8ca9a539d423');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-37c45d0d-ad49-41a3-b9b0-b0b6277cb848\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37c45d0d-ad49-41a3-b9b0-b0b6277cb848')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-37c45d0d-ad49-41a3-b9b0-b0b6277cb848 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 345,\n  \"fields\": [\n    {\n      \"column\": \"PTRF\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRNP\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLNA\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SPARCL1\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RHOB\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PALLD\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SNCA\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CSRP1\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TNS1\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DPYSL3\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TIMP2\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEF2C\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ILK\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ACTN1\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TGFB1I1\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RHOQ\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CAV2\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "kKJzERYneP-g",
        "outputId": "9e197949-fac0-462b-e7a1-468bc227db71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PTRF       False\n",
              "PRNP       False\n",
              "FLNA       False\n",
              "SPARCL1    False\n",
              "RHOB       False\n",
              "PALLD      False\n",
              "SNCA       False\n",
              "CSRP1      False\n",
              "TNS1       False\n",
              "DPYSL3     False\n",
              "TIMP2      False\n",
              "MEF2C      False\n",
              "ILK        False\n",
              "ACTN1      False\n",
              "TGFB1I1    False\n",
              "RHOQ       False\n",
              "CAV2       False\n",
              "target     False\n",
              "dtype: bool"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PTRF</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRNP</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FLNA</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SPARCL1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHOB</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PALLD</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNCA</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSRP1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TNS1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DPYSL3</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TIMP2</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEF2C</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ILK</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACTN1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TGFB1I1</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHOQ</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CAV2</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> bool</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns[df_train.isnull().any()]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGQNqj0-eU-H",
        "outputId": "8912bcf9-80d4-497e-d77c-7613cbc73707"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_common.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "hVUYL32CdVKV",
        "outputId": "53f97026-5c06-49e4-aa56-ff82e73dd57f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TGFB1I1    1\n",
              "ILK        1\n",
              "PRNP       1\n",
              "FLNA       0\n",
              "PTRF       1\n",
              "PALLD      0\n",
              "CSRP1      1\n",
              "ACTN1      1\n",
              "CAV2       0\n",
              "SNCA       0\n",
              "SPARCL1    1\n",
              "RHOB       1\n",
              "TIMP2      0\n",
              "MEF2C      1\n",
              "TNS1       1\n",
              "RHOQ       1\n",
              "DPYSL3     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TGFB1I1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ILK</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRNP</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FLNA</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PTRF</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PALLD</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSRP1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACTN1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CAV2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNCA</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SPARCL1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHOB</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TIMP2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEF2C</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TNS1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHOQ</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DPYSL3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "n6DdfaAIdD-m",
        "outputId": "c2964b3f-16f7-4156-db0b-40bcf474a7b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        TGFB1I1          ILK        PRNP        FLNA         PTRF       PALLD  \\\n",
              "0     128.91800  1334.019000  2525.33300   129.41580   1815.16700   1638.3070   \n",
              "1     146.04050   987.597000  2478.02200   201.83130   2654.56600   1938.3600   \n",
              "2    1500.77200  7076.318000  9752.85100  4744.04400  28658.69000  13296.1000   \n",
              "3      81.53146   806.585600  3518.65300   117.58670   1306.54900   1377.7610   \n",
              "4     689.19950  3120.280000  3504.77600  2481.00300  19209.36000   7848.7430   \n",
              "..          ...          ...         ...         ...          ...         ...   \n",
              "340   107.19923   122.380936   545.56420   157.54823    151.43648    708.5136   \n",
              "341   297.19640   126.028010   981.79990   718.72160    432.79465   1053.3113   \n",
              "342   217.49037   102.200990   550.05050   236.23875    210.38971    946.1038   \n",
              "343   169.52666    91.976770  1805.44070   330.13992    230.68983    642.6224   \n",
              "344   216.89598   152.711800   315.76483   250.65533    178.45993    887.7274   \n",
              "\n",
              "           CSRP1        ACTN1       CAV2        SNCA      SPARCL1  \\\n",
              "0     2751.51100   1302.68100  126.19610   83.461750   1551.67000   \n",
              "1     3448.62500   2296.27400  231.37260  106.309100   1086.06200   \n",
              "2    33181.18000  13146.80000  731.82810  423.809000  14092.56000   \n",
              "3     2822.32000   2180.37600  123.91260   51.109720    436.62330   \n",
              "4    21789.32000   8109.85700  297.20290  161.920600   5941.94700   \n",
              "..           ...          ...        ...         ...          ...   \n",
              "340    231.58870   1126.93410  117.67158   11.782273     92.23398   \n",
              "341    482.59622    908.68695  191.21483   11.031474    639.76310   \n",
              "342    235.60060    798.90920  301.34503   11.946541    220.10430   \n",
              "343    201.74535    751.56934  140.47680    9.838649    587.13130   \n",
              "344    295.43042    775.24620  105.24062   10.121882    321.88965   \n",
              "\n",
              "            RHOB       TIMP2       MEF2C        TNS1         RHOQ  \\\n",
              "0    2217.573000  2282.42000  393.864700   34.813670  1656.344000   \n",
              "1    1288.359000  1225.11500  123.796300   55.187360  2055.204000   \n",
              "2    8771.118000  9982.55900  555.490400  770.330300  6837.925000   \n",
              "3    4295.779000   921.19460   45.465650   29.580640   749.293500   \n",
              "4    3388.500000  4066.77300  391.786400  240.026600  2012.708000   \n",
              "..           ...         ...         ...         ...          ...   \n",
              "340    78.066880   404.02570   24.651146   27.482962    16.600166   \n",
              "341   123.509910   593.02783   68.284325   43.319565    64.263370   \n",
              "342    92.778730   682.45400   56.451244   30.474731    28.831093   \n",
              "343   117.208885   741.87305   77.262535   32.156227    35.960430   \n",
              "344   105.054630   548.22970   44.847412   31.868118    28.070868   \n",
              "\n",
              "           DPYSL3  \n",
              "0      855.154400  \n",
              "1     1548.034000  \n",
              "2    22552.190000  \n",
              "3      408.538600  \n",
              "4     7205.648000  \n",
              "..            ...  \n",
              "340     22.743372  \n",
              "341    124.817505  \n",
              "342    131.724150  \n",
              "343    102.946260  \n",
              "344     74.763750  \n",
              "\n",
              "[345 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d720d7d-4dee-4087-8573-ec9069cc2e29\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TGFB1I1</th>\n",
              "      <th>ILK</th>\n",
              "      <th>PRNP</th>\n",
              "      <th>FLNA</th>\n",
              "      <th>PTRF</th>\n",
              "      <th>PALLD</th>\n",
              "      <th>CSRP1</th>\n",
              "      <th>ACTN1</th>\n",
              "      <th>CAV2</th>\n",
              "      <th>SNCA</th>\n",
              "      <th>SPARCL1</th>\n",
              "      <th>RHOB</th>\n",
              "      <th>TIMP2</th>\n",
              "      <th>MEF2C</th>\n",
              "      <th>TNS1</th>\n",
              "      <th>RHOQ</th>\n",
              "      <th>DPYSL3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128.91800</td>\n",
              "      <td>1334.019000</td>\n",
              "      <td>2525.33300</td>\n",
              "      <td>129.41580</td>\n",
              "      <td>1815.16700</td>\n",
              "      <td>1638.3070</td>\n",
              "      <td>2751.51100</td>\n",
              "      <td>1302.68100</td>\n",
              "      <td>126.19610</td>\n",
              "      <td>83.461750</td>\n",
              "      <td>1551.67000</td>\n",
              "      <td>2217.573000</td>\n",
              "      <td>2282.42000</td>\n",
              "      <td>393.864700</td>\n",
              "      <td>34.813670</td>\n",
              "      <td>1656.344000</td>\n",
              "      <td>855.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>146.04050</td>\n",
              "      <td>987.597000</td>\n",
              "      <td>2478.02200</td>\n",
              "      <td>201.83130</td>\n",
              "      <td>2654.56600</td>\n",
              "      <td>1938.3600</td>\n",
              "      <td>3448.62500</td>\n",
              "      <td>2296.27400</td>\n",
              "      <td>231.37260</td>\n",
              "      <td>106.309100</td>\n",
              "      <td>1086.06200</td>\n",
              "      <td>1288.359000</td>\n",
              "      <td>1225.11500</td>\n",
              "      <td>123.796300</td>\n",
              "      <td>55.187360</td>\n",
              "      <td>2055.204000</td>\n",
              "      <td>1548.034000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1500.77200</td>\n",
              "      <td>7076.318000</td>\n",
              "      <td>9752.85100</td>\n",
              "      <td>4744.04400</td>\n",
              "      <td>28658.69000</td>\n",
              "      <td>13296.1000</td>\n",
              "      <td>33181.18000</td>\n",
              "      <td>13146.80000</td>\n",
              "      <td>731.82810</td>\n",
              "      <td>423.809000</td>\n",
              "      <td>14092.56000</td>\n",
              "      <td>8771.118000</td>\n",
              "      <td>9982.55900</td>\n",
              "      <td>555.490400</td>\n",
              "      <td>770.330300</td>\n",
              "      <td>6837.925000</td>\n",
              "      <td>22552.190000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81.53146</td>\n",
              "      <td>806.585600</td>\n",
              "      <td>3518.65300</td>\n",
              "      <td>117.58670</td>\n",
              "      <td>1306.54900</td>\n",
              "      <td>1377.7610</td>\n",
              "      <td>2822.32000</td>\n",
              "      <td>2180.37600</td>\n",
              "      <td>123.91260</td>\n",
              "      <td>51.109720</td>\n",
              "      <td>436.62330</td>\n",
              "      <td>4295.779000</td>\n",
              "      <td>921.19460</td>\n",
              "      <td>45.465650</td>\n",
              "      <td>29.580640</td>\n",
              "      <td>749.293500</td>\n",
              "      <td>408.538600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>689.19950</td>\n",
              "      <td>3120.280000</td>\n",
              "      <td>3504.77600</td>\n",
              "      <td>2481.00300</td>\n",
              "      <td>19209.36000</td>\n",
              "      <td>7848.7430</td>\n",
              "      <td>21789.32000</td>\n",
              "      <td>8109.85700</td>\n",
              "      <td>297.20290</td>\n",
              "      <td>161.920600</td>\n",
              "      <td>5941.94700</td>\n",
              "      <td>3388.500000</td>\n",
              "      <td>4066.77300</td>\n",
              "      <td>391.786400</td>\n",
              "      <td>240.026600</td>\n",
              "      <td>2012.708000</td>\n",
              "      <td>7205.648000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>107.19923</td>\n",
              "      <td>122.380936</td>\n",
              "      <td>545.56420</td>\n",
              "      <td>157.54823</td>\n",
              "      <td>151.43648</td>\n",
              "      <td>708.5136</td>\n",
              "      <td>231.58870</td>\n",
              "      <td>1126.93410</td>\n",
              "      <td>117.67158</td>\n",
              "      <td>11.782273</td>\n",
              "      <td>92.23398</td>\n",
              "      <td>78.066880</td>\n",
              "      <td>404.02570</td>\n",
              "      <td>24.651146</td>\n",
              "      <td>27.482962</td>\n",
              "      <td>16.600166</td>\n",
              "      <td>22.743372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>297.19640</td>\n",
              "      <td>126.028010</td>\n",
              "      <td>981.79990</td>\n",
              "      <td>718.72160</td>\n",
              "      <td>432.79465</td>\n",
              "      <td>1053.3113</td>\n",
              "      <td>482.59622</td>\n",
              "      <td>908.68695</td>\n",
              "      <td>191.21483</td>\n",
              "      <td>11.031474</td>\n",
              "      <td>639.76310</td>\n",
              "      <td>123.509910</td>\n",
              "      <td>593.02783</td>\n",
              "      <td>68.284325</td>\n",
              "      <td>43.319565</td>\n",
              "      <td>64.263370</td>\n",
              "      <td>124.817505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>217.49037</td>\n",
              "      <td>102.200990</td>\n",
              "      <td>550.05050</td>\n",
              "      <td>236.23875</td>\n",
              "      <td>210.38971</td>\n",
              "      <td>946.1038</td>\n",
              "      <td>235.60060</td>\n",
              "      <td>798.90920</td>\n",
              "      <td>301.34503</td>\n",
              "      <td>11.946541</td>\n",
              "      <td>220.10430</td>\n",
              "      <td>92.778730</td>\n",
              "      <td>682.45400</td>\n",
              "      <td>56.451244</td>\n",
              "      <td>30.474731</td>\n",
              "      <td>28.831093</td>\n",
              "      <td>131.724150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>169.52666</td>\n",
              "      <td>91.976770</td>\n",
              "      <td>1805.44070</td>\n",
              "      <td>330.13992</td>\n",
              "      <td>230.68983</td>\n",
              "      <td>642.6224</td>\n",
              "      <td>201.74535</td>\n",
              "      <td>751.56934</td>\n",
              "      <td>140.47680</td>\n",
              "      <td>9.838649</td>\n",
              "      <td>587.13130</td>\n",
              "      <td>117.208885</td>\n",
              "      <td>741.87305</td>\n",
              "      <td>77.262535</td>\n",
              "      <td>32.156227</td>\n",
              "      <td>35.960430</td>\n",
              "      <td>102.946260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>216.89598</td>\n",
              "      <td>152.711800</td>\n",
              "      <td>315.76483</td>\n",
              "      <td>250.65533</td>\n",
              "      <td>178.45993</td>\n",
              "      <td>887.7274</td>\n",
              "      <td>295.43042</td>\n",
              "      <td>775.24620</td>\n",
              "      <td>105.24062</td>\n",
              "      <td>10.121882</td>\n",
              "      <td>321.88965</td>\n",
              "      <td>105.054630</td>\n",
              "      <td>548.22970</td>\n",
              "      <td>44.847412</td>\n",
              "      <td>31.868118</td>\n",
              "      <td>28.070868</td>\n",
              "      <td>74.763750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>345 rows Ã— 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d720d7d-4dee-4087-8573-ec9069cc2e29')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d720d7d-4dee-4087-8573-ec9069cc2e29 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d720d7d-4dee-4087-8573-ec9069cc2e29');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b9438e3f-749a-4baa-9282-9190179e9eee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b9438e3f-749a-4baa-9282-9190179e9eee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b9438e3f-749a-4baa-9282-9190179e9eee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d6fb960f-3211-4937-bc57-12daadce4d06\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_common')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d6fb960f-3211-4937-bc57-12daadce4d06 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_common');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_common",
              "summary": "{\n  \"name\": \"X_train_common\",\n  \"rows\": 345,\n  \"fields\": [\n    {\n      \"column\": \"TGFB1I1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 527.426691185389,\n        \"min\": -1.3774061,\n        \"max\": 3810.448,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          9.689468603,\n          0.44852495,\n          8.89297\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ILK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1675.3817516727704,\n        \"min\": -0.85114,\n        \"max\": 11025.18,\n        \"num_unique_values\": 344,\n        \"samples\": [\n          10.7928468,\n          0.96896553,\n          9.99942\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRNP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2758.6594989074674,\n        \"min\": -1.5577183,\n        \"max\": 16314.31,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          7.744371937,\n          0.005721092,\n          10.2051\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FLNA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1461.8349761269862,\n        \"min\": -2.1861553,\n        \"max\": 10039.17,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          11.67503899,\n          -0.34787893,\n          8.71846\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PTRF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5869.792396639395,\n        \"min\": -1.9631991,\n        \"max\": 39635.0,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          10.11486464,\n          0.7387347,\n          9.05977\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PALLD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2972.2448042176484,\n        \"min\": -1.7374678,\n        \"max\": 18788.26,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          10.82481799,\n          -1.1653662,\n          10.904\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CSRP1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8030.46539177014,\n        \"min\": -1.4255338,\n        \"max\": 44854.32,\n        \"num_unique_values\": 344,\n        \"samples\": [\n          11.60679343,\n          0.5997844,\n          14.0525\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ACTN1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2743.5045063444827,\n        \"min\": -1.3216166,\n        \"max\": 17422.08,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          10.09121418,\n          -0.49361086,\n          11.4388\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CAV2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 172.28730383613177,\n        \"min\": -1.003047,\n        \"max\": 1249.836,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          6.616948171,\n          -0.42472792,\n          7.68087\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SNCA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122.28416535208089,\n        \"min\": -1.3601897,\n        \"max\": 881.6031,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          7.787767502,\n          0.26986098,\n          6.14032\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SPARCL1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2698.213824576141,\n        \"min\": -3.177361,\n        \"max\": 17885.41,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          10.97312415,\n          -3.177361,\n          11.0136\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RHOB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2532.8326175200023,\n        \"min\": -1.4285016,\n        \"max\": 17802.05,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          10.86212414,\n          0.6651988,\n          9.4599\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TIMP2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2209.8716047147173,\n        \"min\": -1.8546109,\n        \"max\": 12086.89,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          8.994156544,\n          0.11473036,\n          8.2852\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEF2C\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 136.43482850840294,\n        \"min\": -1.7116871,\n        \"max\": 861.134,\n        \"num_unique_values\": 343,\n        \"samples\": [\n          5.909,\n          29.96363,\n          25.53901\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TNS1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 216.59842073056814,\n        \"min\": -1.1174521,\n        \"max\": 1691.208,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          7.140399948,\n          0.24713945,\n          5.34492\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RHOQ\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1456.610850437791,\n        \"min\": -1.3476009,\n        \"max\": 8604.916,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          8.715263468,\n          0.031167984,\n          9.43439\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DPYSL3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3423.9662007650427,\n        \"min\": -2.3171244,\n        \"max\": 24321.0,\n        \"num_unique_values\": 345,\n        \"samples\": [\n          7.795126002,\n          -1.3654232,\n          5.6263\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_common.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "-xP9z-1udHf2",
        "outputId": "5e1e0fb8-c8b6-4f8e-f1f6-2570004cdc8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TGFB1I1    0\n",
              "ILK        0\n",
              "PRNP       0\n",
              "FLNA       0\n",
              "PTRF       0\n",
              "PALLD      0\n",
              "CSRP1      0\n",
              "ACTN1      0\n",
              "CAV2       0\n",
              "SNCA       0\n",
              "SPARCL1    0\n",
              "RHOB       0\n",
              "TIMP2      0\n",
              "MEF2C      0\n",
              "TNS1       0\n",
              "RHOQ       0\n",
              "DPYSL3     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TGFB1I1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ILK</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRNP</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FLNA</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PTRF</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PALLD</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CSRP1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ACTN1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CAV2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SNCA</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SPARCL1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHOB</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TIMP2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MEF2C</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TNS1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHOQ</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DPYSL3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Search CV GSE106582"
      ],
      "metadata": {
        "id": "sihKR_2TaZTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA', 'CSRP1', 'TNS1',\n",
        "                       'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_fQ-5TjaSX-",
        "outputId": "8b91196b-4b44-47cb-fc35-23a8cda1a61d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 87.11%\n",
            "Precision: 0.88\n",
            "Recall: 0.87\n",
            "F1-Score: 0.87\n",
            "MCC: 0.74\n",
            "Confusion Matrix:\n",
            "[[114   3]\n",
            " [ 22  55]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.97      0.90       117\n",
            "           1       0.95      0.71      0.81        77\n",
            "\n",
            "    accuracy                           0.87       194\n",
            "   macro avg       0.89      0.84      0.86       194\n",
            "weighted avg       0.88      0.87      0.87       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 47.94%\n",
            "Precision: 0.64\n",
            "Recall: 0.48\n",
            "F1-Score: 0.41\n",
            "MCC: 0.15\n",
            "Confusion Matrix:\n",
            "[[22 95]\n",
            " [ 6 71]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.19      0.30       117\n",
            "           1       0.43      0.92      0.58        77\n",
            "\n",
            "    accuracy                           0.48       194\n",
            "   macro avg       0.61      0.56      0.44       194\n",
            "weighted avg       0.64      0.48      0.41       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Random Forest Testing Accuracy: 57.73%\n",
            "Precision: 0.69\n",
            "Recall: 0.58\n",
            "F1-Score: 0.56\n",
            "MCC: 0.28\n",
            "Confusion Matrix:\n",
            "[[44 73]\n",
            " [ 9 68]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.38      0.52       117\n",
            "           1       0.48      0.88      0.62        77\n",
            "\n",
            "    accuracy                           0.58       194\n",
            "   macro avg       0.66      0.63      0.57       194\n",
            "weighted avg       0.69      0.58      0.56       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.0575)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 65.98%\n",
            "Precision: 0.75\n",
            "Recall: 0.66\n",
            "F1-Score: 0.66\n",
            "MCC: 0.41\n",
            "Confusion Matrix:\n",
            "[[59 58]\n",
            " [ 8 69]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.50      0.64       117\n",
            "           1       0.54      0.90      0.68        77\n",
            "\n",
            "    accuracy                           0.66       194\n",
            "   macro avg       0.71      0.70      0.66       194\n",
            "weighted avg       0.75      0.66      0.66       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 61.86%\n",
            "Precision: 0.72\n",
            "Recall: 0.62\n",
            "F1-Score: 0.61\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[51 66]\n",
            " [ 8 69]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.44      0.58       117\n",
            "           1       0.51      0.90      0.65        77\n",
            "\n",
            "    accuracy                           0.62       194\n",
            "   macro avg       0.69      0.67      0.62       194\n",
            "weighted avg       0.72      0.62      0.61       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.75), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 70.10%\n",
            "Precision: 0.75\n",
            "Recall: 0.70\n",
            "F1-Score: 0.70\n",
            "MCC: 0.45\n",
            "Confusion Matrix:\n",
            "[[71 46]\n",
            " [12 65]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.61      0.71       117\n",
            "           1       0.59      0.84      0.69        77\n",
            "\n",
            "    accuracy                           0.70       194\n",
            "   macro avg       0.72      0.73      0.70       194\n",
            "weighted avg       0.75      0.70      0.70       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 50.00%\n",
            "Precision: 0.71\n",
            "Recall: 0.50\n",
            "F1-Score: 0.43\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[23 94]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.20      0.32       117\n",
            "           1       0.44      0.96      0.60        77\n",
            "\n",
            "    accuracy                           0.50       194\n",
            "   macro avg       0.66      0.58      0.46       194\n",
            "weighted avg       0.71      0.50      0.43       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 67.53%\n",
            "Precision: 0.72\n",
            "Recall: 0.68\n",
            "F1-Score: 0.68\n",
            "MCC: 0.40\n",
            "Confusion Matrix:\n",
            "[[68 49]\n",
            " [14 63]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.58      0.68       117\n",
            "           1       0.56      0.82      0.67        77\n",
            "\n",
            "    accuracy                           0.68       194\n",
            "   macro avg       0.70      0.70      0.68       194\n",
            "weighted avg       0.72      0.68      0.68       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.0575)}\n",
            "AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.75), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Grid Search CV"
      ],
      "metadata": {
        "id": "xHOEnOf3aznk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA', 'CSRP1',\n",
        "                       'TNS1', 'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k3URklZan5c",
        "outputId": "b5adcea2-aad0-4633-9483-d64c98740885"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 95.74%\n",
            "Precision: 0.95\n",
            "Recall: 0.96\n",
            "F1-Score: 0.95\n",
            "MCC: 0.61\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.56      0.62         9\n",
            "           1       0.97      0.98      0.98       132\n",
            "\n",
            "    accuracy                           0.96       141\n",
            "   macro avg       0.84      0.77      0.80       141\n",
            "weighted avg       0.95      0.96      0.95       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 81.56%\n",
            "Precision: 0.90\n",
            "Recall: 0.82\n",
            "F1-Score: 0.85\n",
            "MCC: 0.12\n",
            "Confusion Matrix:\n",
            "[[  3   6]\n",
            " [ 20 112]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.13      0.33      0.19         9\n",
            "           1       0.95      0.85      0.90       132\n",
            "\n",
            "    accuracy                           0.82       141\n",
            "   macro avg       0.54      0.59      0.54       141\n",
            "weighted avg       0.90      0.82      0.85       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 88.65%\n",
            "Precision: 0.92\n",
            "Recall: 0.89\n",
            "F1-Score: 0.90\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 11 121]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.44      0.33         9\n",
            "           1       0.96      0.92      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.61      0.68      0.64       141\n",
            "weighted avg       0.92      0.89      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 86.52%\n",
            "Precision: 0.91\n",
            "Recall: 0.87\n",
            "F1-Score: 0.89\n",
            "MCC: 0.25\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 14 118]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.44      0.30         9\n",
            "           1       0.96      0.89      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.59      0.67      0.61       141\n",
            "weighted avg       0.91      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 85.11%\n",
            "Precision: 0.92\n",
            "Recall: 0.85\n",
            "F1-Score: 0.88\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.56      0.32         9\n",
            "           1       0.97      0.87      0.92       132\n",
            "\n",
            "    accuracy                           0.85       141\n",
            "   macro avg       0.60      0.71      0.62       141\n",
            "weighted avg       0.92      0.85      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 85.11%\n",
            "Precision: 0.92\n",
            "Recall: 0.85\n",
            "F1-Score: 0.88\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.56      0.32         9\n",
            "           1       0.97      0.87      0.92       132\n",
            "\n",
            "    accuracy                           0.85       141\n",
            "   macro avg       0.60      0.71      0.62       141\n",
            "weighted avg       0.92      0.85      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "\n",
            "Extra Trees Testing Accuracy: 87.94%\n",
            "Precision: 0.92\n",
            "Recall: 0.88\n",
            "F1-Score: 0.90\n",
            "MCC: 0.33\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.56      0.37         9\n",
            "           1       0.97      0.90      0.93       132\n",
            "\n",
            "    accuracy                           0.88       141\n",
            "   macro avg       0.62      0.73      0.65       141\n",
            "weighted avg       0.92      0.88      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 88.65%\n",
            "Precision: 0.92\n",
            "Recall: 0.89\n",
            "F1-Score: 0.90\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 11 121]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.44      0.33         9\n",
            "           1       0.96      0.92      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.61      0.68      0.64       141\n",
            "weighted avg       0.92      0.89      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Random SearchCV"
      ],
      "metadata": {
        "id": "SG4ZHqbubCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA', 'CSRP1', 'TNS1',\n",
        "                       'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "IMDhs1_9aoDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7037901e-7454-46e1-e355-edad0d5ea899"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 92.91%\n",
            "Precision: 0.94\n",
            "Recall: 0.93\n",
            "F1-Score: 0.93\n",
            "MCC: 0.52\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [  7 125]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.67      0.55         9\n",
            "           1       0.98      0.95      0.96       132\n",
            "\n",
            "    accuracy                           0.93       141\n",
            "   macro avg       0.72      0.81      0.75       141\n",
            "weighted avg       0.94      0.93      0.93       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 81.56%\n",
            "Precision: 0.90\n",
            "Recall: 0.82\n",
            "F1-Score: 0.85\n",
            "MCC: 0.12\n",
            "Confusion Matrix:\n",
            "[[  3   6]\n",
            " [ 20 112]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.13      0.33      0.19         9\n",
            "           1       0.95      0.85      0.90       132\n",
            "\n",
            "    accuracy                           0.82       141\n",
            "   macro avg       0.54      0.59      0.54       141\n",
            "weighted avg       0.90      0.82      0.85       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 85.11%\n",
            "Precision: 0.92\n",
            "Recall: 0.85\n",
            "F1-Score: 0.88\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.56      0.32         9\n",
            "           1       0.97      0.87      0.92       132\n",
            "\n",
            "    accuracy                           0.85       141\n",
            "   macro avg       0.60      0.71      0.62       141\n",
            "weighted avg       0.92      0.85      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 3, 'learning_rate': np.float64(0.0575)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 87.23%\n",
            "Precision: 0.92\n",
            "Recall: 0.87\n",
            "F1-Score: 0.89\n",
            "MCC: 0.32\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 14 118]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.56      0.36         9\n",
            "           1       0.97      0.89      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.62      0.72      0.64       141\n",
            "weighted avg       0.92      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 85.11%\n",
            "Precision: 0.92\n",
            "Recall: 0.85\n",
            "F1-Score: 0.88\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.56      0.32         9\n",
            "           1       0.97      0.87      0.92       132\n",
            "\n",
            "    accuracy                           0.85       141\n",
            "   macro avg       0.60      0.71      0.62       141\n",
            "weighted avg       0.92      0.85      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.625), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 87.23%\n",
            "Precision: 0.91\n",
            "Recall: 0.87\n",
            "F1-Score: 0.89\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.44      0.31         9\n",
            "           1       0.96      0.90      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.60      0.67      0.62       141\n",
            "weighted avg       0.91      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Extra Trees Testing Accuracy: 85.82%\n",
            "Precision: 0.92\n",
            "Recall: 0.86\n",
            "F1-Score: 0.88\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 16 116]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.56      0.33         9\n",
            "           1       0.97      0.88      0.92       132\n",
            "\n",
            "    accuracy                           0.86       141\n",
            "   macro avg       0.60      0.72      0.63       141\n",
            "weighted avg       0.92      0.86      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 87.94%\n",
            "Precision: 0.91\n",
            "Recall: 0.88\n",
            "F1-Score: 0.89\n",
            "MCC: 0.27\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 12 120]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.44      0.32         9\n",
            "           1       0.96      0.91      0.93       132\n",
            "\n",
            "    accuracy                           0.88       141\n",
            "   macro avg       0.60      0.68      0.63       141\n",
            "weighted avg       0.91      0.88      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 3, 'learning_rate': np.float64(0.0575)}\n",
            "AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.625), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 500, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Grid Serach CV"
      ],
      "metadata": {
        "id": "xUgGf7Ebc0Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA', 'CSRP1', 'TNS1',\n",
        "                       'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "wk70MoTBaoL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ead11e-b3b4-4fa3-c0a1-c134ebb9d16f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 65.54%\n",
            "Precision: 0.58\n",
            "Recall: 0.66\n",
            "F1-Score: 0.55\n",
            "MCC: 0.02\n",
            "Confusion Matrix:\n",
            "[[ 2 48]\n",
            " [ 3 95]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.04      0.07        50\n",
            "           1       0.66      0.97      0.79        98\n",
            "\n",
            "    accuracy                           0.66       148\n",
            "   macro avg       0.53      0.50      0.43       148\n",
            "weighted avg       0.58      0.66      0.55       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 89.19%\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.76\n",
            "Confusion Matrix:\n",
            "[[37 13]\n",
            " [ 3 95]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.74      0.82        50\n",
            "           1       0.88      0.97      0.92        98\n",
            "\n",
            "    accuracy                           0.89       148\n",
            "   macro avg       0.90      0.85      0.87       148\n",
            "weighted avg       0.89      0.89      0.89       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 40.54%\n",
            "Precision: 0.78\n",
            "Recall: 0.41\n",
            "F1-Score: 0.30\n",
            "MCC: 0.19\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [88 10]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.10      0.19        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.68      0.55      0.36       148\n",
            "weighted avg       0.78      0.41      0.30       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 41.89%\n",
            "Precision: 0.55\n",
            "Recall: 0.42\n",
            "F1-Score: 0.40\n",
            "MCC: -0.01\n",
            "Confusion Matrix:\n",
            "[[37 13]\n",
            " [73 25]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.74      0.46        50\n",
            "           1       0.66      0.26      0.37        98\n",
            "\n",
            "    accuracy                           0.42       148\n",
            "   macro avg       0.50      0.50      0.42       148\n",
            "weighted avg       0.55      0.42      0.40       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 56.08%\n",
            "Precision: 0.81\n",
            "Recall: 0.56\n",
            "F1-Score: 0.54\n",
            "MCC: 0.38\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [65 33]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      1.00      0.61        50\n",
            "           1       1.00      0.34      0.50        98\n",
            "\n",
            "    accuracy                           0.56       148\n",
            "   macro avg       0.72      0.67      0.55       148\n",
            "weighted avg       0.81      0.56      0.54       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.75, 'n_estimators': 50}\n",
            "\n",
            "Bagging Testing Accuracy: 37.16%\n",
            "Precision: 0.78\n",
            "Recall: 0.37\n",
            "F1-Score: 0.24\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [93  5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.05      0.10        98\n",
            "\n",
            "    accuracy                           0.37       148\n",
            "   macro avg       0.67      0.53      0.31       148\n",
            "weighted avg       0.78      0.37      0.24       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Extra Trees Testing Accuracy: 60.81%\n",
            "Precision: 0.82\n",
            "Recall: 0.61\n",
            "F1-Score: 0.60\n",
            "MCC: 0.43\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [58 40]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      1.00      0.63        50\n",
            "           1       1.00      0.41      0.58        98\n",
            "\n",
            "    accuracy                           0.61       148\n",
            "   macro avg       0.73      0.70      0.61       148\n",
            "weighted avg       0.82      0.61      0.60       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 44.59%\n",
            "Precision: 0.79\n",
            "Recall: 0.45\n",
            "F1-Score: 0.37\n",
            "MCC: 0.25\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [82 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      1.00      0.55        50\n",
            "           1       1.00      0.16      0.28        98\n",
            "\n",
            "    accuracy                           0.45       148\n",
            "   macro avg       0.69      0.58      0.42       148\n",
            "weighted avg       0.79      0.45      0.37       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.75, 'n_estimators': 50}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Random Search CV"
      ],
      "metadata": {
        "id": "k4wmwmK6dYQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA', 'CSRP1', 'TNS1',\n",
        "                       'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "E6ZKORzjaoQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e67655-6272-43db-e3ec-4a023ca1dccb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 84.46%\n",
            "Precision: 0.84\n",
            "Recall: 0.84\n",
            "F1-Score: 0.84\n",
            "MCC: 0.65\n",
            "Confusion Matrix:\n",
            "[[36 14]\n",
            " [ 9 89]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.72      0.76        50\n",
            "           1       0.86      0.91      0.89        98\n",
            "\n",
            "    accuracy                           0.84       148\n",
            "   macro avg       0.83      0.81      0.82       148\n",
            "weighted avg       0.84      0.84      0.84       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 89.19%\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.76\n",
            "Confusion Matrix:\n",
            "[[37 13]\n",
            " [ 3 95]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.74      0.82        50\n",
            "           1       0.88      0.97      0.92        98\n",
            "\n",
            "    accuracy                           0.89       148\n",
            "   macro avg       0.90      0.85      0.87       148\n",
            "weighted avg       0.89      0.89      0.89       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}\n",
            "\n",
            "Random Forest Testing Accuracy: 41.22%\n",
            "Precision: 0.79\n",
            "Recall: 0.41\n",
            "F1-Score: 0.31\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [87 11]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.11      0.20        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.68      0.56      0.37       148\n",
            "weighted avg       0.79      0.41      0.31       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 3, 'learning_rate': np.float64(0.0575)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 53.38%\n",
            "Precision: 0.80\n",
            "Recall: 0.53\n",
            "F1-Score: 0.50\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [69 29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59        50\n",
            "           1       1.00      0.30      0.46        98\n",
            "\n",
            "    accuracy                           0.53       148\n",
            "   macro avg       0.71      0.65      0.52       148\n",
            "weighted avg       0.80      0.53      0.50       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 54.05%\n",
            "Precision: 0.81\n",
            "Recall: 0.54\n",
            "F1-Score: 0.51\n",
            "MCC: 0.36\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [68 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.60        50\n",
            "           1       1.00      0.31      0.47        98\n",
            "\n",
            "    accuracy                           0.54       148\n",
            "   macro avg       0.71      0.65      0.53       148\n",
            "weighted avg       0.81      0.54      0.51       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.875), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 39.86%\n",
            "Precision: 0.78\n",
            "Recall: 0.40\n",
            "F1-Score: 0.29\n",
            "MCC: 0.18\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [89  9]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.09      0.17        98\n",
            "\n",
            "    accuracy                           0.40       148\n",
            "   macro avg       0.68      0.55      0.35       148\n",
            "weighted avg       0.78      0.40      0.29       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 48.65%\n",
            "Precision: 0.80\n",
            "Recall: 0.49\n",
            "F1-Score: 0.43\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [76 22]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      1.00      0.57        50\n",
            "           1       1.00      0.22      0.37        98\n",
            "\n",
            "    accuracy                           0.49       148\n",
            "   macro avg       0.70      0.61      0.47       148\n",
            "weighted avg       0.80      0.49      0.43       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 44.59%\n",
            "Precision: 0.79\n",
            "Recall: 0.45\n",
            "F1-Score: 0.37\n",
            "MCC: 0.25\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [82 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      1.00      0.55        50\n",
            "           1       1.00      0.16      0.28        98\n",
            "\n",
            "    accuracy                           0.45       148\n",
            "   macro avg       0.69      0.58      0.42       148\n",
            "weighted avg       0.79      0.45      0.37       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 3, 'learning_rate': np.float64(0.0575)}\n",
            "AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.875), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Grid Search CV"
      ],
      "metadata": {
        "id": "ySVQ3aO5do3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA',\n",
        "                       'CSRP1', 'TNS1', 'DPYSL3', 'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "86yX9vwpaoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8501776d-dbaf-434f-906e-7405e3d83116"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 100.00%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "MCC: 1.00\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        32\n",
            "           1       1.00      1.00      1.00        32\n",
            "\n",
            "    accuracy                           1.00        64\n",
            "   macro avg       1.00      1.00      1.00        64\n",
            "weighted avg       1.00      1.00      1.00        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 51.56%\n",
            "Precision: 0.75\n",
            "Recall: 0.52\n",
            "F1-Score: 0.37\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [31  1]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      1.00      0.67        32\n",
            "           1       1.00      0.03      0.06        32\n",
            "\n",
            "    accuracy                           0.52        64\n",
            "   macro avg       0.75      0.52      0.37        64\n",
            "weighted avg       0.75      0.52      0.37        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "\n",
            "Random Forest Testing Accuracy: 71.88%\n",
            "Precision: 0.82\n",
            "Recall: 0.72\n",
            "F1-Score: 0.69\n",
            "MCC: 0.53\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [18 14]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      1.00      0.78        32\n",
            "           1       1.00      0.44      0.61        32\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.82      0.72      0.69        64\n",
            "weighted avg       0.82      0.72      0.69        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 87.50%\n",
            "Precision: 0.90\n",
            "Recall: 0.88\n",
            "F1-Score: 0.87\n",
            "MCC: 0.77\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 8 24]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        32\n",
            "           1       1.00      0.75      0.86        32\n",
            "\n",
            "    accuracy                           0.88        64\n",
            "   macro avg       0.90      0.88      0.87        64\n",
            "weighted avg       0.90      0.88      0.87        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 90.62%\n",
            "Precision: 0.92\n",
            "Recall: 0.91\n",
            "F1-Score: 0.91\n",
            "MCC: 0.83\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 6 26]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      1.00      0.91        32\n",
            "           1       1.00      0.81      0.90        32\n",
            "\n",
            "    accuracy                           0.91        64\n",
            "   macro avg       0.92      0.91      0.91        64\n",
            "weighted avg       0.92      0.91      0.91        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 200}\n",
            "\n",
            "Bagging Testing Accuracy: 75.00%\n",
            "Precision: 0.83\n",
            "Recall: 0.75\n",
            "F1-Score: 0.73\n",
            "MCC: 0.58\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [16 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80        32\n",
            "           1       1.00      0.50      0.67        32\n",
            "\n",
            "    accuracy                           0.75        64\n",
            "   macro avg       0.83      0.75      0.73        64\n",
            "weighted avg       0.83      0.75      0.73        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
            "\n",
            "Extra Trees Testing Accuracy: 87.50%\n",
            "Precision: 0.90\n",
            "Recall: 0.88\n",
            "F1-Score: 0.87\n",
            "MCC: 0.77\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 8 24]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        32\n",
            "           1       1.00      0.75      0.86        32\n",
            "\n",
            "    accuracy                           0.88        64\n",
            "   macro avg       0.90      0.88      0.87        64\n",
            "weighted avg       0.90      0.88      0.87        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 89.06%\n",
            "Precision: 0.91\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.80\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 7 25]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      1.00      0.90        32\n",
            "           1       1.00      0.78      0.88        32\n",
            "\n",
            "    accuracy                           0.89        64\n",
            "   macro avg       0.91      0.89      0.89        64\n",
            "weighted avg       0.91      0.89      0.89        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 200}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Random Search CV"
      ],
      "metadata": {
        "id": "wMfqpONYd9ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'PRNP', 'FLNA', 'SPARCL1', 'RHOB', 'PALLD', 'SNCA', 'CSRP1', 'TNS1', 'DPYSL3',\n",
        "                       'TIMP2', 'MEF2C', 'ILK', 'ACTN1', 'TGFB1I1', 'RHOQ', 'CAV2','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "UTkDQ_0DaoZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07bfb17-3011-4a21-9029-b35027c5a399"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 89.06%\n",
            "Precision: 0.91\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.80\n",
            "Confusion Matrix:\n",
            "[[25  7]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.78      0.88        32\n",
            "           1       0.82      1.00      0.90        32\n",
            "\n",
            "    accuracy                           0.89        64\n",
            "   macro avg       0.91      0.89      0.89        64\n",
            "weighted avg       0.91      0.89      0.89        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 51.56%\n",
            "Precision: 0.75\n",
            "Recall: 0.52\n",
            "F1-Score: 0.37\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [31  1]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      1.00      0.67        32\n",
            "           1       1.00      0.03      0.06        32\n",
            "\n",
            "    accuracy                           0.52        64\n",
            "   macro avg       0.75      0.52      0.37        64\n",
            "weighted avg       0.75      0.52      0.37        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 75.00%\n",
            "Precision: 0.83\n",
            "Recall: 0.75\n",
            "F1-Score: 0.73\n",
            "MCC: 0.58\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [16 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80        32\n",
            "           1       1.00      0.50      0.67        32\n",
            "\n",
            "    accuracy                           0.75        64\n",
            "   macro avg       0.83      0.75      0.73        64\n",
            "weighted avg       0.83      0.75      0.73        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 93.75%\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "F1-Score: 0.94\n",
            "MCC: 0.88\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 4 28]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94        32\n",
            "           1       1.00      0.88      0.93        32\n",
            "\n",
            "    accuracy                           0.94        64\n",
            "   macro avg       0.94      0.94      0.94        64\n",
            "weighted avg       0.94      0.94      0.94        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 90.62%\n",
            "Precision: 0.92\n",
            "Recall: 0.91\n",
            "F1-Score: 0.91\n",
            "MCC: 0.83\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 6 26]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      1.00      0.91        32\n",
            "           1       1.00      0.81      0.90        32\n",
            "\n",
            "    accuracy                           0.91        64\n",
            "   macro avg       0.92      0.91      0.91        64\n",
            "weighted avg       0.92      0.91      0.91        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.625), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 71.88%\n",
            "Precision: 0.82\n",
            "Recall: 0.72\n",
            "F1-Score: 0.69\n",
            "MCC: 0.53\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [18 14]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      1.00      0.78        32\n",
            "           1       1.00      0.44      0.61        32\n",
            "\n",
            "    accuracy                           0.72        64\n",
            "   macro avg       0.82      0.72      0.69        64\n",
            "weighted avg       0.82      0.72      0.69        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 78.12%\n",
            "Precision: 0.85\n",
            "Recall: 0.78\n",
            "F1-Score: 0.77\n",
            "MCC: 0.63\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [14 18]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82        32\n",
            "           1       1.00      0.56      0.72        32\n",
            "\n",
            "    accuracy                           0.78        64\n",
            "   macro avg       0.85      0.78      0.77        64\n",
            "weighted avg       0.85      0.78      0.77        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 92.19%\n",
            "Precision: 0.93\n",
            "Recall: 0.92\n",
            "F1-Score: 0.92\n",
            "MCC: 0.85\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 5 27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.93        32\n",
            "           1       1.00      0.84      0.92        32\n",
            "\n",
            "    accuracy                           0.92        64\n",
            "   macro avg       0.93      0.92      0.92        64\n",
            "weighted avg       0.93      0.92      0.92        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.625), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
          ]
        }
      ]
    }
  ]
}