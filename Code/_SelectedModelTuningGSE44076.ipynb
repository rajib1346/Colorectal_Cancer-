{"cells":[{"cell_type":"markdown","metadata":{"id":"b9Wed-y4skm5"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"J_lgF0vQzS1D","outputId":"f2fbf188-21af-4a64-9be8-631be39c1eb1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-5fb2049e-3a8a-4346-952c-048e7bae5806\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5fb2049e-3a8a-4346-952c-048e7bae5806\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ML_DEG_dataset.csv to ML_DEG_dataset.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # Manually upload the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"r2OFHcYKBIrH","outputId":"b818ba0b-aa1c-4eaf-c9e3-720553eeef18"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-62808559-5186-4761-abe7-aeac2890caee\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-62808559-5186-4761-abe7-aeac2890caee\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving GSE44076_gene_expression_renamed.csv to GSE44076_gene_expression_renamed.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # Manually upload the file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eL6e3aFO13-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aacjzmgj7eez","outputId":"3289f38f-e0ba-4f11-a14d-5c8ff8ef8a4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"markdown","metadata":{"id":"__TcBLoYhShL"},"source":["# Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"0_TYrCmtoqQm"},"source":["# MI Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"GGMZm0mpBymN"},"source":["## Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpYoE9zJA1HI","outputId":"011db893-e35f-4017-8212-6b5170a34027"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 97.97%\n","Precision: 0.98\n","Recall: 0.98\n","F1-Score: 0.98\n","MCC: 0.96\n","Confusion Matrix:\n","[[50  0]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97        50\n","           1       1.00      0.97      0.98        98\n","\n","    accuracy                           0.98       148\n","   macro avg       0.97      0.98      0.98       148\n","weighted avg       0.98      0.98      0.98       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 97.30%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.94\n","Confusion Matrix:\n","[[49  1]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.98      0.96        50\n","           1       0.99      0.97      0.98        98\n","\n","    accuracy                           0.97       148\n","   macro avg       0.97      0.97      0.97       148\n","weighted avg       0.97      0.97      0.97       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 65.54%\n","Precision: 0.64\n","Recall: 0.66\n","F1-Score: 0.64\n","MCC: 0.19\n","Confusion Matrix:\n","[[19 31]\n"," [20 78]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.49      0.38      0.43        50\n","           1       0.72      0.80      0.75        98\n","\n","    accuracy                           0.66       148\n","   macro avg       0.60      0.59      0.59       148\n","weighted avg       0.64      0.66      0.64       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 1.0}\n","\n","Gradient Boosting Testing Accuracy: 65.54%\n","Precision: 0.55\n","Recall: 0.66\n","F1-Score: 0.54\n","MCC: -0.00\n","Confusion Matrix:\n","[[ 1 49]\n"," [ 2 96]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.33      0.02      0.04        50\n","           1       0.66      0.98      0.79        98\n","\n","    accuracy                           0.66       148\n","   macro avg       0.50      0.50      0.41       148\n","weighted avg       0.55      0.66      0.54       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 98.65%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.97\n","Confusion Matrix:\n","[[50  0]\n"," [ 2 96]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98        50\n","           1       1.00      0.98      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.98      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 94.59%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.88\n","Confusion Matrix:\n","[[45  5]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.90      0.92        50\n","           1       0.95      0.97      0.96        98\n","\n","    accuracy                           0.95       148\n","   macro avg       0.94      0.93      0.94       148\n","weighted avg       0.95      0.95      0.95       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","\n","Extra Trees Testing Accuracy: 93.24%\n","Precision: 0.93\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.85\n","Confusion Matrix:\n","[[43  7]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.86      0.90        50\n","           1       0.93      0.97      0.95        98\n","\n","    accuracy                           0.93       148\n","   macro avg       0.93      0.91      0.92       148\n","weighted avg       0.93      0.93      0.93       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 74.32%\n","Precision: 0.81\n","Recall: 0.74\n","F1-Score: 0.75\n","MCC: 0.53\n","Confusion Matrix:\n","[[45  5]\n"," [33 65]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.58      0.90      0.70        50\n","           1       0.93      0.66      0.77        98\n","\n","    accuracy                           0.74       148\n","   macro avg       0.75      0.78      0.74       148\n","weighted avg       0.81      0.74      0.75       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 1.0}\n","AdaBoost: {'learning_rate': 0.1, 'n_estimators': 200}\n","Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 200}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['ADH1B', 'CDH3', 'GUCA2B', 'PDE9A', 'ABCA8', 'GUCA2A', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"o47_gTDJCIe3"},"source":["# Random Search CV\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZwC5eOuCF7d","outputId":"a241e0e6-d71c-4be3-d8b2-6e378489996d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","\n","Logistic Regression Testing Accuracy: 98.65%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.97\n","Confusion Matrix:\n","[[50  0]\n"," [ 2 96]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98        50\n","           1       1.00      0.98      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.98      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 90.54%\n","Precision: 0.90\n","Recall: 0.91\n","F1-Score: 0.90\n","MCC: 0.79\n","Confusion Matrix:\n","[[42  8]\n"," [ 6 92]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.84      0.86        50\n","           1       0.92      0.94      0.93        98\n","\n","    accuracy                           0.91       148\n","   macro avg       0.90      0.89      0.89       148\n","weighted avg       0.90      0.91      0.90       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n","\n","Random Forest Testing Accuracy: 86.49%\n","Precision: 0.87\n","Recall: 0.86\n","F1-Score: 0.87\n","MCC: 0.71\n","Confusion Matrix:\n","[[44  6]\n"," [14 84]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.88      0.81        50\n","           1       0.93      0.86      0.89        98\n","\n","    accuracy                           0.86       148\n","   macro avg       0.85      0.87      0.85       148\n","weighted avg       0.87      0.86      0.87       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 10, 'learning_rate': np.float64(0.105)}\n","\n","Gradient Boosting Testing Accuracy: 93.24%\n","Precision: 0.93\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.85\n","Confusion Matrix:\n","[[43  7]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.86      0.90        50\n","           1       0.93      0.97      0.95        98\n","\n","    accuracy                           0.93       148\n","   macro avg       0.93      0.91      0.92       148\n","weighted avg       0.93      0.93      0.93       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(0.2575)}\n","\n","AdaBoost Testing Accuracy: 99.32%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.99\n","Confusion Matrix:\n","[[50  0]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99        50\n","           1       1.00      0.99      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.99      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 91.89%\n","Precision: 0.93\n","Recall: 0.92\n","F1-Score: 0.92\n","MCC: 0.84\n","Confusion Matrix:\n","[[50  0]\n"," [12 86]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.81      1.00      0.89        50\n","           1       1.00      0.88      0.93        98\n","\n","    accuracy                           0.92       148\n","   macro avg       0.90      0.94      0.91       148\n","weighted avg       0.93      0.92      0.92       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 92.57%\n","Precision: 0.93\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.83\n","Confusion Matrix:\n","[[43  7]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.86      0.89        50\n","           1       0.93      0.96      0.94        98\n","\n","    accuracy                           0.93       148\n","   macro avg       0.92      0.91      0.92       148\n","weighted avg       0.93      0.93      0.93       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 74.32%\n","Precision: 0.81\n","Recall: 0.74\n","F1-Score: 0.75\n","MCC: 0.53\n","Confusion Matrix:\n","[[45  5]\n"," [33 65]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.58      0.90      0.70        50\n","           1       0.93      0.66      0.77        98\n","\n","    accuracy                           0.74       148\n","   macro avg       0.75      0.78      0.74       148\n","weighted avg       0.81      0.74      0.75       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 10, 'learning_rate': np.float64(0.105)}\n","AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(0.2575)}\n","Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': False}\n","Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['ADH1B', 'CDH3', 'GUCA2B', 'PDE9A', 'ABCA8', 'GUCA2A', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"t3mUPawDtFh-"},"source":["# Tuning for Hub genes"]},{"cell_type":"markdown","metadata":{"id":"On-eVOfNH2Mq"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3x_YW_pdH1Zz","outputId":"0a63ce0f-c024-409c-eb03-b69ae4e9ed1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 83.78%\n","Precision: 0.84\n","Recall: 0.84\n","F1-Score: 0.84\n","MCC: 0.64\n","Confusion Matrix:\n","[[38 12]\n"," [12 86]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.76      0.76        50\n","           1       0.88      0.88      0.88        98\n","\n","    accuracy                           0.84       148\n","   macro avg       0.82      0.82      0.82       148\n","weighted avg       0.84      0.84      0.84       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 63.51%\n","Precision: 0.43\n","Recall: 0.64\n","F1-Score: 0.51\n","MCC: -0.12\n","Confusion Matrix:\n","[[ 0 50]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00        50\n","           1       0.65      0.96      0.78        98\n","\n","    accuracy                           0.64       148\n","   macro avg       0.33      0.48      0.39       148\n","weighted avg       0.43      0.64      0.51       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 37.84%\n","Precision: 0.78\n","Recall: 0.38\n","F1-Score: 0.25\n","MCC: 0.15\n","Confusion Matrix:\n","[[50  0]\n"," [92  6]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.35      1.00      0.52        50\n","           1       1.00      0.06      0.12        98\n","\n","    accuracy                           0.38       148\n","   macro avg       0.68      0.53      0.32       148\n","weighted avg       0.78      0.38      0.25       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 41.89%\n","Precision: 0.79\n","Recall: 0.42\n","F1-Score: 0.33\n","MCC: 0.21\n","Confusion Matrix:\n","[[50  0]\n"," [86 12]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.37      1.00      0.54        50\n","           1       1.00      0.12      0.22        98\n","\n","    accuracy                           0.42       148\n","   macro avg       0.68      0.56      0.38       148\n","weighted avg       0.79      0.42      0.33       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 55.41%\n","Precision: 0.81\n","Recall: 0.55\n","F1-Score: 0.53\n","MCC: 0.38\n","Confusion Matrix:\n","[[50  0]\n"," [66 32]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.43      1.00      0.60        50\n","           1       1.00      0.33      0.49        98\n","\n","    accuracy                           0.55       148\n","   macro avg       0.72      0.66      0.55       148\n","weighted avg       0.81      0.55      0.53       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 38.51%\n","Precision: 0.78\n","Recall: 0.39\n","F1-Score: 0.27\n","MCC: 0.16\n","Confusion Matrix:\n","[[50  0]\n"," [91  7]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.35      1.00      0.52        50\n","           1       1.00      0.07      0.13        98\n","\n","    accuracy                           0.39       148\n","   macro avg       0.68      0.54      0.33       148\n","weighted avg       0.78      0.39      0.27       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","\n","Extra Trees Testing Accuracy: 61.49%\n","Precision: 0.82\n","Recall: 0.61\n","F1-Score: 0.61\n","MCC: 0.44\n","Confusion Matrix:\n","[[50  0]\n"," [57 41]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.47      1.00      0.64        50\n","           1       1.00      0.42      0.59        98\n","\n","    accuracy                           0.61       148\n","   macro avg       0.73      0.71      0.61       148\n","weighted avg       0.82      0.61      0.61       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 53.38%\n","Precision: 0.80\n","Recall: 0.53\n","F1-Score: 0.50\n","MCC: 0.35\n","Confusion Matrix:\n","[[50  0]\n"," [69 29]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.42      1.00      0.59        50\n","           1       1.00      0.30      0.46        98\n","\n","    accuracy                           0.53       148\n","   macro avg       0.71      0.65      0.52       148\n","weighted avg       0.80      0.53      0.50       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","Random Forest: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n","Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n","                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n","                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"iSMjPZQEIE8Y"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llUEZucpIIPY","outputId":"c7a5fbac-e61c-4084-f042-bb7a825f60bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 0.75, 'C': np.float64(0.001)}\n","\n","Logistic Regression Testing Accuracy: 77.70%\n","Precision: 0.78\n","Recall: 0.78\n","F1-Score: 0.76\n","MCC: 0.48\n","Confusion Matrix:\n","[[22 28]\n"," [ 5 93]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.44      0.57        50\n","           1       0.77      0.95      0.85        98\n","\n","    accuracy                           0.78       148\n","   macro avg       0.79      0.69      0.71       148\n","weighted avg       0.78      0.78      0.76       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 64.19%\n","Precision: 0.43\n","Recall: 0.64\n","F1-Score: 0.52\n","MCC: -0.10\n","Confusion Matrix:\n","[[ 0 50]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00        50\n","           1       0.66      0.97      0.78        98\n","\n","    accuracy                           0.64       148\n","   macro avg       0.33      0.48      0.39       148\n","weighted avg       0.43      0.64      0.52       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10}\n","\n","Random Forest Testing Accuracy: 40.54%\n","Precision: 0.78\n","Recall: 0.41\n","F1-Score: 0.30\n","MCC: 0.19\n","Confusion Matrix:\n","[[50  0]\n"," [88 10]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.36      1.00      0.53        50\n","           1       1.00      0.10      0.19        98\n","\n","    accuracy                           0.41       148\n","   macro avg       0.68      0.55      0.36       148\n","weighted avg       0.78      0.41      0.30       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.105)}\n","\n","Gradient Boosting Testing Accuracy: 47.30%\n","Precision: 0.79\n","Recall: 0.47\n","F1-Score: 0.41\n","MCC: 0.28\n","Confusion Matrix:\n","[[50  0]\n"," [78 20]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.39      1.00      0.56        50\n","           1       1.00      0.20      0.34        98\n","\n","    accuracy                           0.47       148\n","   macro avg       0.70      0.60      0.45       148\n","weighted avg       0.79      0.47      0.41       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n","\n","AdaBoost Testing Accuracy: 55.41%\n","Precision: 0.81\n","Recall: 0.55\n","F1-Score: 0.53\n","MCC: 0.38\n","Confusion Matrix:\n","[[50  0]\n"," [66 32]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.43      1.00      0.60        50\n","           1       1.00      0.33      0.49        98\n","\n","    accuracy                           0.55       148\n","   macro avg       0.72      0.66      0.55       148\n","weighted avg       0.81      0.55      0.53       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.5), 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 38.51%\n","Precision: 0.78\n","Recall: 0.39\n","F1-Score: 0.27\n","MCC: 0.16\n","Confusion Matrix:\n","[[50  0]\n"," [91  7]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.35      1.00      0.52        50\n","           1       1.00      0.07      0.13        98\n","\n","    accuracy                           0.39       148\n","   macro avg       0.68      0.54      0.33       148\n","weighted avg       0.78      0.39      0.27       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n","\n","Extra Trees Testing Accuracy: 57.43%\n","Precision: 0.81\n","Recall: 0.57\n","F1-Score: 0.56\n","MCC: 0.40\n","Confusion Matrix:\n","[[50  0]\n"," [63 35]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.44      1.00      0.61        50\n","           1       1.00      0.36      0.53        98\n","\n","    accuracy                           0.57       148\n","   macro avg       0.72      0.68      0.57       148\n","weighted avg       0.81      0.57      0.56       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n","\n","XGBoost Testing Accuracy: 43.24%\n","Precision: 0.79\n","Recall: 0.43\n","F1-Score: 0.35\n","MCC: 0.23\n","Confusion Matrix:\n","[[50  0]\n"," [84 14]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.37      1.00      0.54        50\n","           1       1.00      0.14      0.25        98\n","\n","    accuracy                           0.43       148\n","   macro avg       0.69      0.57      0.40       148\n","weighted avg       0.79      0.43      0.35       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 0.75, 'C': np.float64(0.001)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.105)}\n","AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n","Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.5), 'bootstrap': True}\n","Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n","                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n","                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=42)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"p2z4CNu14LfE"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC8LQrRG8AdB"},"outputs":[],"source":["['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esk6-wyL8UJX"},"outputs":[],"source":["# Grid Search CV Tuning for Pearson Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeLialgiJQfK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"19geX5E_fES3"},"source":["# PearsonCorrelation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKwT7Z0hJiGg","outputId":"97c409b2-ee7b-4c47-a557-b6f0ea67a583"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 96.62%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.92\n","Confusion Matrix:\n","[[46  4]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.92      0.95        50\n","           1       0.96      0.99      0.97        98\n","\n","    accuracy                           0.97       148\n","   macro avg       0.97      0.95      0.96       148\n","weighted avg       0.97      0.97      0.97       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 89.19%\n","Precision: 0.90\n","Recall: 0.89\n","F1-Score: 0.89\n","MCC: 0.76\n","Confusion Matrix:\n","[[35 15]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.70      0.81        50\n","           1       0.87      0.99      0.92        98\n","\n","    accuracy                           0.89       148\n","   macro avg       0.92      0.84      0.87       148\n","weighted avg       0.90      0.89      0.89       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 76.35%\n","Precision: 0.77\n","Recall: 0.76\n","F1-Score: 0.74\n","MCC: 0.44\n","Confusion Matrix:\n","[[21 29]\n"," [ 6 92]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.42      0.55        50\n","           1       0.76      0.94      0.84        98\n","\n","    accuracy                           0.76       148\n","   macro avg       0.77      0.68      0.69       148\n","weighted avg       0.77      0.76      0.74       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 85.14%\n","Precision: 0.86\n","Recall: 0.85\n","F1-Score: 0.84\n","MCC: 0.66\n","Confusion Matrix:\n","[[31 19]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.62      0.74        50\n","           1       0.83      0.97      0.90        98\n","\n","    accuracy                           0.85       148\n","   macro avg       0.87      0.79      0.82       148\n","weighted avg       0.86      0.85      0.84       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n","\n","AdaBoost Testing Accuracy: 67.57%\n","Precision: 0.66\n","Recall: 0.68\n","F1-Score: 0.59\n","MCC: 0.15\n","Confusion Matrix:\n","[[ 5 45]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.62      0.10      0.17        50\n","           1       0.68      0.97      0.80        98\n","\n","    accuracy                           0.68       148\n","   macro avg       0.65      0.53      0.49       148\n","weighted avg       0.66      0.68      0.59       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 100}\n","\n","Bagging Testing Accuracy: 75.68%\n","Precision: 0.76\n","Recall: 0.76\n","F1-Score: 0.73\n","MCC: 0.42\n","Confusion Matrix:\n","[[21 29]\n"," [ 7 91]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.42      0.54        50\n","           1       0.76      0.93      0.83        98\n","\n","    accuracy                           0.76       148\n","   macro avg       0.75      0.67      0.69       148\n","weighted avg       0.76      0.76      0.73       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n","\n","Extra Trees Testing Accuracy: 75.00%\n","Precision: 0.82\n","Recall: 0.75\n","F1-Score: 0.70\n","MCC: 0.43\n","Confusion Matrix:\n","[[13 37]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.26      0.41        50\n","           1       0.73      1.00      0.84        98\n","\n","    accuracy                           0.75       148\n","   macro avg       0.86      0.63      0.63       148\n","weighted avg       0.82      0.75      0.70       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 72.97%\n","Precision: 0.73\n","Recall: 0.73\n","F1-Score: 0.69\n","MCC: 0.34\n","Confusion Matrix:\n","[[16 34]\n"," [ 6 92]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.32      0.44        50\n","           1       0.73      0.94      0.82        98\n","\n","    accuracy                           0.73       148\n","   macro avg       0.73      0.63      0.63       148\n","weighted avg       0.73      0.73      0.69       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n","Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n","Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 100}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"5A0a9nKfJuOh"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocACFm9GJxiw","outputId":"45c528de-6340-44b9-f9e2-264986c76cb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.0, 'C': np.float64(0.5994842503189409)}\n","\n","Logistic Regression Testing Accuracy: 96.62%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.93\n","Confusion Matrix:\n","[[50  0]\n"," [ 5 93]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      1.00      0.95        50\n","           1       1.00      0.95      0.97        98\n","\n","    accuracy                           0.97       148\n","   macro avg       0.95      0.97      0.96       148\n","weighted avg       0.97      0.97      0.97       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 9, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 87.16%\n","Precision: 0.89\n","Recall: 0.87\n","F1-Score: 0.86\n","MCC: 0.72\n","Confusion Matrix:\n","[[32 18]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.64      0.77        50\n","           1       0.84      0.99      0.91        98\n","\n","    accuracy                           0.87       148\n","   macro avg       0.91      0.81      0.84       148\n","weighted avg       0.89      0.87      0.86       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n","\n","Random Forest Testing Accuracy: 94.59%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.88\n","Confusion Matrix:\n","[[48  2]\n"," [ 6 92]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.96      0.92        50\n","           1       0.98      0.94      0.96        98\n","\n","    accuracy                           0.95       148\n","   macro avg       0.93      0.95      0.94       148\n","weighted avg       0.95      0.95      0.95       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.105)}\n","\n","Gradient Boosting Testing Accuracy: 78.38%\n","Precision: 0.84\n","Recall: 0.78\n","F1-Score: 0.75\n","MCC: 0.52\n","Confusion Matrix:\n","[[18 32]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.36      0.53        50\n","           1       0.75      1.00      0.86        98\n","\n","    accuracy                           0.78       148\n","   macro avg       0.88      0.68      0.69       148\n","weighted avg       0.84      0.78      0.75       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(0.505)}\n","\n","AdaBoost Testing Accuracy: 64.19%\n","Precision: 0.58\n","Recall: 0.64\n","F1-Score: 0.57\n","MCC: 0.04\n","Confusion Matrix:\n","[[ 6 44]\n"," [ 9 89]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.40      0.12      0.18        50\n","           1       0.67      0.91      0.77        98\n","\n","    accuracy                           0.64       148\n","   macro avg       0.53      0.51      0.48       148\n","weighted avg       0.58      0.64      0.57       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 300, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.625), 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 93.92%\n","Precision: 0.95\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.88\n","Confusion Matrix:\n","[[50  0]\n"," [ 9 89]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.85      1.00      0.92        50\n","           1       1.00      0.91      0.95        98\n","\n","    accuracy                           0.94       148\n","   macro avg       0.92      0.95      0.93       148\n","weighted avg       0.95      0.94      0.94       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 71.62%\n","Precision: 0.77\n","Recall: 0.72\n","F1-Score: 0.65\n","MCC: 0.32\n","Confusion Matrix:\n","[[ 9 41]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.18      0.30        50\n","           1       0.70      0.99      0.82        98\n","\n","    accuracy                           0.72       148\n","   macro avg       0.80      0.58      0.56       148\n","weighted avg       0.77      0.72      0.65       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n","\n","XGBoost Testing Accuracy: 73.65%\n","Precision: 0.77\n","Recall: 0.74\n","F1-Score: 0.69\n","MCC: 0.38\n","Confusion Matrix:\n","[[13 37]\n"," [ 2 96]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.26      0.40        50\n","           1       0.72      0.98      0.83        98\n","\n","    accuracy                           0.74       148\n","   macro avg       0.79      0.62      0.62       148\n","weighted avg       0.77      0.74      0.69       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.0, 'C': np.float64(0.5994842503189409)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 9, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.105)}\n","AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(0.505)}\n","Bagging: {'n_estimators': 300, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.625), 'bootstrap': False}\n","Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=42)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9ZofIrx9od7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wvh8hLzfLpAN"},"source":["# Lasso Regression Tuning"]},{"cell_type":"markdown","metadata":{"id":"msVjPMdWKHvh"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5s0I3FFKKhi","outputId":"4bd74e4a-67f2-4525-f8d1-d521db4a70ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 99.32%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.99\n","Confusion Matrix:\n","[[50  0]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99        50\n","           1       1.00      0.99      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.99      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 89.86%\n","Precision: 0.90\n","Recall: 0.90\n","F1-Score: 0.90\n","MCC: 0.77\n","Confusion Matrix:\n","[[39 11]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.78      0.84        50\n","           1       0.90      0.96      0.93        98\n","\n","    accuracy                           0.90       148\n","   macro avg       0.90      0.87      0.88       148\n","weighted avg       0.90      0.90      0.90       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n","\n","Random Forest Testing Accuracy: 72.97%\n","Precision: 0.78\n","Recall: 0.73\n","F1-Score: 0.67\n","MCC: 0.36\n","Confusion Matrix:\n","[[11 39]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.22      0.35        50\n","           1       0.71      0.99      0.83        98\n","\n","    accuracy                           0.73       148\n","   macro avg       0.81      0.60      0.59       148\n","weighted avg       0.78      0.73      0.67       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 72.30%\n","Precision: 0.80\n","Recall: 0.72\n","F1-Score: 0.65\n","MCC: 0.36\n","Confusion Matrix:\n","[[ 9 41]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.18      0.31        50\n","           1       0.71      1.00      0.83        98\n","\n","    accuracy                           0.72       148\n","   macro avg       0.85      0.59      0.57       148\n","weighted avg       0.80      0.72      0.65       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 47.97%\n","Precision: 0.49\n","Recall: 0.48\n","F1-Score: 0.49\n","MCC: -0.14\n","Confusion Matrix:\n","[[14 36]\n"," [41 57]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.25      0.28      0.27        50\n","           1       0.61      0.58      0.60        98\n","\n","    accuracy                           0.48       148\n","   macro avg       0.43      0.43      0.43       148\n","weighted avg       0.49      0.48      0.49       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 69.59%\n","Precision: 0.68\n","Recall: 0.70\n","F1-Score: 0.66\n","MCC: 0.25\n","Confusion Matrix:\n","[[14 36]\n"," [ 9 89]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.61      0.28      0.38        50\n","           1       0.71      0.91      0.80        98\n","\n","    accuracy                           0.70       148\n","   macro avg       0.66      0.59      0.59       148\n","weighted avg       0.68      0.70      0.66       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 64.19%\n","Precision: 0.60\n","Recall: 0.64\n","F1-Score: 0.60\n","MCC: 0.10\n","Confusion Matrix:\n","[[11 39]\n"," [14 84]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.22      0.29        50\n","           1       0.68      0.86      0.76        98\n","\n","    accuracy                           0.64       148\n","   macro avg       0.56      0.54      0.53       148\n","weighted avg       0.60      0.64      0.60       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 72.97%\n","Precision: 0.78\n","Recall: 0.73\n","F1-Score: 0.67\n","MCC: 0.36\n","Confusion Matrix:\n","[[11 39]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.22      0.35        50\n","           1       0.71      0.99      0.83        98\n","\n","    accuracy                           0.73       148\n","   macro avg       0.81      0.60      0.59       148\n","weighted avg       0.78      0.73      0.67       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n","Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 200}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['NR3C2', 'BRCA2', 'AFF3', 'LGALS2', 'TMEM100', 'CDH3', 'ABCG2', 'STMN2', 'ABCA8', 'AZGP1', 'TSPAN7', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"9PR0uZu1KdTM"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTE_l8AqKgQI","outputId":"529470e3-cac1-4f77-c22a-b197019dff6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.01)}\n","\n","Logistic Regression Testing Accuracy: 99.32%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.99\n","Confusion Matrix:\n","[[50  0]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99        50\n","           1       1.00      0.99      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.99      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 89.86%\n","Precision: 0.90\n","Recall: 0.90\n","F1-Score: 0.90\n","MCC: 0.77\n","Confusion Matrix:\n","[[39 11]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.78      0.84        50\n","           1       0.90      0.96      0.93        98\n","\n","    accuracy                           0.90       148\n","   macro avg       0.90      0.87      0.88       148\n","weighted avg       0.90      0.90      0.90       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 75.68%\n","Precision: 0.77\n","Recall: 0.76\n","F1-Score: 0.72\n","MCC: 0.42\n","Confusion Matrix:\n","[[18 32]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.36      0.50        50\n","           1       0.75      0.96      0.84        98\n","\n","    accuracy                           0.76       148\n","   macro avg       0.78      0.66      0.67       148\n","weighted avg       0.77      0.76      0.72       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': 0.2}\n","\n","Gradient Boosting Testing Accuracy: 62.84%\n","Precision: 0.60\n","Recall: 0.63\n","F1-Score: 0.61\n","MCC: 0.11\n","Confusion Matrix:\n","[[15 35]\n"," [20 78]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.30      0.35        50\n","           1       0.69      0.80      0.74        98\n","\n","    accuracy                           0.63       148\n","   macro avg       0.56      0.55      0.55       148\n","weighted avg       0.60      0.63      0.61       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': 1}\n","\n","AdaBoost Testing Accuracy: 47.97%\n","Precision: 0.49\n","Recall: 0.48\n","F1-Score: 0.49\n","MCC: -0.14\n","Confusion Matrix:\n","[[14 36]\n"," [41 57]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.25      0.28      0.27        50\n","           1       0.61      0.58      0.60        98\n","\n","    accuracy                           0.48       148\n","   macro avg       0.43      0.43      0.43       148\n","weighted avg       0.49      0.48      0.49       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 200, 'max_samples': 0.75, 'max_features': 0.5, 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 73.65%\n","Precision: 0.76\n","Recall: 0.74\n","F1-Score: 0.69\n","MCC: 0.37\n","Confusion Matrix:\n","[[14 36]\n"," [ 3 95]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.28      0.42        50\n","           1       0.73      0.97      0.83        98\n","\n","    accuracy                           0.74       148\n","   macro avg       0.77      0.62      0.62       148\n","weighted avg       0.76      0.74      0.69       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 74.32%\n","Precision: 0.81\n","Recall: 0.74\n","F1-Score: 0.69\n","MCC: 0.42\n","Confusion Matrix:\n","[[12 38]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.24      0.39        50\n","           1       0.72      1.00      0.84        98\n","\n","    accuracy                           0.74       148\n","   macro avg       0.86      0.62      0.61       148\n","weighted avg       0.81      0.74      0.69       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 70.95%\n","Precision: 0.70\n","Recall: 0.71\n","F1-Score: 0.66\n","MCC: 0.28\n","Confusion Matrix:\n","[[13 37]\n"," [ 6 92]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.26      0.38        50\n","           1       0.71      0.94      0.81        98\n","\n","    accuracy                           0.71       148\n","   macro avg       0.70      0.60      0.59       148\n","weighted avg       0.70      0.71      0.66       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.01)}\n","K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': 0.2}\n","AdaBoost: {'n_estimators': 200, 'learning_rate': 1}\n","Bagging: {'n_estimators': 200, 'max_samples': 0.75, 'max_features': 0.5, 'bootstrap': False}\n","Extra Trees: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['NR3C2', 'BRCA2', 'AFF3', 'LGALS2', 'TMEM100', 'CDH3', 'ABCG2', 'STMN2', 'ABCA8', 'AZGP1', 'TSPAN7', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy', random_state=42)\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"-mNcsm1UOmIy"},"source":["# mRMR Feature Selection Tuning\n"]},{"cell_type":"markdown","metadata":{"id":"4Llii6ZcK7pw"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rsm2PWBgKyH-","outputId":"064313ab-ba13-4abb-882d-8f715774268a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 99.32%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.99\n","Confusion Matrix:\n","[[50  0]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99        50\n","           1       1.00      0.99      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.99      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 76.35%\n","Precision: 0.83\n","Recall: 0.76\n","F1-Score: 0.72\n","MCC: 0.47\n","Confusion Matrix:\n","[[15 35]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.30      0.46        50\n","           1       0.74      1.00      0.85        98\n","\n","    accuracy                           0.76       148\n","   macro avg       0.87      0.65      0.66       148\n","weighted avg       0.83      0.76      0.72       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 68.92%\n","Precision: 0.79\n","Recall: 0.69\n","F1-Score: 0.59\n","MCC: 0.23\n","Confusion Matrix:\n","[[ 4 46]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.08      0.15        50\n","           1       0.68      1.00      0.81        98\n","\n","    accuracy                           0.69       148\n","   macro avg       0.84      0.54      0.48       148\n","weighted avg       0.79      0.69      0.59       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 87.84%\n","Precision: 0.89\n","Recall: 0.88\n","F1-Score: 0.87\n","MCC: 0.73\n","Confusion Matrix:\n","[[33 17]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.66      0.79        50\n","           1       0.85      0.99      0.92        98\n","\n","    accuracy                           0.88       148\n","   macro avg       0.91      0.82      0.85       148\n","weighted avg       0.89      0.88      0.87       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n","\n","AdaBoost Testing Accuracy: 68.24%\n","Precision: 0.79\n","Recall: 0.68\n","F1-Score: 0.57\n","MCC: 0.20\n","Confusion Matrix:\n","[[ 3 47]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.06      0.11        50\n","           1       0.68      1.00      0.81        98\n","\n","    accuracy                           0.68       148\n","   macro avg       0.84      0.53      0.46       148\n","weighted avg       0.79      0.68      0.57       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n","\n","Bagging Testing Accuracy: 73.65%\n","Precision: 0.81\n","Recall: 0.74\n","F1-Score: 0.67\n","MCC: 0.40\n","Confusion Matrix:\n","[[11 39]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.22      0.36        50\n","           1       0.72      1.00      0.83        98\n","\n","    accuracy                           0.74       148\n","   macro avg       0.86      0.61      0.60       148\n","weighted avg       0.81      0.74      0.67       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 75.00%\n","Precision: 0.82\n","Recall: 0.75\n","F1-Score: 0.70\n","MCC: 0.43\n","Confusion Matrix:\n","[[13 37]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.26      0.41        50\n","           1       0.73      1.00      0.84        98\n","\n","    accuracy                           0.75       148\n","   macro avg       0.86      0.63      0.63       148\n","weighted avg       0.82      0.75      0.70       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 78.38%\n","Precision: 0.84\n","Recall: 0.78\n","F1-Score: 0.75\n","MCC: 0.52\n","Confusion Matrix:\n","[[18 32]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.36      0.53        50\n","           1       0.75      1.00      0.86        98\n","\n","    accuracy                           0.78       148\n","   macro avg       0.88      0.68      0.69       148\n","weighted avg       0.84      0.78      0.75       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n","Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n","Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n","Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['CDH3', 'GNG7', 'GDPD3', 'MMP1', 'ABCA8', 'VSNL1', 'ADH1B', 'AZGP1', 'LGALS2', 'TMEM100', 'DPEP1',\n","                       'AFF3', 'BRCA2', 'MS4A12', 'CENPA', 'LIFR', 'CEP72', 'ABCG2', 'ABI3BP', 'COL10A1', 'MT1M', 'MAD2L1',\n","                       'STMN2', 'NFE2L3', 'SLC4A4', 'KRT23', 'PRR7', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"fbobSA-eLGwl"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grBfkHRHLKAY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"388e62de-9351-45eb-c261-62ec19079d06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.027825594022071243)}\n","\n","Logistic Regression Testing Accuracy: 99.32%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.99\n","Confusion Matrix:\n","[[50  0]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99        50\n","           1       1.00      0.99      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.99      0.99      0.99       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 75.68%\n","Precision: 0.80\n","Recall: 0.76\n","F1-Score: 0.71\n","MCC: 0.44\n","Confusion Matrix:\n","[[15 35]\n"," [ 1 97]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.30      0.45        50\n","           1       0.73      0.99      0.84        98\n","\n","    accuracy                           0.76       148\n","   macro avg       0.84      0.64      0.65       148\n","weighted avg       0.80      0.76      0.71       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}\n","\n","Random Forest Testing Accuracy: 68.92%\n","Precision: 0.79\n","Recall: 0.69\n","F1-Score: 0.59\n","MCC: 0.23\n","Confusion Matrix:\n","[[ 4 46]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.08      0.15        50\n","           1       0.68      1.00      0.81        98\n","\n","    accuracy                           0.69       148\n","   macro avg       0.84      0.54      0.48       148\n","weighted avg       0.79      0.69      0.59       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': 0.2}\n","\n","Gradient Boosting Testing Accuracy: 72.30%\n","Precision: 0.80\n","Recall: 0.72\n","F1-Score: 0.65\n","MCC: 0.36\n","Confusion Matrix:\n","[[ 9 41]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.18      0.31        50\n","           1       0.71      1.00      0.83        98\n","\n","    accuracy                           0.72       148\n","   macro avg       0.85      0.59      0.57       148\n","weighted avg       0.80      0.72      0.65       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 100, 'learning_rate': 0.1}\n","\n","AdaBoost Testing Accuracy: 68.24%\n","Precision: 0.79\n","Recall: 0.68\n","F1-Score: 0.57\n","MCC: 0.20\n","Confusion Matrix:\n","[[ 3 47]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.06      0.11        50\n","           1       0.68      1.00      0.81        98\n","\n","    accuracy                           0.68       148\n","   macro avg       0.84      0.53      0.46       148\n","weighted avg       0.79      0.68      0.57       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 100, 'max_samples': 1.0, 'max_features': 0.5, 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 75.68%\n","Precision: 0.79\n","Recall: 0.76\n","F1-Score: 0.72\n","MCC: 0.43\n","Confusion Matrix:\n","[[16 34]\n"," [ 2 96]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.32      0.47        50\n","           1       0.74      0.98      0.84        98\n","\n","    accuracy                           0.76       148\n","   macro avg       0.81      0.65      0.66       148\n","weighted avg       0.79      0.76      0.72       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Extra Trees Testing Accuracy: 70.95%\n","Precision: 0.80\n","Recall: 0.71\n","F1-Score: 0.63\n","MCC: 0.31\n","Confusion Matrix:\n","[[ 7 43]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.14      0.25        50\n","           1       0.70      1.00      0.82        98\n","\n","    accuracy                           0.71       148\n","   macro avg       0.85      0.57      0.53       148\n","weighted avg       0.80      0.71      0.63       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 79.73%\n","Precision: 0.84\n","Recall: 0.80\n","F1-Score: 0.77\n","MCC: 0.55\n","Confusion Matrix:\n","[[20 30]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.40      0.57        50\n","           1       0.77      1.00      0.87        98\n","\n","    accuracy                           0.80       148\n","   macro avg       0.88      0.70      0.72       148\n","weighted avg       0.84      0.80      0.77       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.027825594022071243)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': 0.2}\n","AdaBoost: {'n_estimators': 100, 'learning_rate': 0.1}\n","Bagging: {'n_estimators': 100, 'max_samples': 1.0, 'max_features': 0.5, 'bootstrap': False}\n","Extra Trees: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['CDH3', 'GNG7', 'GDPD3', 'MMP1', 'ABCA8', 'VSNL1', 'ADH1B', 'AZGP1', 'LGALS2', 'TMEM100', 'DPEP1',\n","                       'AFF3', 'BRCA2', 'MS4A12', 'CENPA', 'LIFR', 'CEP72', 'ABCG2', 'ABI3BP', 'COL10A1', 'MT1M', 'MAD2L1',\n","                       'STMN2', 'NFE2L3', 'SLC4A4', 'KRT23', 'PRR7', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy', random_state=42)\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix2WJa7FfAgG"},"outputs":[],"source":["# GRid Search CV for 15 Dataset"]},{"cell_type":"markdown","metadata":{"id":"ZACfMYOlj2Pr"},"source":["# Elastic Net Feature Selection Tuning"]},{"cell_type":"markdown","metadata":{"id":"p6PWrX_bMkJY"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVGl5Ew1MoIr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be9f0884-59b8-47e9-f11e-e43dc4e2c50c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 97.30%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.94\n","Confusion Matrix:\n","[[50  0]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      1.00      0.96        50\n","           1       1.00      0.96      0.98        98\n","\n","    accuracy                           0.97       148\n","   macro avg       0.96      0.98      0.97       148\n","weighted avg       0.97      0.97      0.97       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 98.65%\n","Precision: 0.99\n","Recall: 0.99\n","F1-Score: 0.99\n","MCC: 0.97\n","Confusion Matrix:\n","[[48  2]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.96      0.98        50\n","           1       0.98      1.00      0.99        98\n","\n","    accuracy                           0.99       148\n","   macro avg       0.99      0.98      0.98       148\n","weighted avg       0.99      0.99      0.99       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 72.30%\n","Precision: 0.80\n","Recall: 0.72\n","F1-Score: 0.65\n","MCC: 0.36\n","Confusion Matrix:\n","[[ 9 41]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.18      0.31        50\n","           1       0.71      1.00      0.83        98\n","\n","    accuracy                           0.72       148\n","   macro avg       0.85      0.59      0.57       148\n","weighted avg       0.80      0.72      0.65       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 1.0}\n","\n","Gradient Boosting Testing Accuracy: 68.24%\n","Precision: 0.79\n","Recall: 0.68\n","F1-Score: 0.57\n","MCC: 0.20\n","Confusion Matrix:\n","[[ 3 47]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.06      0.11        50\n","           1       0.68      1.00      0.81        98\n","\n","    accuracy                           0.68       148\n","   macro avg       0.84      0.53      0.46       148\n","weighted avg       0.79      0.68      0.57       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 50}\n","\n","AdaBoost Testing Accuracy: 81.76%\n","Precision: 0.86\n","Recall: 0.82\n","F1-Score: 0.79\n","MCC: 0.60\n","Confusion Matrix:\n","[[23 27]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.46      0.63        50\n","           1       0.78      1.00      0.88        98\n","\n","    accuracy                           0.82       148\n","   macro avg       0.89      0.73      0.75       148\n","weighted avg       0.86      0.82      0.79       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 50}\n","\n","Bagging Testing Accuracy: 71.62%\n","Precision: 0.80\n","Recall: 0.72\n","F1-Score: 0.64\n","MCC: 0.33\n","Confusion Matrix:\n","[[ 8 42]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.16      0.28        50\n","           1       0.70      1.00      0.82        98\n","\n","    accuracy                           0.72       148\n","   macro avg       0.85      0.58      0.55       148\n","weighted avg       0.80      0.72      0.64       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 72.97%\n","Precision: 0.81\n","Recall: 0.73\n","F1-Score: 0.66\n","MCC: 0.38\n","Confusion Matrix:\n","[[10 40]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.20      0.33        50\n","           1       0.71      1.00      0.83        98\n","\n","    accuracy                           0.73       148\n","   macro avg       0.86      0.60      0.58       148\n","weighted avg       0.81      0.73      0.66       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 72.97%\n","Precision: 0.81\n","Recall: 0.73\n","F1-Score: 0.66\n","MCC: 0.38\n","Confusion Matrix:\n","[[10 40]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.20      0.33        50\n","           1       0.71      1.00      0.83        98\n","\n","    accuracy                           0.73       148\n","   macro avg       0.86      0.60      0.58       148\n","weighted avg       0.81      0.73      0.66       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n","Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 1.0}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 50}\n","Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 50}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['NEBL.1', 'CDH3', 'LGI1', 'BRCA2', 'PTGDS', 'ASPA', 'ABCG2', 'SST', 'TNFRSF11B', 'SLC4A4', 'TAF1A', 'SPINK2', 'PTGDR', 'AFF3', 'DHRS9', 'TG', 'MMP12', 'ADH1B', 'IL23A', 'ACSL6', 'TSPAN7', 'MMP1', 'PDE9A', 'SPINK5', 'CDKN2A','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"N0FQoca8MxEl"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGHnKrB2L5A5","outputId":"beeef356-212e-414f-ee88-c2195b3b2514"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewqHYkoBMzyj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb52c450-3704-4b77-b471-9e0bba83f36d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(0.01)}\n","\n","Logistic Regression Testing Accuracy: 97.30%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.94\n","Confusion Matrix:\n","[[50  0]\n"," [ 4 94]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      1.00      0.96        50\n","           1       1.00      0.96      0.98        98\n","\n","    accuracy                           0.97       148\n","   macro avg       0.96      0.98      0.97       148\n","weighted avg       0.97      0.97      0.97       148\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 96.62%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.93\n","Confusion Matrix:\n","[[45  5]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.90      0.95        50\n","           1       0.95      1.00      0.98        98\n","\n","    accuracy                           0.97       148\n","   macro avg       0.98      0.95      0.96       148\n","weighted avg       0.97      0.97      0.97       148\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Random Forest Testing Accuracy: 95.95%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.91\n","Confusion Matrix:\n","[[44  6]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.88      0.94        50\n","           1       0.94      1.00      0.97        98\n","\n","    accuracy                           0.96       148\n","   macro avg       0.97      0.94      0.95       148\n","weighted avg       0.96      0.96      0.96       148\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.2}\n","\n","Gradient Boosting Testing Accuracy: 68.92%\n","Precision: 0.79\n","Recall: 0.69\n","F1-Score: 0.59\n","MCC: 0.23\n","Confusion Matrix:\n","[[ 4 46]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.08      0.15        50\n","           1       0.68      1.00      0.81        98\n","\n","    accuracy                           0.69       148\n","   macro avg       0.84      0.54      0.48       148\n","weighted avg       0.79      0.69      0.59       148\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 50, 'learning_rate': 1}\n","\n","AdaBoost Testing Accuracy: 81.76%\n","Precision: 0.86\n","Recall: 0.82\n","F1-Score: 0.79\n","MCC: 0.60\n","Confusion Matrix:\n","[[23 27]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.46      0.63        50\n","           1       0.78      1.00      0.88        98\n","\n","    accuracy                           0.82       148\n","   macro avg       0.89      0.73      0.75       148\n","weighted avg       0.86      0.82      0.79       148\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 200, 'max_samples': 0.75, 'max_features': 0.5, 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 77.70%\n","Precision: 0.83\n","Recall: 0.78\n","F1-Score: 0.74\n","MCC: 0.50\n","Confusion Matrix:\n","[[17 33]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.34      0.51        50\n","           1       0.75      1.00      0.86        98\n","\n","    accuracy                           0.78       148\n","   macro avg       0.87      0.67      0.68       148\n","weighted avg       0.83      0.78      0.74       148\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Extra Trees Testing Accuracy: 89.19%\n","Precision: 0.91\n","Recall: 0.89\n","F1-Score: 0.89\n","MCC: 0.76\n","Confusion Matrix:\n","[[34 16]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.68      0.81        50\n","           1       0.86      1.00      0.92        98\n","\n","    accuracy                           0.89       148\n","   macro avg       0.93      0.84      0.87       148\n","weighted avg       0.91      0.89      0.89       148\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 72.97%\n","Precision: 0.81\n","Recall: 0.73\n","F1-Score: 0.66\n","MCC: 0.38\n","Confusion Matrix:\n","[[10 40]\n"," [ 0 98]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.20      0.33        50\n","           1       0.71      1.00      0.83        98\n","\n","    accuracy                           0.73       148\n","   macro avg       0.86      0.60      0.58       148\n","weighted avg       0.81      0.73      0.66       148\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(0.01)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 6, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.2}\n","AdaBoost: {'n_estimators': 50, 'learning_rate': 1}\n","Bagging: {'n_estimators': 200, 'max_samples': 0.75, 'max_features': 0.5, 'bootstrap': True}\n","Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n","from xgboost import XGBClassifier\n","\n","\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest =['NEBL.1', 'CDH3', 'LGI1', 'BRCA2', 'PTGDS', 'ASPA', 'ABCG2', 'SST', 'TNFRSF11B', 'SLC4A4', 'TAF1A', 'SPINK2', 'PTGDR', 'AFF3', 'DHRS9', 'TG', 'MMP12', 'ADH1B', 'IL23A', 'ACSL6', 'TSPAN7', 'MMP1', 'PDE9A', 'SPINK5', 'CDKN2A','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy', random_state=42)\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}