{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHHLy9QKaJCa",
        "outputId": "43bd1d30-768d-466a-9b3e-6ddc6a3d13ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipA6kDmcZbxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Tuning"
      ],
      "metadata": {
        "id": "mv1tPRdxZ3Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Serach CV GSE106582"
      ],
      "metadata": {
        "id": "n61h2KX1aff8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zLpq9kuZ53D",
        "outputId": "cf12b3db-7179-45aa-c2f4-a23f603bdfbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 60.31%\n",
            "Precision: 0.36\n",
            "Recall: 0.60\n",
            "F1-Score: 0.45\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[117   0]\n",
            " [ 77   0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75       117\n",
            "           1       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.60       194\n",
            "   macro avg       0.30      0.50      0.38       194\n",
            "weighted avg       0.36      0.60      0.45       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 47.42%\n",
            "Precision: 0.50\n",
            "Recall: 0.47\n",
            "F1-Score: 0.48\n",
            "MCC: -0.04\n",
            "Confusion Matrix:\n",
            "[[53 64]\n",
            " [38 39]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.45      0.51       117\n",
            "           1       0.38      0.51      0.43        77\n",
            "\n",
            "    accuracy                           0.47       194\n",
            "   macro avg       0.48      0.48      0.47       194\n",
            "weighted avg       0.50      0.47      0.48       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "\n",
            "Random Forest Testing Accuracy: 61.34%\n",
            "Precision: 0.59\n",
            "Recall: 0.61\n",
            "F1-Score: 0.59\n",
            "MCC: 0.14\n",
            "Confusion Matrix:\n",
            "[[95 22]\n",
            " [53 24]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.81      0.72       117\n",
            "           1       0.52      0.31      0.39        77\n",
            "\n",
            "    accuracy                           0.61       194\n",
            "   macro avg       0.58      0.56      0.55       194\n",
            "weighted avg       0.59      0.61      0.59       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 60.82%\n",
            "Precision: 0.59\n",
            "Recall: 0.61\n",
            "F1-Score: 0.59\n",
            "MCC: 0.14\n",
            "Confusion Matrix:\n",
            "[[91 26]\n",
            " [50 27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.78      0.71       117\n",
            "           1       0.51      0.35      0.42        77\n",
            "\n",
            "    accuracy                           0.61       194\n",
            "   macro avg       0.58      0.56      0.56       194\n",
            "weighted avg       0.59      0.61      0.59       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 57.73%\n",
            "Precision: 0.45\n",
            "Recall: 0.58\n",
            "F1-Score: 0.46\n",
            "MCC: -0.08\n",
            "Confusion Matrix:\n",
            "[[110   7]\n",
            " [ 75   2]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.94      0.73       117\n",
            "           1       0.22      0.03      0.05        77\n",
            "\n",
            "    accuracy                           0.58       194\n",
            "   macro avg       0.41      0.48      0.39       194\n",
            "weighted avg       0.45      0.58      0.46       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "\n",
            "Bagging Testing Accuracy: 61.34%\n",
            "Precision: 0.59\n",
            "Recall: 0.61\n",
            "F1-Score: 0.55\n",
            "MCC: 0.11\n",
            "Confusion Matrix:\n",
            "[[106  11]\n",
            " [ 64  13]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.91      0.74       117\n",
            "           1       0.54      0.17      0.26        77\n",
            "\n",
            "    accuracy                           0.61       194\n",
            "   macro avg       0.58      0.54      0.50       194\n",
            "weighted avg       0.59      0.61      0.55       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "\n",
            "Extra Trees Testing Accuracy: 50.00%\n",
            "Precision: 0.53\n",
            "Recall: 0.50\n",
            "F1-Score: 0.50\n",
            "MCC: 0.02\n",
            "Confusion Matrix:\n",
            "[[55 62]\n",
            " [35 42]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.47      0.53       117\n",
            "           1       0.40      0.55      0.46        77\n",
            "\n",
            "    accuracy                           0.50       194\n",
            "   macro avg       0.51      0.51      0.50       194\n",
            "weighted avg       0.53      0.50      0.50       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 62.89%\n",
            "Precision: 0.61\n",
            "Recall: 0.63\n",
            "F1-Score: 0.59\n",
            "MCC: 0.17\n",
            "Confusion Matrix:\n",
            "[[100  17]\n",
            " [ 55  22]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.85      0.74       117\n",
            "           1       0.56      0.29      0.38        77\n",
            "\n",
            "    accuracy                           0.63       194\n",
            "   macro avg       0.60      0.57      0.56       194\n",
            "weighted avg       0.61      0.63      0.59       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Search CV GSE106582"
      ],
      "metadata": {
        "id": "sihKR_2TaZTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_fQ-5TjaSX-",
        "outputId": "6992941a-1694-45a8-fcba-a1ec338b5f9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 60.31%\n",
            "Precision: 0.36\n",
            "Recall: 0.60\n",
            "F1-Score: 0.45\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[117   0]\n",
            " [ 77   0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75       117\n",
            "           1       0.00      0.00      0.00        77\n",
            "\n",
            "    accuracy                           0.60       194\n",
            "   macro avg       0.30      0.50      0.38       194\n",
            "weighted avg       0.36      0.60      0.45       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 42.78%\n",
            "Precision: 0.46\n",
            "Recall: 0.43\n",
            "F1-Score: 0.43\n",
            "MCC: -0.12\n",
            "Confusion Matrix:\n",
            "[[46 71]\n",
            " [40 37]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.39      0.45       117\n",
            "           1       0.34      0.48      0.40        77\n",
            "\n",
            "    accuracy                           0.43       194\n",
            "   macro avg       0.44      0.44      0.43       194\n",
            "weighted avg       0.46      0.43      0.43       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Random Forest Testing Accuracy: 61.86%\n",
            "Precision: 0.60\n",
            "Recall: 0.62\n",
            "F1-Score: 0.57\n",
            "MCC: 0.14\n",
            "Confusion Matrix:\n",
            "[[102  15]\n",
            " [ 59  18]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.87      0.73       117\n",
            "           1       0.55      0.23      0.33        77\n",
            "\n",
            "    accuracy                           0.62       194\n",
            "   macro avg       0.59      0.55      0.53       194\n",
            "weighted avg       0.60      0.62      0.57       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 10, 'max_depth': 7, 'learning_rate': np.float64(0.2)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 62.89%\n",
            "Precision: 0.61\n",
            "Recall: 0.63\n",
            "F1-Score: 0.61\n",
            "MCC: 0.19\n",
            "Confusion Matrix:\n",
            "[[94 23]\n",
            " [49 28]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.80      0.72       117\n",
            "           1       0.55      0.36      0.44        77\n",
            "\n",
            "    accuracy                           0.63       194\n",
            "   macro avg       0.60      0.58      0.58       194\n",
            "weighted avg       0.61      0.63      0.61       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 59.79%\n",
            "Precision: 0.55\n",
            "Recall: 0.60\n",
            "F1-Score: 0.50\n",
            "MCC: 0.04\n",
            "Confusion Matrix:\n",
            "[[110   7]\n",
            " [ 71   6]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.94      0.74       117\n",
            "           1       0.46      0.08      0.13        77\n",
            "\n",
            "    accuracy                           0.60       194\n",
            "   macro avg       0.53      0.51      0.44       194\n",
            "weighted avg       0.55      0.60      0.50       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.75), 'bootstrap': False}\n",
            "\n",
            "Bagging Testing Accuracy: 51.55%\n",
            "Precision: 0.52\n",
            "Recall: 0.52\n",
            "F1-Score: 0.52\n",
            "MCC: -0.01\n",
            "Confusion Matrix:\n",
            "[[70 47]\n",
            " [47 30]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.60      0.60       117\n",
            "           1       0.39      0.39      0.39        77\n",
            "\n",
            "    accuracy                           0.52       194\n",
            "   macro avg       0.49      0.49      0.49       194\n",
            "weighted avg       0.52      0.52      0.52       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 59.28%\n",
            "Precision: 0.59\n",
            "Recall: 0.59\n",
            "F1-Score: 0.59\n",
            "MCC: 0.15\n",
            "Confusion Matrix:\n",
            "[[77 40]\n",
            " [39 38]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.66      0.66       117\n",
            "           1       0.49      0.49      0.49        77\n",
            "\n",
            "    accuracy                           0.59       194\n",
            "   macro avg       0.58      0.58      0.58       194\n",
            "weighted avg       0.59      0.59      0.59       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 61.34%\n",
            "Precision: 0.62\n",
            "Recall: 0.61\n",
            "F1-Score: 0.50\n",
            "MCC: 0.10\n",
            "Confusion Matrix:\n",
            "[[114   3]\n",
            " [ 72   5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.97      0.75       117\n",
            "           1       0.62      0.06      0.12        77\n",
            "\n",
            "    accuracy                           0.61       194\n",
            "   macro avg       0.62      0.52      0.44       194\n",
            "weighted avg       0.62      0.61      0.50       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 10, 'max_depth': 7, 'learning_rate': np.float64(0.2)}\n",
            "AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.75), 'bootstrap': False}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Grid Search CV"
      ],
      "metadata": {
        "id": "xHOEnOf3aznk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k3URklZan5c",
        "outputId": "7a262818-dad0-48f9-b26e-e664f6d87159"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 93.62%\n",
            "Precision: 0.88\n",
            "Recall: 0.94\n",
            "F1-Score: 0.91\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[  0   9]\n",
            " [  0 132]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.94      1.00      0.97       132\n",
            "\n",
            "    accuracy                           0.94       141\n",
            "   macro avg       0.47      0.50      0.48       141\n",
            "weighted avg       0.88      0.94      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 92.91%\n",
            "Precision: 0.90\n",
            "Recall: 0.93\n",
            "F1-Score: 0.91\n",
            "MCC: 0.16\n",
            "Confusion Matrix:\n",
            "[[  1   8]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.11      0.17         9\n",
            "           1       0.94      0.98      0.96       132\n",
            "\n",
            "    accuracy                           0.93       141\n",
            "   macro avg       0.64      0.55      0.56       141\n",
            "weighted avg       0.90      0.93      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 93.62%\n",
            "Precision: 0.92\n",
            "Recall: 0.94\n",
            "F1-Score: 0.92\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.22      0.31         9\n",
            "           1       0.95      0.98      0.97       132\n",
            "\n",
            "    accuracy                           0.94       141\n",
            "   macro avg       0.72      0.60      0.64       141\n",
            "weighted avg       0.92      0.94      0.92       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 87.23%\n",
            "Precision: 0.91\n",
            "Recall: 0.87\n",
            "F1-Score: 0.89\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.44      0.31         9\n",
            "           1       0.96      0.90      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.60      0.67      0.62       141\n",
            "weighted avg       0.91      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 92.91%\n",
            "Precision: 0.90\n",
            "Recall: 0.93\n",
            "F1-Score: 0.91\n",
            "MCC: 0.16\n",
            "Confusion Matrix:\n",
            "[[  1   8]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.11      0.17         9\n",
            "           1       0.94      0.98      0.96       132\n",
            "\n",
            "    accuracy                           0.93       141\n",
            "   macro avg       0.64      0.55      0.56       141\n",
            "weighted avg       0.90      0.93      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 92.20%\n",
            "Precision: 0.91\n",
            "Recall: 0.92\n",
            "F1-Score: 0.91\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  4 128]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.22      0.27         9\n",
            "           1       0.95      0.97      0.96       132\n",
            "\n",
            "    accuracy                           0.92       141\n",
            "   macro avg       0.64      0.60      0.61       141\n",
            "weighted avg       0.91      0.92      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Extra Trees Testing Accuracy: 92.91%\n",
            "Precision: 0.91\n",
            "Recall: 0.93\n",
            "F1-Score: 0.92\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  3 129]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.22      0.29         9\n",
            "           1       0.95      0.98      0.96       132\n",
            "\n",
            "    accuracy                           0.93       141\n",
            "   macro avg       0.67      0.60      0.62       141\n",
            "weighted avg       0.91      0.93      0.92       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 93.62%\n",
            "Precision: 0.92\n",
            "Recall: 0.94\n",
            "F1-Score: 0.92\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.22      0.31         9\n",
            "           1       0.95      0.98      0.97       132\n",
            "\n",
            "    accuracy                           0.94       141\n",
            "   macro avg       0.72      0.60      0.64       141\n",
            "weighted avg       0.92      0.94      0.92       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Random SearchCV"
      ],
      "metadata": {
        "id": "SG4ZHqbubCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "IMDhs1_9aoDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e983ba45-3303-49bb-e51b-3c67c8a418df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 93.62%\n",
            "Precision: 0.88\n",
            "Recall: 0.94\n",
            "F1-Score: 0.91\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[  0   9]\n",
            " [  0 132]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.94      1.00      0.97       132\n",
            "\n",
            "    accuracy                           0.94       141\n",
            "   macro avg       0.47      0.50      0.48       141\n",
            "weighted avg       0.88      0.94      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 89.36%\n",
            "Precision: 0.89\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.06\n",
            "Confusion Matrix:\n",
            "[[  1   8]\n",
            " [  7 125]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.11      0.12         9\n",
            "           1       0.94      0.95      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.53      0.53      0.53       141\n",
            "weighted avg       0.89      0.89      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 92.20%\n",
            "Precision: 0.91\n",
            "Recall: 0.92\n",
            "F1-Score: 0.91\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  4 128]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.22      0.27         9\n",
            "           1       0.95      0.97      0.96       132\n",
            "\n",
            "    accuracy                           0.92       141\n",
            "   macro avg       0.64      0.60      0.61       141\n",
            "weighted avg       0.91      0.92      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.0575)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 91.49%\n",
            "Precision: 0.91\n",
            "Recall: 0.91\n",
            "F1-Score: 0.91\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  3   6]\n",
            " [  6 126]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.33      0.33         9\n",
            "           1       0.95      0.95      0.95       132\n",
            "\n",
            "    accuracy                           0.91       141\n",
            "   macro avg       0.64      0.64      0.64       141\n",
            "weighted avg       0.91      0.91      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 92.91%\n",
            "Precision: 0.90\n",
            "Recall: 0.93\n",
            "F1-Score: 0.91\n",
            "MCC: 0.16\n",
            "Confusion Matrix:\n",
            "[[  1   8]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.11      0.17         9\n",
            "           1       0.94      0.98      0.96       132\n",
            "\n",
            "    accuracy                           0.93       141\n",
            "   macro avg       0.64      0.55      0.56       141\n",
            "weighted avg       0.90      0.93      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.75), 'bootstrap': False}\n",
            "\n",
            "Bagging Testing Accuracy: 92.20%\n",
            "Precision: 0.88\n",
            "Recall: 0.92\n",
            "F1-Score: 0.90\n",
            "MCC: -0.03\n",
            "Confusion Matrix:\n",
            "[[  0   9]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.94      0.98      0.96       132\n",
            "\n",
            "    accuracy                           0.92       141\n",
            "   macro avg       0.47      0.49      0.48       141\n",
            "weighted avg       0.88      0.92      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 92.91%\n",
            "Precision: 0.91\n",
            "Recall: 0.93\n",
            "F1-Score: 0.92\n",
            "MCC: 0.26\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  3 129]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.22      0.29         9\n",
            "           1       0.95      0.98      0.96       132\n",
            "\n",
            "    accuracy                           0.93       141\n",
            "   macro avg       0.67      0.60      0.62       141\n",
            "weighted avg       0.91      0.93      0.92       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 92.20%\n",
            "Precision: 0.91\n",
            "Recall: 0.92\n",
            "F1-Score: 0.91\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[  2   7]\n",
            " [  4 128]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.22      0.27         9\n",
            "           1       0.95      0.97      0.96       132\n",
            "\n",
            "    accuracy                           0.92       141\n",
            "   macro avg       0.64      0.60      0.61       141\n",
            "weighted avg       0.91      0.92      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 10, 'max_depth': 5, 'learning_rate': np.float64(0.0575)}\n",
            "AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.75), 'bootstrap': False}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Grid Serach CV"
      ],
      "metadata": {
        "id": "xUgGf7Ebc0Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "wk70MoTBaoL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2ee2e4-a4b2-4d1e-9b6d-657ca4750649"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 37.16%\n",
            "Precision: 0.78\n",
            "Recall: 0.37\n",
            "F1-Score: 0.24\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [93  5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.05      0.10        98\n",
            "\n",
            "    accuracy                           0.37       148\n",
            "   macro avg       0.67      0.53      0.31       148\n",
            "weighted avg       0.78      0.37      0.24       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 62.84%\n",
            "Precision: 0.65\n",
            "Recall: 0.63\n",
            "F1-Score: 0.64\n",
            "MCC: 0.22\n",
            "Confusion Matrix:\n",
            "[[29 21]\n",
            " [34 64]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.58      0.51        50\n",
            "           1       0.75      0.65      0.70        98\n",
            "\n",
            "    accuracy                           0.63       148\n",
            "   macro avg       0.61      0.62      0.61       148\n",
            "weighted avg       0.65      0.63      0.64       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "\n",
            "Random Forest Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "\n",
            "Bagging Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Extra Trees Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Random Search CV"
      ],
      "metadata": {
        "id": "k4wmwmK6dYQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "E6ZKORzjaoQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe51300a-f21d-42d7-b805-2d712fcba412"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 78.38%\n",
            "Precision: 0.78\n",
            "Recall: 0.78\n",
            "F1-Score: 0.78\n",
            "MCC: 0.50\n",
            "Confusion Matrix:\n",
            "[[28 22]\n",
            " [10 88]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.56      0.64        50\n",
            "           1       0.80      0.90      0.85        98\n",
            "\n",
            "    accuracy                           0.78       148\n",
            "   macro avg       0.77      0.73      0.74       148\n",
            "weighted avg       0.78      0.78      0.78       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Random Forest Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 250, 'max_samples': np.float64(0.625), 'max_features': np.float64(1.0), 'bootstrap': False}\n",
            "\n",
            "Bagging Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Extra Trees Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 33.78%\n",
            "Precision: 0.11\n",
            "Recall: 0.34\n",
            "F1-Score: 0.17\n",
            "MCC: 0.00\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 250, 'max_samples': np.float64(0.625), 'max_features': np.float64(1.0), 'bootstrap': False}\n",
            "Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Grid Search CV"
      ],
      "metadata": {
        "id": "ySVQ3aO5do3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "86yX9vwpaoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d180d4da-730c-477e-9af0-038889f8be9f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 53.12%\n",
            "Precision: 0.76\n",
            "Recall: 0.53\n",
            "F1-Score: 0.40\n",
            "MCC: 0.18\n",
            "Confusion Matrix:\n",
            "[[ 2 30]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.06      0.12        32\n",
            "           1       0.52      1.00      0.68        32\n",
            "\n",
            "    accuracy                           0.53        64\n",
            "   macro avg       0.76      0.53      0.40        64\n",
            "weighted avg       0.76      0.53      0.40        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 96.88%\n",
            "Precision: 0.97\n",
            "Recall: 0.97\n",
            "F1-Score: 0.97\n",
            "MCC: 0.94\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 1 31]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.97      0.97      0.97        32\n",
            "\n",
            "    accuracy                           0.97        64\n",
            "   macro avg       0.97      0.97      0.97        64\n",
            "weighted avg       0.97      0.97      0.97        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 64.06%\n",
            "Precision: 0.75\n",
            "Recall: 0.64\n",
            "F1-Score: 0.60\n",
            "MCC: 0.37\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [22 10]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.97      0.73        32\n",
            "           1       0.91      0.31      0.47        32\n",
            "\n",
            "    accuracy                           0.64        64\n",
            "   macro avg       0.75      0.64      0.60        64\n",
            "weighted avg       0.75      0.64      0.60        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 54.69%\n",
            "Precision: 0.76\n",
            "Recall: 0.55\n",
            "F1-Score: 0.43\n",
            "MCC: 0.22\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [29  3]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      1.00      0.69        32\n",
            "           1       1.00      0.09      0.17        32\n",
            "\n",
            "    accuracy                           0.55        64\n",
            "   macro avg       0.76      0.55      0.43        64\n",
            "weighted avg       0.76      0.55      0.43        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 79.69%\n",
            "Precision: 0.82\n",
            "Recall: 0.80\n",
            "F1-Score: 0.79\n",
            "MCC: 0.62\n",
            "Confusion Matrix:\n",
            "[[30  2]\n",
            " [11 21]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.82        32\n",
            "           1       0.91      0.66      0.76        32\n",
            "\n",
            "    accuracy                           0.80        64\n",
            "   macro avg       0.82      0.80      0.79        64\n",
            "weighted avg       0.82      0.80      0.79        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 60.94%\n",
            "Precision: 0.73\n",
            "Recall: 0.61\n",
            "F1-Score: 0.55\n",
            "MCC: 0.31\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [24  8]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.97      0.71        32\n",
            "           1       0.89      0.25      0.39        32\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.73      0.61      0.55        64\n",
            "weighted avg       0.73      0.61      0.55        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Extra Trees Testing Accuracy: 90.62%\n",
            "Precision: 0.91\n",
            "Recall: 0.91\n",
            "F1-Score: 0.91\n",
            "MCC: 0.82\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 5 27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91        32\n",
            "           1       0.96      0.84      0.90        32\n",
            "\n",
            "    accuracy                           0.91        64\n",
            "   macro avg       0.91      0.91      0.91        64\n",
            "weighted avg       0.91      0.91      0.91        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 73.44%\n",
            "Precision: 0.80\n",
            "Recall: 0.73\n",
            "F1-Score: 0.72\n",
            "MCC: 0.53\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [16 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.97      0.78        32\n",
            "           1       0.94      0.50      0.65        32\n",
            "\n",
            "    accuracy                           0.73        64\n",
            "   macro avg       0.80      0.73      0.72        64\n",
            "weighted avg       0.80      0.73      0.72        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Random Search CV"
      ],
      "metadata": {
        "id": "wMfqpONYd9ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['PTRF', 'FLNA', 'RHOB', 'TNS1', 'ACTN1', 'TGFB1I1', 'RHOQ','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "UTkDQ_0DaoZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2821c85-3ce7-41fb-a30a-a756d31dc914"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 54.69%\n",
            "Precision: 0.76\n",
            "Recall: 0.55\n",
            "F1-Score: 0.43\n",
            "MCC: 0.22\n",
            "Confusion Matrix:\n",
            "[[ 3 29]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.09      0.17        32\n",
            "           1       0.52      1.00      0.69        32\n",
            "\n",
            "    accuracy                           0.55        64\n",
            "   macro avg       0.76      0.55      0.43        64\n",
            "weighted avg       0.76      0.55      0.43        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 76.56%\n",
            "Precision: 0.82\n",
            "Recall: 0.77\n",
            "F1-Score: 0.76\n",
            "MCC: 0.58\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [14 18]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.97      0.81        32\n",
            "           1       0.95      0.56      0.71        32\n",
            "\n",
            "    accuracy                           0.77        64\n",
            "   macro avg       0.82      0.77      0.76        64\n",
            "weighted avg       0.82      0.77      0.76        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
            "\n",
            "Random Forest Testing Accuracy: 67.19%\n",
            "Precision: 0.77\n",
            "Recall: 0.67\n",
            "F1-Score: 0.64\n",
            "MCC: 0.43\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [20 12]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.97      0.75        32\n",
            "           1       0.92      0.38      0.53        32\n",
            "\n",
            "    accuracy                           0.67        64\n",
            "   macro avg       0.77      0.67      0.64        64\n",
            "weighted avg       0.77      0.67      0.64        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 68.75%\n",
            "Precision: 0.81\n",
            "Recall: 0.69\n",
            "F1-Score: 0.65\n",
            "MCC: 0.48\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [20 12]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.76        32\n",
            "           1       1.00      0.38      0.55        32\n",
            "\n",
            "    accuracy                           0.69        64\n",
            "   macro avg       0.81      0.69      0.65        64\n",
            "weighted avg       0.81      0.69      0.65        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 79.69%\n",
            "Precision: 0.80\n",
            "Recall: 0.80\n",
            "F1-Score: 0.80\n",
            "MCC: 0.59\n",
            "Confusion Matrix:\n",
            "[[26  6]\n",
            " [ 7 25]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.81      0.80        32\n",
            "           1       0.81      0.78      0.79        32\n",
            "\n",
            "    accuracy                           0.80        64\n",
            "   macro avg       0.80      0.80      0.80        64\n",
            "weighted avg       0.80      0.80      0.80        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.625), 'bootstrap': False}\n",
            "\n",
            "Bagging Testing Accuracy: 57.81%\n",
            "Precision: 0.77\n",
            "Recall: 0.58\n",
            "F1-Score: 0.49\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [27  5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      1.00      0.70        32\n",
            "           1       1.00      0.16      0.27        32\n",
            "\n",
            "    accuracy                           0.58        64\n",
            "   macro avg       0.77      0.58      0.49        64\n",
            "weighted avg       0.77      0.58      0.49        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
            "\n",
            "Extra Trees Testing Accuracy: 75.00%\n",
            "Precision: 0.81\n",
            "Recall: 0.75\n",
            "F1-Score: 0.74\n",
            "MCC: 0.56\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [15 17]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.97      0.79        32\n",
            "           1       0.94      0.53      0.68        32\n",
            "\n",
            "    accuracy                           0.75        64\n",
            "   macro avg       0.81      0.75      0.74        64\n",
            "weighted avg       0.81      0.75      0.74        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 76.56%\n",
            "Precision: 0.77\n",
            "Recall: 0.77\n",
            "F1-Score: 0.77\n",
            "MCC: 0.53\n",
            "Confusion Matrix:\n",
            "[[25  7]\n",
            " [ 8 24]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.78      0.77        32\n",
            "           1       0.77      0.75      0.76        32\n",
            "\n",
            "    accuracy                           0.77        64\n",
            "   macro avg       0.77      0.77      0.77        64\n",
            "weighted avg       0.77      0.77      0.77        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 1000, 'l1_ratio': 0.25, 'C': np.float64(7.742636826811277)}\n",
            "K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 50, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.625), 'bootstrap': False}\n",
            "Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n",
            "XGBoost: {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
          ]
        }
      ]
    }
  ]
}