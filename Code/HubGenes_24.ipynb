{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GSE106582"
      ],
      "metadata": {
        "id": "IeAhQKN7YdAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFJRgR3bWRS4",
        "outputId": "fccced07-4161-4778-9504-c7dd6fe7cf5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Training Accuracy: 90.74%\n",
            "LR Testing Accuracy: 92.27%\n",
            "Precision: 0.92, Recall: 0.92, F1-Score: 0.92, MCC: 0.84\n",
            "LR Testing Accuracy: 92.27%\n",
            "Confusion Matrix:\n",
            "[[108   9]\n",
            " [  6  71]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.92      0.94       117\n",
            "           1       0.89      0.92      0.90        77\n",
            "\n",
            "    accuracy                           0.92       194\n",
            "   macro avg       0.92      0.92      0.92       194\n",
            "weighted avg       0.92      0.92      0.92       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "KNN Training Accuracy: 83.14%\n",
            "KNN Testing Accuracy: 51.55%\n",
            "Precision: 0.72, Recall: 0.52, F1-Score: 0.46, MCC: 0.25\n",
            "KNN Testing Accuracy: 51.55%\n",
            "Confusion Matrix:\n",
            "[[26 91]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.22      0.36       117\n",
            "           1       0.45      0.96      0.61        77\n",
            "\n",
            "    accuracy                           0.52       194\n",
            "   macro avg       0.67      0.59      0.48       194\n",
            "weighted avg       0.72      0.52      0.46       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Random Forest Training Accuracy: 84.33%\n",
            "Random Forest Testing Accuracy: 74.23%\n",
            "Precision: 0.75, Recall: 0.74, F1-Score: 0.74, MCC: 0.48\n",
            "Random Forest Testing Accuracy: 74.23%\n",
            "Confusion Matrix:\n",
            "[[86 31]\n",
            " [19 58]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.74      0.77       117\n",
            "           1       0.65      0.75      0.70        77\n",
            "\n",
            "    accuracy                           0.74       194\n",
            "   macro avg       0.74      0.74      0.74       194\n",
            "weighted avg       0.75      0.74      0.74       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Gradient Boosting Training Accuracy: 87.57%\n",
            "Gradient Boosting Testing Accuracy: 76.80%\n",
            "Precision: 0.77, Recall: 0.77, F1-Score: 0.77, MCC: 0.51\n",
            "Gradient Boosting Testing Accuracy: 76.80%\n",
            "Confusion Matrix:\n",
            "[[99 18]\n",
            " [27 50]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.81       117\n",
            "           1       0.74      0.65      0.69        77\n",
            "\n",
            "    accuracy                           0.77       194\n",
            "   macro avg       0.76      0.75      0.75       194\n",
            "weighted avg       0.77      0.77      0.77       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "AdaBoost Training Accuracy: 82.96%\n",
            "AdaBoost Testing Accuracy: 68.56%\n",
            "Precision: 0.77, Recall: 0.69, F1-Score: 0.62, MCC: 0.36\n",
            "AdaBoost Testing Accuracy: 68.56%\n",
            "Confusion Matrix:\n",
            "[[116   1]\n",
            " [ 60  17]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.99      0.79       117\n",
            "           1       0.94      0.22      0.36        77\n",
            "\n",
            "    accuracy                           0.69       194\n",
            "   macro avg       0.80      0.61      0.57       194\n",
            "weighted avg       0.77      0.69      0.62       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Bagging Training Accuracy: 82.08%\n",
            "Bagging Testing Accuracy: 66.49%\n",
            "Precision: 0.71, Recall: 0.66, F1-Score: 0.60, MCC: 0.28\n",
            "Bagging Testing Accuracy: 66.49%\n",
            "Confusion Matrix:\n",
            "[[113   4]\n",
            " [ 61  16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.97      0.78       117\n",
            "           1       0.80      0.21      0.33        77\n",
            "\n",
            "    accuracy                           0.66       194\n",
            "   macro avg       0.72      0.59      0.55       194\n",
            "weighted avg       0.71      0.66      0.60       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Extra Trees Training Accuracy: 87.51%\n",
            "Extra Trees Testing Accuracy: 47.42%\n",
            "Precision: 0.69, Recall: 0.47, F1-Score: 0.39, MCC: 0.18\n",
            "Extra Trees Testing Accuracy: 47.42%\n",
            "Confusion Matrix:\n",
            "[[18 99]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.15      0.26       117\n",
            "           1       0.43      0.96      0.59        77\n",
            "\n",
            "    accuracy                           0.47       194\n",
            "   macro avg       0.64      0.56      0.43       194\n",
            "weighted avg       0.69      0.47      0.39       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "XGBoost Training Accuracy: 86.98%\n",
            "XGBoost Testing Accuracy: 74.23%\n",
            "Precision: 0.75, Recall: 0.74, F1-Score: 0.74, MCC: 0.48\n",
            "XGBoost Testing Accuracy: 74.23%\n",
            "Confusion Matrix:\n",
            "[[87 30]\n",
            " [20 57]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.74      0.78       117\n",
            "           1       0.66      0.74      0.70        77\n",
            "\n",
            "    accuracy                           0.74       194\n",
            "   macro avg       0.73      0.74      0.74       194\n",
            "weighted avg       0.75      0.74      0.74       194\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, StackingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "path = \"ML_DEG_dataset.csv\"\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "path = \"GSE44076_gene_expression_renamed.csv\"\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "\n",
        "# Validation dataset\n",
        "validation_size = 0.30\n",
        "seed = 7\n",
        "\n",
        "# Define base models\n",
        "models = {\n",
        "    'LR': LogisticRegression(max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost':XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models.items():\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    cv_results = cross_val_score(model, X_train_common, y_train, cv=kfold, scoring='accuracy')\n",
        "    print(f\"{name} Training Accuracy: {cv_results.mean() * 100:.2f}%\")\n",
        "\n",
        "    # Train and test each model\n",
        "    model.fit(X_train_common, y_train)\n",
        "    predictions = model.predict(X_test_common)\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name} Testing Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, MCC: {mcc:.2f}\")\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815"
      ],
      "metadata": {
        "id": "grjcAmeFY83U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, StackingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "path = \"ML_DEG_dataset.csv\"\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "path = \"GSE44076_gene_expression_renamed.csv\"\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "\n",
        "# Validation dataset\n",
        "validation_size = 0.30\n",
        "seed = 7\n",
        "\n",
        "# Define base models\n",
        "models = {\n",
        "    'LR': LogisticRegression(max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost':XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models.items():\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    cv_results = cross_val_score(model, X_train_common, y_train, cv=kfold, scoring='accuracy')\n",
        "    print(f\"{name} Training Accuracy: {cv_results.mean() * 100:.2f}%\")\n",
        "\n",
        "    # Train and test each model\n",
        "    model.fit(X_train_common, y_train)\n",
        "    predictions = model.predict(X_test_common)\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name} Testing Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, MCC: {mcc:.2f}\")\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFMWGZkjYmPk",
        "outputId": "a2f663b3-6874-4ee0-f5ea-42391d4266f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Training Accuracy: 90.74%\n",
            "LR Testing Accuracy: 95.04%\n",
            "Precision: 0.94, Recall: 0.95, F1-Score: 0.94, MCC: 0.48\n",
            "LR Testing Accuracy: 95.04%\n",
            "Confusion Matrix:\n",
            "[[  3   6]\n",
            " [  1 131]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.33      0.46         9\n",
            "           1       0.96      0.99      0.97       132\n",
            "\n",
            "    accuracy                           0.95       141\n",
            "   macro avg       0.85      0.66      0.72       141\n",
            "weighted avg       0.94      0.95      0.94       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "KNN Training Accuracy: 83.14%\n",
            "KNN Testing Accuracy: 85.11%\n",
            "Precision: 0.90, Recall: 0.85, F1-Score: 0.87, MCC: 0.16\n",
            "KNN Testing Accuracy: 85.11%\n",
            "Confusion Matrix:\n",
            "[[  3   6]\n",
            " [ 15 117]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.33      0.22         9\n",
            "           1       0.95      0.89      0.92       132\n",
            "\n",
            "    accuracy                           0.85       141\n",
            "   macro avg       0.56      0.61      0.57       141\n",
            "weighted avg       0.90      0.85      0.87       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Random Forest Training Accuracy: 86.09%\n",
            "Random Forest Testing Accuracy: 87.94%\n",
            "Precision: 0.91, Recall: 0.88, F1-Score: 0.89, MCC: 0.27\n",
            "Random Forest Testing Accuracy: 87.94%\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 12 120]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.44      0.32         9\n",
            "           1       0.96      0.91      0.93       132\n",
            "\n",
            "    accuracy                           0.88       141\n",
            "   macro avg       0.60      0.68      0.63       141\n",
            "weighted avg       0.91      0.88      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Gradient Boosting Training Accuracy: 87.58%\n",
            "Gradient Boosting Testing Accuracy: 84.40%\n",
            "Precision: 0.91, Recall: 0.84, F1-Score: 0.87, MCC: 0.22\n",
            "Gradient Boosting Testing Accuracy: 84.40%\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.44      0.27         9\n",
            "           1       0.96      0.87      0.91       132\n",
            "\n",
            "    accuracy                           0.84       141\n",
            "   macro avg       0.57      0.66      0.59       141\n",
            "weighted avg       0.91      0.84      0.87       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "AdaBoost Training Accuracy: 82.96%\n",
            "AdaBoost Testing Accuracy: 87.23%\n",
            "Precision: 0.91, Recall: 0.87, F1-Score: 0.89, MCC: 0.26\n",
            "AdaBoost Testing Accuracy: 87.23%\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.44      0.31         9\n",
            "           1       0.96      0.90      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.60      0.67      0.62       141\n",
            "weighted avg       0.91      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Bagging Training Accuracy: 84.39%\n",
            "Bagging Testing Accuracy: 91.49%\n",
            "Precision: 0.91, Recall: 0.91, F1-Score: 0.91, MCC: 0.29\n",
            "Bagging Testing Accuracy: 91.49%\n",
            "Confusion Matrix:\n",
            "[[  3   6]\n",
            " [  6 126]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.33      0.33         9\n",
            "           1       0.95      0.95      0.95       132\n",
            "\n",
            "    accuracy                           0.91       141\n",
            "   macro avg       0.64      0.64      0.64       141\n",
            "weighted avg       0.91      0.91      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Extra Trees Training Accuracy: 89.24%\n",
            "Extra Trees Testing Accuracy: 90.78%\n",
            "Precision: 0.92, Recall: 0.91, F1-Score: 0.91, MCC: 0.34\n",
            "Extra Trees Testing Accuracy: 90.78%\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  8 124]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.44      0.38         9\n",
            "           1       0.96      0.94      0.95       132\n",
            "\n",
            "    accuracy                           0.91       141\n",
            "   macro avg       0.65      0.69      0.67       141\n",
            "weighted avg       0.92      0.91      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "XGBoost Training Accuracy: 86.98%\n",
            "XGBoost Testing Accuracy: 90.07%\n",
            "Precision: 0.92, Recall: 0.90, F1-Score: 0.91, MCC: 0.32\n",
            "XGBoost Testing Accuracy: 90.07%\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  9 123]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.44      0.36         9\n",
            "           1       0.96      0.93      0.95       132\n",
            "\n",
            "    accuracy                           0.90       141\n",
            "   macro avg       0.63      0.69      0.65       141\n",
            "weighted avg       0.92      0.90      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076"
      ],
      "metadata": {
        "id": "wgRzmHjlZrAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, StackingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "path = \"ML_DEG_dataset.csv\"\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "path = \"GSE44076_gene_expression_renamed.csv\"\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "\n",
        "# Validation dataset\n",
        "validation_size = 0.30\n",
        "seed = 7\n",
        "\n",
        "# Define base models\n",
        "models = {\n",
        "    'LR': LogisticRegression(max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost':XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models.items():\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    cv_results = cross_val_score(model, X_train_common, y_train, cv=kfold, scoring='accuracy')\n",
        "    print(f\"{name} Training Accuracy: {cv_results.mean() * 100:.2f}%\")\n",
        "\n",
        "    # Train and test each model\n",
        "    model.fit(X_train_common, y_train)\n",
        "    predictions = model.predict(X_test_common)\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name} Testing Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, MCC: {mcc:.2f}\")\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXTu0cOhZG9c",
        "outputId": "f591565e-2352-419c-86c6-3240eb1dc8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Training Accuracy: 90.74%\n",
            "LR Testing Accuracy: 68.92%\n",
            "Precision: 0.79, Recall: 0.69, F1-Score: 0.59, MCC: 0.23\n",
            "LR Testing Accuracy: 68.92%\n",
            "Confusion Matrix:\n",
            "[[ 4 46]\n",
            " [ 0 98]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.08      0.15        50\n",
            "           1       0.68      1.00      0.81        98\n",
            "\n",
            "    accuracy                           0.69       148\n",
            "   macro avg       0.84      0.54      0.48       148\n",
            "weighted avg       0.79      0.69      0.59       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "KNN Training Accuracy: 83.14%\n",
            "KNN Testing Accuracy: 64.19%\n",
            "Precision: 0.43, Recall: 0.64, F1-Score: 0.52, MCC: -0.10\n",
            "KNN Testing Accuracy: 64.19%\n",
            "Confusion Matrix:\n",
            "[[ 0 50]\n",
            " [ 3 95]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        50\n",
            "           1       0.66      0.97      0.78        98\n",
            "\n",
            "    accuracy                           0.64       148\n",
            "   macro avg       0.33      0.48      0.39       148\n",
            "weighted avg       0.43      0.64      0.52       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Random Forest Training Accuracy: 84.62%\n",
            "Random Forest Testing Accuracy: 41.22%\n",
            "Precision: 0.79, Recall: 0.41, F1-Score: 0.31, MCC: 0.20\n",
            "Random Forest Testing Accuracy: 41.22%\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [87 11]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.11      0.20        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.68      0.56      0.37       148\n",
            "weighted avg       0.79      0.41      0.31       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Gradient Boosting Training Accuracy: 87.58%\n",
            "Gradient Boosting Testing Accuracy: 41.89%\n",
            "Precision: 0.79, Recall: 0.42, F1-Score: 0.33, MCC: 0.21\n",
            "Gradient Boosting Testing Accuracy: 41.89%\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [86 12]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.12      0.22        98\n",
            "\n",
            "    accuracy                           0.42       148\n",
            "   macro avg       0.68      0.56      0.38       148\n",
            "weighted avg       0.79      0.42      0.33       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "AdaBoost Training Accuracy: 82.96%\n",
            "AdaBoost Testing Accuracy: 53.38%\n",
            "Precision: 0.80, Recall: 0.53, F1-Score: 0.50, MCC: 0.35\n",
            "AdaBoost Testing Accuracy: 53.38%\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [69 29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      1.00      0.59        50\n",
            "           1       1.00      0.30      0.46        98\n",
            "\n",
            "    accuracy                           0.53       148\n",
            "   macro avg       0.71      0.65      0.52       148\n",
            "weighted avg       0.80      0.53      0.50       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Bagging Training Accuracy: 83.50%\n",
            "Bagging Testing Accuracy: 33.78%\n",
            "Precision: 0.11, Recall: 0.34, F1-Score: 0.17, MCC: 0.00\n",
            "Bagging Testing Accuracy: 33.78%\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [98  0]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      1.00      0.51        50\n",
            "           1       0.00      0.00      0.00        98\n",
            "\n",
            "    accuracy                           0.34       148\n",
            "   macro avg       0.17      0.50      0.25       148\n",
            "weighted avg       0.11      0.34      0.17       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Extra Trees Training Accuracy: 87.21%\n",
            "Extra Trees Testing Accuracy: 75.00%\n",
            "Precision: 0.79, Recall: 0.75, F1-Score: 0.76, MCC: 0.52\n",
            "Extra Trees Testing Accuracy: 75.00%\n",
            "Confusion Matrix:\n",
            "[[42  8]\n",
            " [29 69]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.84      0.69        50\n",
            "           1       0.90      0.70      0.79        98\n",
            "\n",
            "    accuracy                           0.75       148\n",
            "   macro avg       0.74      0.77      0.74       148\n",
            "weighted avg       0.79      0.75      0.76       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "XGBoost Training Accuracy: 86.98%\n",
            "XGBoost Testing Accuracy: 40.54%\n",
            "Precision: 0.69, Recall: 0.41, F1-Score: 0.32, MCC: 0.13\n",
            "XGBoost Testing Accuracy: 40.54%\n",
            "Confusion Matrix:\n",
            "[[48  2]\n",
            " [86 12]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.96      0.52        50\n",
            "           1       0.86      0.12      0.21        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.61      0.54      0.37       148\n",
            "weighted avg       0.69      0.41      0.32       148\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ipA6kDmcZbxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671"
      ],
      "metadata": {
        "id": "l_JPA_FHZceG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, StackingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "path = \"ML_DEG_dataset.csv\"\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "path = \"GSE44076_gene_expression_renamed.csv\"\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "\n",
        "# Validation dataset\n",
        "validation_size = 0.30\n",
        "seed = 7\n",
        "\n",
        "# Define base models\n",
        "models = {\n",
        "    'LR': LogisticRegression(max_iter=1000),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost':XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models.items():\n",
        "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    cv_results = cross_val_score(model, X_train_common, y_train, cv=kfold, scoring='accuracy')\n",
        "    print(f\"{name} Training Accuracy: {cv_results.mean() * 100:.2f}%\")\n",
        "\n",
        "    # Train and test each model\n",
        "    model.fit(X_train_common, y_train)\n",
        "    predictions = model.predict(X_test_common)\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{name} Testing Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, MCC: {mcc:.2f}\")\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZGzcJRbZPuD",
        "outputId": "8ac6c75b-cbdc-4270-fa97-79787be1bdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Training Accuracy: 90.74%\n",
            "LR Testing Accuracy: 100.00%\n",
            "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, MCC: 1.00\n",
            "LR Testing Accuracy: 100.00%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        32\n",
            "           1       1.00      1.00      1.00        32\n",
            "\n",
            "    accuracy                           1.00        64\n",
            "   macro avg       1.00      1.00      1.00        64\n",
            "weighted avg       1.00      1.00      1.00        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "KNN Training Accuracy: 83.14%\n",
            "KNN Testing Accuracy: 100.00%\n",
            "Precision: 1.00, Recall: 1.00, F1-Score: 1.00, MCC: 1.00\n",
            "KNN Testing Accuracy: 100.00%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        32\n",
            "           1       1.00      1.00      1.00        32\n",
            "\n",
            "    accuracy                           1.00        64\n",
            "   macro avg       1.00      1.00      1.00        64\n",
            "weighted avg       1.00      1.00      1.00        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Random Forest Training Accuracy: 86.66%\n",
            "Random Forest Testing Accuracy: 60.94%\n",
            "Precision: 0.78, Recall: 0.61, F1-Score: 0.54, MCC: 0.35\n",
            "Random Forest Testing Accuracy: 60.94%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [25  7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72        32\n",
            "           1       1.00      0.22      0.36        32\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.78      0.61      0.54        64\n",
            "weighted avg       0.78      0.61      0.54        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Gradient Boosting Training Accuracy: 86.99%\n",
            "Gradient Boosting Testing Accuracy: 89.06%\n",
            "Precision: 0.91, Recall: 0.89, F1-Score: 0.89, MCC: 0.80\n",
            "Gradient Boosting Testing Accuracy: 89.06%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 7 25]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      1.00      0.90        32\n",
            "           1       1.00      0.78      0.88        32\n",
            "\n",
            "    accuracy                           0.89        64\n",
            "   macro avg       0.91      0.89      0.89        64\n",
            "weighted avg       0.91      0.89      0.89        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "AdaBoost Training Accuracy: 82.96%\n",
            "AdaBoost Testing Accuracy: 90.62%\n",
            "Precision: 0.91, Recall: 0.91, F1-Score: 0.91, MCC: 0.82\n",
            "AdaBoost Testing Accuracy: 90.62%\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 5 27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91        32\n",
            "           1       0.96      0.84      0.90        32\n",
            "\n",
            "    accuracy                           0.91        64\n",
            "   macro avg       0.91      0.91      0.91        64\n",
            "weighted avg       0.91      0.91      0.91        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Bagging Training Accuracy: 81.76%\n",
            "Bagging Testing Accuracy: 60.94%\n",
            "Precision: 0.78, Recall: 0.61, F1-Score: 0.54, MCC: 0.35\n",
            "Bagging Testing Accuracy: 60.94%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [25  7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72        32\n",
            "           1       1.00      0.22      0.36        32\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.78      0.61      0.54        64\n",
            "weighted avg       0.78      0.61      0.54        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Extra Trees Training Accuracy: 87.24%\n",
            "Extra Trees Testing Accuracy: 78.12%\n",
            "Precision: 0.85, Recall: 0.78, F1-Score: 0.77, MCC: 0.63\n",
            "Extra Trees Testing Accuracy: 78.12%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [14 18]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82        32\n",
            "           1       1.00      0.56      0.72        32\n",
            "\n",
            "    accuracy                           0.78        64\n",
            "   macro avg       0.85      0.78      0.77        64\n",
            "weighted avg       0.85      0.78      0.77        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "XGBoost Training Accuracy: 86.98%\n",
            "XGBoost Testing Accuracy: 81.25%\n",
            "Precision: 0.86, Recall: 0.81, F1-Score: 0.81, MCC: 0.67\n",
            "XGBoost Testing Accuracy: 81.25%\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [12 20]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      1.00      0.84        32\n",
            "           1       1.00      0.62      0.77        32\n",
            "\n",
            "    accuracy                           0.81        64\n",
            "   macro avg       0.86      0.81      0.81        64\n",
            "weighted avg       0.86      0.81      0.81        64\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Tuning"
      ],
      "metadata": {
        "id": "mv1tPRdxZ3Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Serach CV GSE106582"
      ],
      "metadata": {
        "id": "n61h2KX1aff8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zLpq9kuZ53D",
        "outputId": "0f6e8bcb-9045-4a98-ff29-369fb14334cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 77.32%\n",
            "Precision: 0.82\n",
            "Recall: 0.77\n",
            "F1-Score: 0.75\n",
            "MCC: 0.54\n",
            "Confusion Matrix:\n",
            "[[115   2]\n",
            " [ 42  35]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.98      0.84       117\n",
            "           1       0.95      0.45      0.61        77\n",
            "\n",
            "    accuracy                           0.77       194\n",
            "   macro avg       0.84      0.72      0.73       194\n",
            "weighted avg       0.82      0.77      0.75       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 48.97%\n",
            "Precision: 0.70\n",
            "Recall: 0.49\n",
            "F1-Score: 0.42\n",
            "MCC: 0.21\n",
            "Confusion Matrix:\n",
            "[[21 96]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.18      0.30       117\n",
            "           1       0.44      0.96      0.60        77\n",
            "\n",
            "    accuracy                           0.49       194\n",
            "   macro avg       0.66      0.57      0.45       194\n",
            "weighted avg       0.70      0.49      0.42       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Random Forest Testing Accuracy: 74.23%\n",
            "Precision: 0.76\n",
            "Recall: 0.74\n",
            "F1-Score: 0.75\n",
            "MCC: 0.50\n",
            "Confusion Matrix:\n",
            "[[82 35]\n",
            " [15 62]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.70      0.77       117\n",
            "           1       0.64      0.81      0.71        77\n",
            "\n",
            "    accuracy                           0.74       194\n",
            "   macro avg       0.74      0.75      0.74       194\n",
            "weighted avg       0.76      0.74      0.75       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 80.41%\n",
            "Precision: 0.80\n",
            "Recall: 0.80\n",
            "F1-Score: 0.80\n",
            "MCC: 0.59\n",
            "Confusion Matrix:\n",
            "[[102  15]\n",
            " [ 23  54]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       117\n",
            "           1       0.78      0.70      0.74        77\n",
            "\n",
            "    accuracy                           0.80       194\n",
            "   macro avg       0.80      0.79      0.79       194\n",
            "weighted avg       0.80      0.80      0.80       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 85.57%\n",
            "Precision: 0.86\n",
            "Recall: 0.86\n",
            "F1-Score: 0.85\n",
            "MCC: 0.70\n",
            "Confusion Matrix:\n",
            "[[109   8]\n",
            " [ 20  57]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.89       117\n",
            "           1       0.88      0.74      0.80        77\n",
            "\n",
            "    accuracy                           0.86       194\n",
            "   macro avg       0.86      0.84      0.84       194\n",
            "weighted avg       0.86      0.86      0.85       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 50}\n",
            "\n",
            "Bagging Testing Accuracy: 72.16%\n",
            "Precision: 0.74\n",
            "Recall: 0.72\n",
            "F1-Score: 0.70\n",
            "MCC: 0.41\n",
            "Confusion Matrix:\n",
            "[[109   8]\n",
            " [ 46  31]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.93      0.80       117\n",
            "           1       0.79      0.40      0.53        77\n",
            "\n",
            "    accuracy                           0.72       194\n",
            "   macro avg       0.75      0.67      0.67       194\n",
            "weighted avg       0.74      0.72      0.70       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Extra Trees Testing Accuracy: 51.55%\n",
            "Precision: 0.72\n",
            "Recall: 0.52\n",
            "F1-Score: 0.46\n",
            "MCC: 0.25\n",
            "Confusion Matrix:\n",
            "[[26 91]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.22      0.36       117\n",
            "           1       0.45      0.96      0.61        77\n",
            "\n",
            "    accuracy                           0.52       194\n",
            "   macro avg       0.67      0.59      0.48       194\n",
            "weighted avg       0.72      0.52      0.46       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 78.87%\n",
            "Precision: 0.80\n",
            "Recall: 0.79\n",
            "F1-Score: 0.79\n",
            "MCC: 0.57\n",
            "Confusion Matrix:\n",
            "[[91 26]\n",
            " [15 62]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.78      0.82       117\n",
            "           1       0.70      0.81      0.75        77\n",
            "\n",
            "    accuracy                           0.79       194\n",
            "   macro avg       0.78      0.79      0.78       194\n",
            "weighted avg       0.80      0.79      0.79       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': True, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 50}\n",
            "Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Search CV GSE106582"
      ],
      "metadata": {
        "id": "sihKR_2TaZTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_fQ-5TjaSX-",
        "outputId": "bc3a8732-46f9-49e1-fbb8-203f0cf2648d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 0.75, 'C': np.float64(100.0)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 77.32%\n",
            "Precision: 0.82\n",
            "Recall: 0.77\n",
            "F1-Score: 0.75\n",
            "MCC: 0.54\n",
            "Confusion Matrix:\n",
            "[[115   2]\n",
            " [ 42  35]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.98      0.84       117\n",
            "           1       0.95      0.45      0.61        77\n",
            "\n",
            "    accuracy                           0.77       194\n",
            "   macro avg       0.84      0.72      0.73       194\n",
            "weighted avg       0.82      0.77      0.75       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 49.48%\n",
            "Precision: 0.69\n",
            "Recall: 0.49\n",
            "F1-Score: 0.43\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[23 94]\n",
            " [ 4 73]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.20      0.32       117\n",
            "           1       0.44      0.95      0.60        77\n",
            "\n",
            "    accuracy                           0.49       194\n",
            "   macro avg       0.64      0.57      0.46       194\n",
            "weighted avg       0.69      0.49      0.43       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 77.32%\n",
            "Precision: 0.77\n",
            "Recall: 0.77\n",
            "F1-Score: 0.77\n",
            "MCC: 0.53\n",
            "Confusion Matrix:\n",
            "[[95 22]\n",
            " [22 55]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81       117\n",
            "           1       0.71      0.71      0.71        77\n",
            "\n",
            "    accuracy                           0.77       194\n",
            "   macro avg       0.76      0.76      0.76       194\n",
            "weighted avg       0.77      0.77      0.77       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 7, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 63.92%\n",
            "Precision: 0.73\n",
            "Recall: 0.64\n",
            "F1-Score: 0.63\n",
            "MCC: 0.37\n",
            "Confusion Matrix:\n",
            "[[56 61]\n",
            " [ 9 68]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.48      0.62       117\n",
            "           1       0.53      0.88      0.66        77\n",
            "\n",
            "    accuracy                           0.64       194\n",
            "   macro avg       0.69      0.68      0.64       194\n",
            "weighted avg       0.73      0.64      0.63       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 84.54%\n",
            "Precision: 0.85\n",
            "Recall: 0.85\n",
            "F1-Score: 0.84\n",
            "MCC: 0.67\n",
            "Confusion Matrix:\n",
            "[[106  11]\n",
            " [ 19  58]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       117\n",
            "           1       0.84      0.75      0.79        77\n",
            "\n",
            "    accuracy                           0.85       194\n",
            "   macro avg       0.84      0.83      0.84       194\n",
            "weighted avg       0.85      0.85      0.84       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.75), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 74.74%\n",
            "Precision: 0.81\n",
            "Recall: 0.75\n",
            "F1-Score: 0.71\n",
            "MCC: 0.50\n",
            "Confusion Matrix:\n",
            "[[116   1]\n",
            " [ 48  29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.99      0.83       117\n",
            "           1       0.97      0.38      0.54        77\n",
            "\n",
            "    accuracy                           0.75       194\n",
            "   macro avg       0.84      0.68      0.68       194\n",
            "weighted avg       0.81      0.75      0.71       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "\n",
            "Extra Trees Testing Accuracy: 47.42%\n",
            "Precision: 0.69\n",
            "Recall: 0.47\n",
            "F1-Score: 0.39\n",
            "MCC: 0.18\n",
            "Confusion Matrix:\n",
            "[[18 99]\n",
            " [ 3 74]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.15      0.26       117\n",
            "           1       0.43      0.96      0.59        77\n",
            "\n",
            "    accuracy                           0.47       194\n",
            "   macro avg       0.64      0.56      0.43       194\n",
            "weighted avg       0.69      0.47      0.39       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 64.43%\n",
            "Precision: 0.68\n",
            "Recall: 0.64\n",
            "F1-Score: 0.65\n",
            "MCC: 0.31\n",
            "Confusion Matrix:\n",
            "[[69 48]\n",
            " [21 56]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.59      0.67       117\n",
            "           1       0.54      0.73      0.62        77\n",
            "\n",
            "    accuracy                           0.64       194\n",
            "   macro avg       0.65      0.66      0.64       194\n",
            "weighted avg       0.68      0.64      0.65       194\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 0.75, 'C': np.float64(100.0)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 7, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.75), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Grid Search CV"
      ],
      "metadata": {
        "id": "xHOEnOf3aznk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k3URklZan5c",
        "outputId": "864f0bb3-4587-49a8-c267-9c7689d80612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 95.04%\n",
            "Precision: 0.94\n",
            "Recall: 0.95\n",
            "F1-Score: 0.95\n",
            "MCC: 0.52\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.44      0.53         9\n",
            "           1       0.96      0.98      0.97       132\n",
            "\n",
            "    accuracy                           0.95       141\n",
            "   macro avg       0.81      0.71      0.75       141\n",
            "weighted avg       0.94      0.95      0.95       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 70.92%\n",
            "Precision: 0.92\n",
            "Recall: 0.71\n",
            "F1-Score: 0.78\n",
            "MCC: 0.20\n",
            "Confusion Matrix:\n",
            "[[ 6  3]\n",
            " [38 94]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.67      0.23         9\n",
            "           1       0.97      0.71      0.82       132\n",
            "\n",
            "    accuracy                           0.71       141\n",
            "   macro avg       0.55      0.69      0.52       141\n",
            "weighted avg       0.92      0.71      0.78       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Random Forest Testing Accuracy: 85.82%\n",
            "Precision: 0.92\n",
            "Recall: 0.86\n",
            "F1-Score: 0.88\n",
            "MCC: 0.30\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 16 116]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.56      0.33         9\n",
            "           1       0.97      0.88      0.92       132\n",
            "\n",
            "    accuracy                           0.86       141\n",
            "   macro avg       0.60      0.72      0.63       141\n",
            "weighted avg       0.92      0.86      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 88.65%\n",
            "Precision: 0.93\n",
            "Recall: 0.89\n",
            "F1-Score: 0.90\n",
            "MCC: 0.41\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43         9\n",
            "           1       0.98      0.90      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.65      0.78      0.68       141\n",
            "weighted avg       0.93      0.89      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 87.94%\n",
            "Precision: 0.92\n",
            "Recall: 0.88\n",
            "F1-Score: 0.90\n",
            "MCC: 0.33\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.56      0.37         9\n",
            "           1       0.97      0.90      0.93       132\n",
            "\n",
            "    accuracy                           0.88       141\n",
            "   macro avg       0.62      0.73      0.65       141\n",
            "weighted avg       0.92      0.88      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 86.52%\n",
            "Precision: 0.91\n",
            "Recall: 0.87\n",
            "F1-Score: 0.89\n",
            "MCC: 0.25\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 14 118]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.44      0.30         9\n",
            "           1       0.96      0.89      0.93       132\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.59      0.67      0.61       141\n",
            "weighted avg       0.91      0.87      0.89       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Extra Trees Testing Accuracy: 90.07%\n",
            "Precision: 0.92\n",
            "Recall: 0.90\n",
            "F1-Score: 0.91\n",
            "MCC: 0.32\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  9 123]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.44      0.36         9\n",
            "           1       0.96      0.93      0.95       132\n",
            "\n",
            "    accuracy                           0.90       141\n",
            "   macro avg       0.63      0.69      0.65       141\n",
            "weighted avg       0.92      0.90      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}\n",
            "\n",
            "XGBoost Testing Accuracy: 88.65%\n",
            "Precision: 0.92\n",
            "Recall: 0.89\n",
            "F1-Score: 0.90\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 12 120]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.56      0.38         9\n",
            "           1       0.97      0.91      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.63      0.73      0.66       141\n",
            "weighted avg       0.92      0.89      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE21815 Random SearchCV"
      ],
      "metadata": {
        "id": "SG4ZHqbubCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE21815_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "IMDhs1_9aoDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41aec55-d51d-4d7a-f7bd-ccfb48a3ed03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 95.04%\n",
            "Precision: 0.94\n",
            "Recall: 0.95\n",
            "F1-Score: 0.95\n",
            "MCC: 0.52\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  2 130]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.44      0.53         9\n",
            "           1       0.96      0.98      0.97       132\n",
            "\n",
            "    accuracy                           0.95       141\n",
            "   macro avg       0.81      0.71      0.75       141\n",
            "weighted avg       0.94      0.95      0.95       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 88.65%\n",
            "Precision: 0.92\n",
            "Recall: 0.89\n",
            "F1-Score: 0.90\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [ 11 121]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.44      0.33         9\n",
            "           1       0.96      0.92      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.61      0.68      0.64       141\n",
            "weighted avg       0.92      0.89      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 90.07%\n",
            "Precision: 0.92\n",
            "Recall: 0.90\n",
            "F1-Score: 0.91\n",
            "MCC: 0.32\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  9 123]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.44      0.36         9\n",
            "           1       0.96      0.93      0.95       132\n",
            "\n",
            "    accuracy                           0.90       141\n",
            "   macro avg       0.63      0.69      0.65       141\n",
            "weighted avg       0.92      0.90      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 7, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 89.36%\n",
            "Precision: 0.93\n",
            "Recall: 0.89\n",
            "F1-Score: 0.91\n",
            "MCC: 0.42\n",
            "Confusion Matrix:\n",
            "[[  6   3]\n",
            " [ 12 120]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.67      0.44         9\n",
            "           1       0.98      0.91      0.94       132\n",
            "\n",
            "    accuracy                           0.89       141\n",
            "   macro avg       0.65      0.79      0.69       141\n",
            "weighted avg       0.93      0.89      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 87.94%\n",
            "Precision: 0.92\n",
            "Recall: 0.88\n",
            "F1-Score: 0.90\n",
            "MCC: 0.33\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 13 119]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.56      0.37         9\n",
            "           1       0.97      0.90      0.93       132\n",
            "\n",
            "    accuracy                           0.88       141\n",
            "   macro avg       0.62      0.73      0.65       141\n",
            "weighted avg       0.92      0.88      0.90       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.75), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 84.40%\n",
            "Precision: 0.92\n",
            "Recall: 0.84\n",
            "F1-Score: 0.87\n",
            "MCC: 0.28\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 18 114]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.56      0.31         9\n",
            "           1       0.97      0.86      0.91       132\n",
            "\n",
            "    accuracy                           0.84       141\n",
            "   macro avg       0.59      0.71      0.61       141\n",
            "weighted avg       0.92      0.84      0.87       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 90.78%\n",
            "Precision: 0.92\n",
            "Recall: 0.91\n",
            "F1-Score: 0.91\n",
            "MCC: 0.34\n",
            "Confusion Matrix:\n",
            "[[  4   5]\n",
            " [  8 124]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.44      0.38         9\n",
            "           1       0.96      0.94      0.95       132\n",
            "\n",
            "    accuracy                           0.91       141\n",
            "   macro avg       0.65      0.69      0.67       141\n",
            "weighted avg       0.92      0.91      0.91       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 85.11%\n",
            "Precision: 0.92\n",
            "Recall: 0.85\n",
            "F1-Score: 0.88\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[  5   4]\n",
            " [ 17 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.56      0.32         9\n",
            "           1       0.97      0.87      0.92       132\n",
            "\n",
            "    accuracy                           0.85       141\n",
            "   macro avg       0.60      0.71      0.62       141\n",
            "weighted avg       0.92      0.85      0.88       141\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 2, 'max_depth': 7, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.625), 'max_features': np.float64(0.75), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Grid Serach CV"
      ],
      "metadata": {
        "id": "xUgGf7Ebc0Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "wk70MoTBaoL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c50221-4487-469a-c7a4-dbac7a4ea2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 82.43%\n",
            "Precision: 0.83\n",
            "Recall: 0.82\n",
            "F1-Score: 0.83\n",
            "MCC: 0.61\n",
            "Confusion Matrix:\n",
            "[[38 12]\n",
            " [14 84]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.76      0.75        50\n",
            "           1       0.88      0.86      0.87        98\n",
            "\n",
            "    accuracy                           0.82       148\n",
            "   macro avg       0.80      0.81      0.81       148\n",
            "weighted avg       0.83      0.82      0.83       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 63.51%\n",
            "Precision: 0.43\n",
            "Recall: 0.64\n",
            "F1-Score: 0.51\n",
            "MCC: -0.12\n",
            "Confusion Matrix:\n",
            "[[ 0 50]\n",
            " [ 4 94]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        50\n",
            "           1       0.65      0.96      0.78        98\n",
            "\n",
            "    accuracy                           0.64       148\n",
            "   macro avg       0.33      0.48      0.39       148\n",
            "weighted avg       0.43      0.64      0.51       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Random Forest Testing Accuracy: 40.54%\n",
            "Precision: 0.78\n",
            "Recall: 0.41\n",
            "F1-Score: 0.30\n",
            "MCC: 0.19\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [88 10]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      1.00      0.53        50\n",
            "           1       1.00      0.10      0.19        98\n",
            "\n",
            "    accuracy                           0.41       148\n",
            "   macro avg       0.68      0.55      0.36       148\n",
            "weighted avg       0.78      0.41      0.30       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 47.30%\n",
            "Precision: 0.79\n",
            "Recall: 0.47\n",
            "F1-Score: 0.41\n",
            "MCC: 0.28\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [78 20]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      1.00      0.56        50\n",
            "           1       1.00      0.20      0.34        98\n",
            "\n",
            "    accuracy                           0.47       148\n",
            "   macro avg       0.70      0.60      0.45       148\n",
            "weighted avg       0.79      0.47      0.41       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 55.41%\n",
            "Precision: 0.81\n",
            "Recall: 0.55\n",
            "F1-Score: 0.53\n",
            "MCC: 0.38\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [66 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      1.00      0.60        50\n",
            "           1       1.00      0.33      0.49        98\n",
            "\n",
            "    accuracy                           0.55       148\n",
            "   macro avg       0.72      0.66      0.55       148\n",
            "weighted avg       0.81      0.55      0.53       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "\n",
            "Bagging Testing Accuracy: 37.16%\n",
            "Precision: 0.78\n",
            "Recall: 0.37\n",
            "F1-Score: 0.24\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [93  5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.05      0.10        98\n",
            "\n",
            "    accuracy                           0.37       148\n",
            "   macro avg       0.67      0.53      0.31       148\n",
            "weighted avg       0.78      0.37      0.24       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Extra Trees Testing Accuracy: 81.08%\n",
            "Precision: 0.81\n",
            "Recall: 0.81\n",
            "F1-Score: 0.81\n",
            "MCC: 0.57\n",
            "Confusion Matrix:\n",
            "[[35 15]\n",
            " [13 85]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.70      0.71        50\n",
            "           1       0.85      0.87      0.86        98\n",
            "\n",
            "    accuracy                           0.81       148\n",
            "   macro avg       0.79      0.78      0.79       148\n",
            "weighted avg       0.81      0.81      0.81       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 43.24%\n",
            "Precision: 0.79\n",
            "Recall: 0.43\n",
            "F1-Score: 0.35\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [84 14]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.14      0.25        98\n",
            "\n",
            "    accuracy                           0.43       148\n",
            "   macro avg       0.69      0.57      0.40       148\n",
            "weighted avg       0.79      0.43      0.35       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE44076 Random Search CV"
      ],
      "metadata": {
        "id": "k4wmwmK6dYQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE44076_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "E6ZKORzjaoQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5c53fe-6b40-4cbd-ffe0-913dc4cc93c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 1.0, 'C': np.float64(0.1668100537200059)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 83.78%\n",
            "Precision: 0.84\n",
            "Recall: 0.84\n",
            "F1-Score: 0.84\n",
            "MCC: 0.64\n",
            "Confusion Matrix:\n",
            "[[38 12]\n",
            " [12 86]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.76      0.76        50\n",
            "           1       0.88      0.88      0.88        98\n",
            "\n",
            "    accuracy                           0.84       148\n",
            "   macro avg       0.82      0.82      0.82       148\n",
            "weighted avg       0.84      0.84      0.84       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 64.86%\n",
            "Precision: 0.44\n",
            "Recall: 0.65\n",
            "F1-Score: 0.52\n",
            "MCC: -0.08\n",
            "Confusion Matrix:\n",
            "[[ 0 50]\n",
            " [ 2 96]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        50\n",
            "           1       0.66      0.98      0.79        98\n",
            "\n",
            "    accuracy                           0.65       148\n",
            "   macro avg       0.33      0.49      0.39       148\n",
            "weighted avg       0.44      0.65      0.52       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 38.51%\n",
            "Precision: 0.78\n",
            "Recall: 0.39\n",
            "F1-Score: 0.27\n",
            "MCC: 0.16\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [91  7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.07      0.13        98\n",
            "\n",
            "    accuracy                           0.39       148\n",
            "   macro avg       0.68      0.54      0.33       148\n",
            "weighted avg       0.78      0.39      0.27       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 10, 'learning_rate': np.float64(0.105)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 47.97%\n",
            "Precision: 0.80\n",
            "Recall: 0.48\n",
            "F1-Score: 0.42\n",
            "MCC: 0.29\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [77 21]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      1.00      0.56        50\n",
            "           1       1.00      0.21      0.35        98\n",
            "\n",
            "    accuracy                           0.48       148\n",
            "   macro avg       0.70      0.61      0.46       148\n",
            "weighted avg       0.80      0.48      0.42       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 55.41%\n",
            "Precision: 0.81\n",
            "Recall: 0.55\n",
            "F1-Score: 0.53\n",
            "MCC: 0.38\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [66 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      1.00      0.60        50\n",
            "           1       1.00      0.33      0.49        98\n",
            "\n",
            "    accuracy                           0.55       148\n",
            "   macro avg       0.72      0.66      0.55       148\n",
            "weighted avg       0.81      0.55      0.53       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': True}\n",
            "\n",
            "Bagging Testing Accuracy: 37.16%\n",
            "Precision: 0.78\n",
            "Recall: 0.37\n",
            "F1-Score: 0.24\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [93  5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        50\n",
            "           1       1.00      0.05      0.10        98\n",
            "\n",
            "    accuracy                           0.37       148\n",
            "   macro avg       0.67      0.53      0.31       148\n",
            "weighted avg       0.78      0.37      0.24       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 61.49%\n",
            "Precision: 0.82\n",
            "Recall: 0.61\n",
            "F1-Score: 0.61\n",
            "MCC: 0.44\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [57 41]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      1.00      0.64        50\n",
            "           1       1.00      0.42      0.59        98\n",
            "\n",
            "    accuracy                           0.61       148\n",
            "   macro avg       0.73      0.71      0.61       148\n",
            "weighted avg       0.82      0.61      0.61       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 43.24%\n",
            "Precision: 0.79\n",
            "Recall: 0.43\n",
            "F1-Score: 0.35\n",
            "MCC: 0.23\n",
            "Confusion Matrix:\n",
            "[[50  0]\n",
            " [84 14]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      1.00      0.54        50\n",
            "           1       1.00      0.14      0.25        98\n",
            "\n",
            "    accuracy                           0.43       148\n",
            "   macro avg       0.69      0.57      0.40       148\n",
            "weighted avg       0.79      0.43      0.35       148\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 1.0, 'C': np.float64(0.1668100537200059)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 10, 'learning_rate': np.float64(0.105)}\n",
            "AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': True}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Grid Search CV"
      ],
      "metadata": {
        "id": "ySVQ3aO5do3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500]\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1]\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_samples': [0.5, 0.75, 1.0],\n",
        "        'max_features': [0.5, 0.75, 1.0],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "     'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform GridSearchCV\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "86yX9vwpaoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0506e2-fb84-42fb-bca8-be8d4dcbe9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 98.44%\n",
            "Precision: 0.98\n",
            "Recall: 0.98\n",
            "F1-Score: 0.98\n",
            "MCC: 0.97\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.97      1.00      0.98        32\n",
            "\n",
            "    accuracy                           0.98        64\n",
            "   macro avg       0.98      0.98      0.98        64\n",
            "weighted avg       0.98      0.98      0.98        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 100.00%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "MCC: 1.00\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        32\n",
            "           1       1.00      1.00      1.00        32\n",
            "\n",
            "    accuracy                           1.00        64\n",
            "   macro avg       1.00      1.00      1.00        64\n",
            "weighted avg       1.00      1.00      1.00        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Random Forest Testing Accuracy: 60.94%\n",
            "Precision: 0.78\n",
            "Recall: 0.61\n",
            "F1-Score: 0.54\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [25  7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72        32\n",
            "           1       1.00      0.22      0.36        32\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.78      0.61      0.54        64\n",
            "weighted avg       0.78      0.61      0.54        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 93.75%\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "F1-Score: 0.94\n",
            "MCC: 0.88\n",
            "Confusion Matrix:\n",
            "[[31  1]\n",
            " [ 3 29]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94        32\n",
            "           1       0.97      0.91      0.94        32\n",
            "\n",
            "    accuracy                           0.94        64\n",
            "   macro avg       0.94      0.94      0.94        64\n",
            "weighted avg       0.94      0.94      0.94        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "\n",
            "AdaBoost Testing Accuracy: 92.19%\n",
            "Precision: 0.93\n",
            "Recall: 0.92\n",
            "F1-Score: 0.92\n",
            "MCC: 0.85\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 5 27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.93        32\n",
            "           1       1.00      0.84      0.92        32\n",
            "\n",
            "    accuracy                           0.92        64\n",
            "   macro avg       0.93      0.92      0.92        64\n",
            "weighted avg       0.93      0.92      0.92        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 200}\n",
            "\n",
            "Bagging Testing Accuracy: 65.62%\n",
            "Precision: 0.80\n",
            "Recall: 0.66\n",
            "F1-Score: 0.61\n",
            "MCC: 0.43\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [22 10]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      1.00      0.74        32\n",
            "           1       1.00      0.31      0.48        32\n",
            "\n",
            "    accuracy                           0.66        64\n",
            "   macro avg       0.80      0.66      0.61        64\n",
            "weighted avg       0.80      0.66      0.61        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "\n",
            "Extra Trees Testing Accuracy: 75.00%\n",
            "Precision: 0.83\n",
            "Recall: 0.75\n",
            "F1-Score: 0.73\n",
            "MCC: 0.58\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [16 16]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80        32\n",
            "           1       1.00      0.50      0.67        32\n",
            "\n",
            "    accuracy                           0.75        64\n",
            "   macro avg       0.83      0.75      0.73        64\n",
            "weighted avg       0.83      0.75      0.73        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 87.50%\n",
            "Precision: 0.90\n",
            "Recall: 0.88\n",
            "F1-Score: 0.87\n",
            "MCC: 0.77\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 8 24]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        32\n",
            "           1       1.00      0.75      0.86        32\n",
            "\n",
            "    accuracy                           0.88        64\n",
            "   macro avg       0.90      0.88      0.87        64\n",
            "weighted avg       0.90      0.88      0.87        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
            "K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
            "Random Forest: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
            "Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 200}\n",
            "Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSE8671 Random Search CV"
      ],
      "metadata": {
        "id": "wMfqpONYd9ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier,\n",
        "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n",
        "columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n",
        "                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n",
        "                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n",
        "df_train = df_train[columns_of_interest]\n",
        "#df_train = df_train.iloc[:, 1:]\n",
        "df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n",
        "X_train = df_train.iloc[:, 0:-1]\n",
        "y_train = df_train.iloc[:, -1]\n",
        "\n",
        "df_test = pd.read_csv(\"GSE8671_gene_expression_renamed.csv\")\n",
        "df_test = df_test.iloc[:, 1:]\n",
        "df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n",
        "X_test = df_test.iloc[:, 0:-1]\n",
        "y_test = df_test.iloc[:, -1]\n",
        "\n",
        "# Ensure common features in both train and test sets\n",
        "common_cols = set(X_train.columns).intersection(X_test.columns)\n",
        "X_train_common = X_train[list(common_cols)]\n",
        "X_test_common = X_test[list(common_cols)]\n",
        "\n",
        "# Hyperparameter grid for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': np.logspace(-3, 2, 10),\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [500, 1000, 5000],\n",
        "        'l1_ratio': np.linspace(0, 1, 5).tolist()\n",
        "    },\n",
        "    'K-Nearest Neighbors': {\n",
        "        'n_neighbors': range(3, 15),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'learning_rate': np.linspace(0.01, 0.2, 5),\n",
        "        'max_depth': [3, 5, 7, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'learning_rate': np.linspace(0.01, 1, 5)\n",
        "    },\n",
        "    'Bagging': {\n",
        "        'n_estimators': range(50, 301, 50),\n",
        "        'max_samples': np.linspace(0.5, 1.0, 5),\n",
        "        'max_features': np.linspace(0.5, 1.0, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Extra Trees': {\n",
        "        'n_estimators': range(100, 501, 100),\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'Extra Trees': ExtraTreesClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store best parameters\n",
        "best_params = {}\n",
        "\n",
        "# Tune and evaluate models using RandomizedSearchCV\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    # Perform RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(model, param_grids[name],\n",
        "                                       n_iter=20, cv=10,\n",
        "                                       n_jobs=-1, scoring='accuracy', random_state=7)\n",
        "\n",
        "    random_search.fit(X_train_common, y_train)\n",
        "\n",
        "    # Store best parameters\n",
        "    best_params[name] = random_search.best_params_\n",
        "    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n",
        "\n",
        "    # Train model with best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_model.fit(X_train_common, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    predictions = best_model.predict(X_test_common)\n",
        "\n",
        "    # Compute metrics\n",
        "    test_accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "    print(f\"MCC: {mcc:.2f}\")\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Print final best parameters\n",
        "print(\"\\nFinal Best Parameters for All Models:\")\n",
        "for model, params in best_params.items():\n",
        "    print(f\"{model}: {params}\")"
      ],
      "metadata": {
        "id": "UTkDQ_0DaoZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7091dc3f-40dc-4048-cc45-781b13ff4f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 0.75, 'C': np.float64(0.046415888336127795)}\n",
            "\n",
            "Logistic Regression Testing Accuracy: 82.81%\n",
            "Precision: 0.87\n",
            "Recall: 0.83\n",
            "F1-Score: 0.82\n",
            "MCC: 0.70\n",
            "Confusion Matrix:\n",
            "[[21 11]\n",
            " [ 0 32]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.66      0.79        32\n",
            "           1       0.74      1.00      0.85        32\n",
            "\n",
            "    accuracy                           0.83        64\n",
            "   macro avg       0.87      0.83      0.82        64\n",
            "weighted avg       0.87      0.83      0.82        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning K-Nearest Neighbors...\n",
            "Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "\n",
            "K-Nearest Neighbors Testing Accuracy: 51.56%\n",
            "Precision: 0.75\n",
            "Recall: 0.52\n",
            "F1-Score: 0.37\n",
            "MCC: 0.13\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [31  1]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      1.00      0.67        32\n",
            "           1       1.00      0.03      0.06        32\n",
            "\n",
            "    accuracy                           0.52        64\n",
            "   macro avg       0.75      0.52      0.37        64\n",
            "weighted avg       0.75      0.52      0.37        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Random Forest Testing Accuracy: 62.50%\n",
            "Precision: 0.79\n",
            "Recall: 0.62\n",
            "F1-Score: 0.56\n",
            "MCC: 0.38\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [24  8]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.73        32\n",
            "           1       1.00      0.25      0.40        32\n",
            "\n",
            "    accuracy                           0.62        64\n",
            "   macro avg       0.79      0.62      0.56        64\n",
            "weighted avg       0.79      0.62      0.56        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Gradient Boosting...\n",
            "Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "\n",
            "Gradient Boosting Testing Accuracy: 73.44%\n",
            "Precision: 0.83\n",
            "Recall: 0.73\n",
            "F1-Score: 0.71\n",
            "MCC: 0.55\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [17 15]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      1.00      0.79        32\n",
            "           1       1.00      0.47      0.64        32\n",
            "\n",
            "    accuracy                           0.73        64\n",
            "   macro avg       0.83      0.73      0.71        64\n",
            "weighted avg       0.83      0.73      0.71        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "\n",
            "AdaBoost Testing Accuracy: 92.19%\n",
            "Precision: 0.93\n",
            "Recall: 0.92\n",
            "F1-Score: 0.92\n",
            "MCC: 0.85\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 5 27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      1.00      0.93        32\n",
            "           1       1.00      0.84      0.92        32\n",
            "\n",
            "    accuracy                           0.92        64\n",
            "   macro avg       0.93      0.92      0.92        64\n",
            "weighted avg       0.93      0.92      0.92        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Bagging...\n",
            "Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.5), 'bootstrap': False}\n",
            "\n",
            "Bagging Testing Accuracy: 60.94%\n",
            "Precision: 0.78\n",
            "Recall: 0.61\n",
            "F1-Score: 0.54\n",
            "MCC: 0.35\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [25  7]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72        32\n",
            "           1       1.00      0.22      0.36        32\n",
            "\n",
            "    accuracy                           0.61        64\n",
            "   macro avg       0.78      0.61      0.54        64\n",
            "weighted avg       0.78      0.61      0.54        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning Extra Trees...\n",
            "Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "\n",
            "Extra Trees Testing Accuracy: 79.69%\n",
            "Precision: 0.86\n",
            "Recall: 0.80\n",
            "F1-Score: 0.79\n",
            "MCC: 0.65\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [13 19]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      1.00      0.83        32\n",
            "           1       1.00      0.59      0.75        32\n",
            "\n",
            "    accuracy                           0.80        64\n",
            "   macro avg       0.86      0.80      0.79        64\n",
            "weighted avg       0.86      0.80      0.79        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tuning XGBoost...\n",
            "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "\n",
            "XGBoost Testing Accuracy: 89.06%\n",
            "Precision: 0.91\n",
            "Recall: 0.89\n",
            "F1-Score: 0.89\n",
            "MCC: 0.80\n",
            "Confusion Matrix:\n",
            "[[32  0]\n",
            " [ 7 25]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      1.00      0.90        32\n",
            "           1       1.00      0.78      0.88        32\n",
            "\n",
            "    accuracy                           0.89        64\n",
            "   macro avg       0.91      0.89      0.89        64\n",
            "weighted avg       0.91      0.89      0.89        64\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final Best Parameters for All Models:\n",
            "Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 0.75, 'C': np.float64(0.046415888336127795)}\n",
            "K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n",
            "Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
            "Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n",
            "AdaBoost: {'n_estimators': 200, 'learning_rate': np.float64(1.0)}\n",
            "Bagging: {'n_estimators': 200, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.5), 'bootstrap': False}\n",
            "Extra Trees: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n",
            "XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    }
  ]
}