{"cells":[{"cell_type":"markdown","metadata":{"id":"b9Wed-y4skm5"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"J_lgF0vQzS1D","outputId":"d195a34a-4070-45a3-fa87-9778a661b0c2"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-f3b89d9f-6d41-494f-8dab-cfb2396ec160\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f3b89d9f-6d41-494f-8dab-cfb2396ec160\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving ML_DEG_dataset.csv to ML_DEG_dataset.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # Manually upload the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"r2OFHcYKBIrH","outputId":"bc87049f-8dc8-4f64-c1d2-f37c54344559"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-619cc4cf-e59a-4db1-aabc-702301374531\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-619cc4cf-e59a-4db1-aabc-702301374531\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving GSE106582_gene_expression_renamed.csv to GSE106582_gene_expression_renamed.csv\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()  # Manually upload the file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eL6e3aFO13-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aacjzmgj7eez","outputId":"e8397e43-9a0c-414b-92af-02df258d4602"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"markdown","metadata":{"id":"__TcBLoYhShL"},"source":["# Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"0_TYrCmtoqQm"},"source":["# MI Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"GGMZm0mpBymN"},"source":["## Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpYoE9zJA1HI","outputId":"c50e598c-0827-4c9e-84ca-f42ac18e7dc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 96.91%\n","Precision: 0.97\n","Recall: 0.97\n","F1-Score: 0.97\n","MCC: 0.94\n","Confusion Matrix:\n","[[114   3]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       117\n","           1       0.96      0.96      0.96        77\n","\n","    accuracy                           0.97       194\n","   macro avg       0.97      0.97      0.97       194\n","weighted avg       0.97      0.97      0.97       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 95.36%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.90\n","Confusion Matrix:\n","[[111   6]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.95      0.96       117\n","           1       0.93      0.96      0.94        77\n","\n","    accuracy                           0.95       194\n","   macro avg       0.95      0.95      0.95       194\n","weighted avg       0.95      0.95      0.95       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 90.21%\n","Precision: 0.91\n","Recall: 0.90\n","F1-Score: 0.90\n","MCC: 0.81\n","Confusion Matrix:\n","[[101  16]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.86      0.91       117\n","           1       0.82      0.96      0.89        77\n","\n","    accuracy                           0.90       194\n","   macro avg       0.90      0.91      0.90       194\n","weighted avg       0.91      0.90      0.90       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 76.29%\n","Precision: 0.83\n","Recall: 0.76\n","F1-Score: 0.76\n","MCC: 0.59\n","Confusion Matrix:\n","[[74 43]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.63      0.76       117\n","           1       0.63      0.96      0.76        77\n","\n","    accuracy                           0.76       194\n","   macro avg       0.80      0.80      0.76       194\n","weighted avg       0.83      0.76      0.76       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 92.78%\n","Precision: 0.93\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.85\n","Confusion Matrix:\n","[[111   6]\n"," [  8  69]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.95      0.94       117\n","           1       0.92      0.90      0.91        77\n","\n","    accuracy                           0.93       194\n","   macro avg       0.93      0.92      0.92       194\n","weighted avg       0.93      0.93      0.93       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 89.69%\n","Precision: 0.91\n","Recall: 0.90\n","F1-Score: 0.90\n","MCC: 0.80\n","Confusion Matrix:\n","[[100  17]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.85      0.91       117\n","           1       0.81      0.96      0.88        77\n","\n","    accuracy                           0.90       194\n","   macro avg       0.89      0.91      0.90       194\n","weighted avg       0.91      0.90      0.90       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","\n","Extra Trees Testing Accuracy: 95.36%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.90\n","Confusion Matrix:\n","[[111   6]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.95      0.96       117\n","           1       0.93      0.96      0.94        77\n","\n","    accuracy                           0.95       194\n","   macro avg       0.95      0.95      0.95       194\n","weighted avg       0.95      0.95      0.95       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 92.78%\n","Precision: 0.93\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.85\n","Confusion Matrix:\n","[[107  10]\n"," [  4  73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.91      0.94       117\n","           1       0.88      0.95      0.91        77\n","\n","    accuracy                           0.93       194\n","   macro avg       0.92      0.93      0.93       194\n","weighted avg       0.93      0.93      0.93       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 0.1, 'n_estimators': 200}\n","Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n","Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['ADH1B', 'CDH3', 'GUCA2B', 'PDE9A', 'ABCA8', 'GUCA2A', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"o47_gTDJCIe3"},"source":["# Random Search CV\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZwC5eOuCF7d","outputId":"1fe927fd-b7cb-43a6-f0a1-88141e649e26"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","\n","Logistic Regression Testing Accuracy: 96.39%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.92\n","Confusion Matrix:\n","[[114   3]\n"," [  4  73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       117\n","           1       0.96      0.95      0.95        77\n","\n","    accuracy                           0.96       194\n","   macro avg       0.96      0.96      0.96       194\n","weighted avg       0.96      0.96      0.96       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 95.88%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.91\n","Confusion Matrix:\n","[[112   5]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.96      0.97       117\n","           1       0.94      0.96      0.95        77\n","\n","    accuracy                           0.96       194\n","   macro avg       0.96      0.96      0.96       194\n","weighted avg       0.96      0.96      0.96       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n","\n","Random Forest Testing Accuracy: 91.24%\n","Precision: 0.92\n","Recall: 0.91\n","F1-Score: 0.91\n","MCC: 0.83\n","Confusion Matrix:\n","[[103  14]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.88      0.92       117\n","           1       0.84      0.96      0.90        77\n","\n","    accuracy                           0.91       194\n","   macro avg       0.91      0.92      0.91       194\n","weighted avg       0.92      0.91      0.91       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 10, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n","\n","Gradient Boosting Testing Accuracy: 54.64%\n","Precision: 0.74\n","Recall: 0.55\n","F1-Score: 0.50\n","MCC: 0.30\n","Confusion Matrix:\n","[[32 85]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.27      0.42       117\n","           1       0.47      0.96      0.63        77\n","\n","    accuracy                           0.55       194\n","   macro avg       0.69      0.62      0.52       194\n","weighted avg       0.74      0.55      0.50       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(0.2575)}\n","\n","AdaBoost Testing Accuracy: 92.78%\n","Precision: 0.93\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.85\n","Confusion Matrix:\n","[[106  11]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.91      0.94       117\n","           1       0.87      0.96      0.91        77\n","\n","    accuracy                           0.93       194\n","   macro avg       0.92      0.93      0.93       194\n","weighted avg       0.93      0.93      0.93       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.75), 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 89.69%\n","Precision: 0.91\n","Recall: 0.90\n","F1-Score: 0.90\n","MCC: 0.80\n","Confusion Matrix:\n","[[100  17]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.85      0.91       117\n","           1       0.81      0.96      0.88        77\n","\n","    accuracy                           0.90       194\n","   macro avg       0.89      0.91      0.90       194\n","weighted avg       0.91      0.90      0.90       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 94.85%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.89\n","Confusion Matrix:\n","[[110   7]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.94      0.96       117\n","           1       0.91      0.96      0.94        77\n","\n","    accuracy                           0.95       194\n","   macro avg       0.94      0.95      0.95       194\n","weighted avg       0.95      0.95      0.95       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 88.66%\n","Precision: 0.90\n","Recall: 0.89\n","F1-Score: 0.89\n","MCC: 0.78\n","Confusion Matrix:\n","[[98 19]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.84      0.90       117\n","           1       0.80      0.96      0.87        77\n","\n","    accuracy                           0.89       194\n","   macro avg       0.88      0.90      0.88       194\n","weighted avg       0.90      0.89      0.89       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 300, 'min_samples_split': 10, 'max_depth': 10, 'learning_rate': np.float64(0.2)}\n","AdaBoost: {'n_estimators': 250, 'learning_rate': np.float64(0.2575)}\n","Bagging: {'n_estimators': 200, 'max_samples': np.float64(0.875), 'max_features': np.float64(0.75), 'bootstrap': True}\n","Extra Trees: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['ADH1B', 'CDH3', 'GUCA2B', 'PDE9A', 'ABCA8', 'GUCA2A', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"t3mUPawDtFh-"},"source":["# Tuning for Hub genes"]},{"cell_type":"markdown","metadata":{"id":"On-eVOfNH2Mq"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3x_YW_pdH1Zz","outputId":"e85e85b9-ae9a-4a8c-8384-068d61511a8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 77.84%\n","Precision: 0.83\n","Recall: 0.78\n","F1-Score: 0.75\n","MCC: 0.56\n","Confusion Matrix:\n","[[116   1]\n"," [ 42  35]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.99      0.84       117\n","           1       0.97      0.45      0.62        77\n","\n","    accuracy                           0.78       194\n","   macro avg       0.85      0.72      0.73       194\n","weighted avg       0.83      0.78      0.75       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 48.97%\n","Precision: 0.70\n","Recall: 0.49\n","F1-Score: 0.42\n","MCC: 0.21\n","Confusion Matrix:\n","[[21 96]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.18      0.30       117\n","           1       0.44      0.96      0.60        77\n","\n","    accuracy                           0.49       194\n","   macro avg       0.66      0.57      0.45       194\n","weighted avg       0.70      0.49      0.42       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n","\n","Random Forest Testing Accuracy: 71.13%\n","Precision: 0.74\n","Recall: 0.71\n","F1-Score: 0.71\n","MCC: 0.44\n","Confusion Matrix:\n","[[78 39]\n"," [17 60]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.67      0.74       117\n","           1       0.61      0.78      0.68        77\n","\n","    accuracy                           0.71       194\n","   macro avg       0.71      0.72      0.71       194\n","weighted avg       0.74      0.71      0.71       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 76.80%\n","Precision: 0.77\n","Recall: 0.77\n","F1-Score: 0.77\n","MCC: 0.51\n","Confusion Matrix:\n","[[95 22]\n"," [23 54]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.81      0.81       117\n","           1       0.71      0.70      0.71        77\n","\n","    accuracy                           0.77       194\n","   macro avg       0.76      0.76      0.76       194\n","weighted avg       0.77      0.77      0.77       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 85.57%\n","Precision: 0.86\n","Recall: 0.86\n","F1-Score: 0.85\n","MCC: 0.70\n","Confusion Matrix:\n","[[109   8]\n"," [ 20  57]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.93      0.89       117\n","           1       0.88      0.74      0.80        77\n","\n","    accuracy                           0.86       194\n","   macro avg       0.86      0.84      0.84       194\n","weighted avg       0.86      0.86      0.85       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 50}\n","\n","Bagging Testing Accuracy: 71.65%\n","Precision: 0.71\n","Recall: 0.72\n","F1-Score: 0.71\n","MCC: 0.39\n","Confusion Matrix:\n","[[96 21]\n"," [34 43]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.74      0.82      0.78       117\n","           1       0.67      0.56      0.61        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.71      0.69      0.69       194\n","weighted avg       0.71      0.72      0.71       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 52.06%\n","Precision: 0.72\n","Recall: 0.52\n","F1-Score: 0.47\n","MCC: 0.26\n","Confusion Matrix:\n","[[27 90]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.23      0.37       117\n","           1       0.45      0.96      0.61        77\n","\n","    accuracy                           0.52       194\n","   macro avg       0.68      0.60      0.49       194\n","weighted avg       0.72      0.52      0.47       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 72.16%\n","Precision: 0.74\n","Recall: 0.72\n","F1-Score: 0.72\n","MCC: 0.45\n","Confusion Matrix:\n","[[80 37]\n"," [17 60]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.68      0.75       117\n","           1       0.62      0.78      0.69        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.72      0.73      0.72       194\n","weighted avg       0.74      0.72      0.72       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n","Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n","Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 50}\n","Extra Trees: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n","                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n","                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"iSMjPZQEIE8Y"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llUEZucpIIPY","outputId":"94408847-7d08-4a6b-9df3-f18a933e50c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 0.75, 'C': np.float64(0.046415888336127795)}\n","\n","Logistic Regression Testing Accuracy: 69.59%\n","Precision: 0.80\n","Recall: 0.70\n","F1-Score: 0.63\n","MCC: 0.39\n","Confusion Matrix:\n","[[117   0]\n"," [ 59  18]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.66      1.00      0.80       117\n","           1       1.00      0.23      0.38        77\n","\n","    accuracy                           0.70       194\n","   macro avg       0.83      0.62      0.59       194\n","weighted avg       0.80      0.70      0.63       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 49.48%\n","Precision: 0.69\n","Recall: 0.49\n","F1-Score: 0.43\n","MCC: 0.20\n","Confusion Matrix:\n","[[23 94]\n"," [ 4 73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.20      0.32       117\n","           1       0.44      0.95      0.60        77\n","\n","    accuracy                           0.49       194\n","   macro avg       0.64      0.57      0.46       194\n","weighted avg       0.69      0.49      0.43       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 78.87%\n","Precision: 0.81\n","Recall: 0.79\n","F1-Score: 0.79\n","MCC: 0.59\n","Confusion Matrix:\n","[[87 30]\n"," [11 66]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.74      0.81       117\n","           1       0.69      0.86      0.76        77\n","\n","    accuracy                           0.79       194\n","   macro avg       0.79      0.80      0.79       194\n","weighted avg       0.81      0.79      0.79       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 3, 'learning_rate': np.float64(0.0575)}\n","\n","Gradient Boosting Testing Accuracy: 75.77%\n","Precision: 0.78\n","Recall: 0.76\n","F1-Score: 0.76\n","MCC: 0.52\n","Confusion Matrix:\n","[[84 33]\n"," [14 63]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.72      0.78       117\n","           1       0.66      0.82      0.73        77\n","\n","    accuracy                           0.76       194\n","   macro avg       0.76      0.77      0.75       194\n","weighted avg       0.78      0.76      0.76       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n","\n","AdaBoost Testing Accuracy: 79.90%\n","Precision: 0.80\n","Recall: 0.80\n","F1-Score: 0.80\n","MCC: 0.59\n","Confusion Matrix:\n","[[94 23]\n"," [16 61]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.80      0.83       117\n","           1       0.73      0.79      0.76        77\n","\n","    accuracy                           0.80       194\n","   macro avg       0.79      0.80      0.79       194\n","weighted avg       0.80      0.80      0.80       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 78.35%\n","Precision: 0.81\n","Recall: 0.78\n","F1-Score: 0.77\n","MCC: 0.55\n","Confusion Matrix:\n","[[112   5]\n"," [ 37  40]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.96      0.84       117\n","           1       0.89      0.52      0.66        77\n","\n","    accuracy                           0.78       194\n","   macro avg       0.82      0.74      0.75       194\n","weighted avg       0.81      0.78      0.77       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 53.09%\n","Precision: 0.73\n","Recall: 0.53\n","F1-Score: 0.48\n","MCC: 0.28\n","Confusion Matrix:\n","[[29 88]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.25      0.39       117\n","           1       0.46      0.96      0.62        77\n","\n","    accuracy                           0.53       194\n","   macro avg       0.68      0.60      0.50       194\n","weighted avg       0.73      0.53      0.48       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n","\n","XGBoost Testing Accuracy: 64.43%\n","Precision: 0.68\n","Recall: 0.64\n","F1-Score: 0.65\n","MCC: 0.31\n","Confusion Matrix:\n","[[69 48]\n"," [21 56]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.59      0.67       117\n","           1       0.54      0.73      0.62        77\n","\n","    accuracy                           0.64       194\n","   macro avg       0.65      0.66      0.64       194\n","weighted avg       0.68      0.64      0.65       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'l1_ratio': 0.75, 'C': np.float64(0.046415888336127795)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 400, 'min_samples_split': 5, 'max_depth': 3, 'learning_rate': np.float64(0.0575)}\n","AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(1.0)}\n","Bagging: {'n_estimators': 50, 'max_samples': np.float64(0.5), 'max_features': np.float64(0.5), 'bootstrap': False}\n","Extra Trees: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['EHD2', 'TUBB', 'CSRP1', 'DPYSL3', 'TIMP2', 'RHOQ', 'ILK', 'SPARCL1', 'PALLD', 'PTRF',\n","                       'PPP3CB', 'MEF2C', 'RASL12', 'FLNA', 'PRNP', 'CAV2', 'SNCA', 'TNS1', 'TGFB1I1',\n","                       'SLIT2', 'ITPR1', 'LAMB2', 'RHOB', 'ACTN1','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"p2z4CNu14LfE"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC8LQrRG8AdB"},"outputs":[],"source":["['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esk6-wyL8UJX"},"outputs":[],"source":["# Grid Search CV Tuning for Pearson Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeLialgiJQfK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"19geX5E_fES3"},"source":["# PearsonCorrelation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKwT7Z0hJiGg","outputId":"5913c993-c836-4702-f4bf-4da527d7ebef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 68.04%\n","Precision: 0.82\n","Recall: 0.68\n","F1-Score: 0.67\n","MCC: 0.51\n","Confusion Matrix:\n","[[55 62]\n"," [ 0 77]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.47      0.64       117\n","           1       0.55      1.00      0.71        77\n","\n","    accuracy                           0.68       194\n","   macro avg       0.78      0.74      0.68       194\n","weighted avg       0.82      0.68      0.67       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n","\n","K-Nearest Neighbors Testing Accuracy: 89.69%\n","Precision: 0.91\n","Recall: 0.90\n","F1-Score: 0.90\n","MCC: 0.80\n","Confusion Matrix:\n","[[99 18]\n"," [ 2 75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.85      0.91       117\n","           1       0.81      0.97      0.88        77\n","\n","    accuracy                           0.90       194\n","   macro avg       0.89      0.91      0.90       194\n","weighted avg       0.91      0.90      0.90       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 91.75%\n","Precision: 0.92\n","Recall: 0.92\n","F1-Score: 0.92\n","MCC: 0.83\n","Confusion Matrix:\n","[[107  10]\n"," [  6  71]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.91      0.93       117\n","           1       0.88      0.92      0.90        77\n","\n","    accuracy                           0.92       194\n","   macro avg       0.91      0.92      0.91       194\n","weighted avg       0.92      0.92      0.92       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 86.60%\n","Precision: 0.88\n","Recall: 0.87\n","F1-Score: 0.87\n","MCC: 0.74\n","Confusion Matrix:\n","[[96 21]\n"," [ 5 72]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.82      0.88       117\n","           1       0.77      0.94      0.85        77\n","\n","    accuracy                           0.87       194\n","   macro avg       0.86      0.88      0.86       194\n","weighted avg       0.88      0.87      0.87       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n","\n","AdaBoost Testing Accuracy: 72.16%\n","Precision: 0.82\n","Recall: 0.72\n","F1-Score: 0.72\n","MCC: 0.54\n","Confusion Matrix:\n","[[65 52]\n"," [ 2 75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.56      0.71       117\n","           1       0.59      0.97      0.74        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.78      0.76      0.72       194\n","weighted avg       0.82      0.72      0.72       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\n","\n","Bagging Testing Accuracy: 86.08%\n","Precision: 0.88\n","Recall: 0.86\n","F1-Score: 0.86\n","MCC: 0.73\n","Confusion Matrix:\n","[[95 22]\n"," [ 5 72]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.81      0.88       117\n","           1       0.77      0.94      0.84        77\n","\n","    accuracy                           0.86       194\n","   macro avg       0.86      0.87      0.86       194\n","weighted avg       0.88      0.86      0.86       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 94.33%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.88\n","Confusion Matrix:\n","[[110   7]\n"," [  4  73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.94      0.95       117\n","           1       0.91      0.95      0.93        77\n","\n","    accuracy                           0.94       194\n","   macro avg       0.94      0.94      0.94       194\n","weighted avg       0.94      0.94      0.94       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 86.08%\n","Precision: 0.88\n","Recall: 0.86\n","F1-Score: 0.86\n","MCC: 0.74\n","Confusion Matrix:\n","[[93 24]\n"," [ 3 74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.79      0.87       117\n","           1       0.76      0.96      0.85        77\n","\n","    accuracy                           0.86       194\n","   macro avg       0.86      0.88      0.86       194\n","weighted avg       0.88      0.86      0.86       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n","Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 100}\n","Bagging: {'bootstrap': False, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 50}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"5A0a9nKfJuOh"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocACFm9GJxiw","outputId":"8398396f-fdae-4eb0-80ee-417bc0a39ad4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","\n","Logistic Regression Testing Accuracy: 93.30%\n","Precision: 0.94\n","Recall: 0.93\n","F1-Score: 0.93\n","MCC: 0.86\n","Confusion Matrix:\n","[[115   2]\n"," [ 11  66]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.98      0.95       117\n","           1       0.97      0.86      0.91        77\n","\n","    accuracy                           0.93       194\n","   macro avg       0.94      0.92      0.93       194\n","weighted avg       0.94      0.93      0.93       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 4, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 93.81%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.88\n","Confusion Matrix:\n","[[107  10]\n"," [  2  75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.91      0.95       117\n","           1       0.88      0.97      0.93        77\n","\n","    accuracy                           0.94       194\n","   macro avg       0.93      0.94      0.94       194\n","weighted avg       0.94      0.94      0.94       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 91.24%\n","Precision: 0.92\n","Recall: 0.91\n","F1-Score: 0.91\n","MCC: 0.82\n","Confusion Matrix:\n","[[105  12]\n"," [  5  72]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.90      0.93       117\n","           1       0.86      0.94      0.89        77\n","\n","    accuracy                           0.91       194\n","   macro avg       0.91      0.92      0.91       194\n","weighted avg       0.92      0.91      0.91       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n","\n","Gradient Boosting Testing Accuracy: 88.14%\n","Precision: 0.89\n","Recall: 0.88\n","F1-Score: 0.88\n","MCC: 0.76\n","Confusion Matrix:\n","[[101  16]\n"," [  7  70]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.86      0.90       117\n","           1       0.81      0.91      0.86        77\n","\n","    accuracy                           0.88       194\n","   macro avg       0.87      0.89      0.88       194\n","weighted avg       0.89      0.88      0.88       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(0.505)}\n","\n","AdaBoost Testing Accuracy: 89.18%\n","Precision: 0.90\n","Recall: 0.89\n","F1-Score: 0.89\n","MCC: 0.79\n","Confusion Matrix:\n","[[100  17]\n"," [  4  73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.85      0.90       117\n","           1       0.81      0.95      0.87        77\n","\n","    accuracy                           0.89       194\n","   macro avg       0.89      0.90      0.89       194\n","weighted avg       0.90      0.89      0.89       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 50, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.875), 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 88.14%\n","Precision: 0.89\n","Recall: 0.88\n","F1-Score: 0.88\n","MCC: 0.76\n","Confusion Matrix:\n","[[100  17]\n"," [  6  71]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.85      0.90       117\n","           1       0.81      0.92      0.86        77\n","\n","    accuracy                           0.88       194\n","   macro avg       0.88      0.89      0.88       194\n","weighted avg       0.89      0.88      0.88       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Extra Trees Testing Accuracy: 95.88%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.91\n","Confusion Matrix:\n","[[114   3]\n"," [  5  72]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.97       117\n","           1       0.96      0.94      0.95        77\n","\n","    accuracy                           0.96       194\n","   macro avg       0.96      0.95      0.96       194\n","weighted avg       0.96      0.96      0.96       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n","\n","XGBoost Testing Accuracy: 87.11%\n","Precision: 0.89\n","Recall: 0.87\n","F1-Score: 0.87\n","MCC: 0.75\n","Confusion Matrix:\n","[[96 21]\n"," [ 4 73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.82      0.88       117\n","           1       0.78      0.95      0.85        77\n","\n","    accuracy                           0.87       194\n","   macro avg       0.87      0.88      0.87       194\n","weighted avg       0.89      0.87      0.87       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 5000, 'l1_ratio': 0.5, 'C': np.float64(0.1668100537200059)}\n","K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 4, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.6, 'n_estimators': 500, 'min_samples_split': 2, 'max_depth': 3, 'learning_rate': np.float64(0.105)}\n","AdaBoost: {'n_estimators': 300, 'learning_rate': np.float64(0.505)}\n","Bagging: {'n_estimators': 50, 'max_samples': np.float64(1.0), 'max_features': np.float64(0.875), 'bootstrap': True}\n","Extra Trees: {'n_estimators': 400, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['MT1M', 'ITM2A', 'VSNL1', 'BRCA2', 'AFF3', 'LGALS2', 'PCSK2', 'TMEM100', 'CDH3', 'LIFR', 'FAM107A', 'STMN2', 'ABCA8', 'CENPA', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-3, 2, 10),\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500, 1000, 5000],\n","        'l1_ratio': np.linspace(0, 1, 5).tolist()\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 15),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': range(100, 501, 100),\n","        'learning_rate': np.linspace(0.01, 0.2, 5),\n","        'max_depth': [3, 5, 7, 10],\n","        'subsample': [0.6, 0.8, 1.0],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': range(50, 301, 50),\n","        'learning_rate': np.linspace(0.01, 1, 5)\n","    },\n","    'Bagging': {\n","        'n_estimators': range(50, 301, 50),\n","        'max_samples': np.linspace(0.5, 1.0, 5),\n","        'max_features': np.linspace(0.5, 1.0, 5),\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': range(100, 501, 100),\n","        'max_depth': [None, 10, 20, 30],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","     'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models using RandomizedSearchCV\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_grids[name],\n","                                       n_iter=20, cv=10,\n","                                       n_jobs=-1, scoring='accuracy', random_state=7)\n","\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    # Print confusion matrix and classification report\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","# Print final best parameters\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9ZofIrx9od7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wvh8hLzfLpAN"},"source":["# Lasso Regression Tuning"]},{"cell_type":"markdown","metadata":{"id":"msVjPMdWKHvh"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5s0I3FFKKhi","outputId":"80cffd9a-0bac-4c80-ba7a-5678af24d2b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 95.36%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.90\n","Confusion Matrix:\n","[[115   2]\n"," [  7  70]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.98      0.96       117\n","           1       0.97      0.91      0.94        77\n","\n","    accuracy                           0.95       194\n","   macro avg       0.96      0.95      0.95       194\n","weighted avg       0.95      0.95      0.95       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 87.63%\n","Precision: 0.90\n","Recall: 0.88\n","F1-Score: 0.88\n","MCC: 0.77\n","Confusion Matrix:\n","[[94 23]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.80      0.89       117\n","           1       0.77      0.99      0.86        77\n","\n","    accuracy                           0.88       194\n","   macro avg       0.88      0.90      0.88       194\n","weighted avg       0.90      0.88      0.88       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 76.29%\n","Precision: 0.84\n","Recall: 0.76\n","F1-Score: 0.76\n","MCC: 0.61\n","Confusion Matrix:\n","[[72 45]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.62      0.76       117\n","           1       0.63      0.99      0.77        77\n","\n","    accuracy                           0.76       194\n","   macro avg       0.81      0.80      0.76       194\n","weighted avg       0.84      0.76      0.76       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 68.56%\n","Precision: 0.81\n","Recall: 0.69\n","F1-Score: 0.68\n","MCC: 0.51\n","Confusion Matrix:\n","[[57 60]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.49      0.65       117\n","           1       0.56      0.99      0.71        77\n","\n","    accuracy                           0.69       194\n","   macro avg       0.77      0.74      0.68       194\n","weighted avg       0.81      0.69      0.68       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","\n","AdaBoost Testing Accuracy: 50.52%\n","Precision: 0.75\n","Recall: 0.51\n","F1-Score: 0.43\n","MCC: 0.26\n","Confusion Matrix:\n","[[22 95]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.19      0.31       117\n","           1       0.44      0.99      0.61        77\n","\n","    accuracy                           0.51       194\n","   macro avg       0.70      0.59      0.46       194\n","weighted avg       0.75      0.51      0.43       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 76.80%\n","Precision: 0.85\n","Recall: 0.77\n","F1-Score: 0.77\n","MCC: 0.62\n","Confusion Matrix:\n","[[73 44]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.62      0.76       117\n","           1       0.63      0.99      0.77        77\n","\n","    accuracy                           0.77       194\n","   macro avg       0.81      0.81      0.77       194\n","weighted avg       0.85      0.77      0.77       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 84.02%\n","Precision: 0.88\n","Recall: 0.84\n","F1-Score: 0.84\n","MCC: 0.72\n","Confusion Matrix:\n","[[87 30]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.74      0.85       117\n","           1       0.72      0.99      0.83        77\n","\n","    accuracy                           0.84       194\n","   macro avg       0.85      0.87      0.84       194\n","weighted avg       0.88      0.84      0.84       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 70.62%\n","Precision: 0.82\n","Recall: 0.71\n","F1-Score: 0.70\n","MCC: 0.53\n","Confusion Matrix:\n","[[61 56]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.52      0.68       117\n","           1       0.58      0.99      0.73        77\n","\n","    accuracy                           0.71       194\n","   macro avg       0.78      0.75      0.70       194\n","weighted avg       0.82      0.71      0.70       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n","Random Forest: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n","Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 0.75, 'n_estimators': 200}\n","Extra Trees: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['NR3C2', 'BRCA2', 'AFF3', 'LGALS2', 'TMEM100', 'CDH3', 'ABCG2', 'STMN2', 'ABCA8', 'AZGP1', 'TSPAN7', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"9PR0uZu1KdTM"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTE_l8AqKgQI","outputId":"b64451cc-4926-4ec5-9c80-35d3644c7487"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.01)}\n","\n","Logistic Regression Testing Accuracy: 95.36%\n","Precision: 0.95\n","Recall: 0.95\n","F1-Score: 0.95\n","MCC: 0.90\n","Confusion Matrix:\n","[[115   2]\n"," [  7  70]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.98      0.96       117\n","           1       0.97      0.91      0.94        77\n","\n","    accuracy                           0.95       194\n","   macro avg       0.96      0.95      0.95       194\n","weighted avg       0.95      0.95      0.95       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 87.63%\n","Precision: 0.90\n","Recall: 0.88\n","F1-Score: 0.88\n","MCC: 0.77\n","Confusion Matrix:\n","[[94 23]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.80      0.89       117\n","           1       0.77      0.99      0.86        77\n","\n","    accuracy                           0.88       194\n","   macro avg       0.88      0.90      0.88       194\n","weighted avg       0.90      0.88      0.88       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Random Forest Testing Accuracy: 74.74%\n","Precision: 0.84\n","Recall: 0.75\n","F1-Score: 0.75\n","MCC: 0.59\n","Confusion Matrix:\n","[[69 48]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.59      0.74       117\n","           1       0.61      0.99      0.76        77\n","\n","    accuracy                           0.75       194\n","   macro avg       0.80      0.79      0.75       194\n","weighted avg       0.84      0.75      0.75       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.1}\n","\n","Gradient Boosting Testing Accuracy: 68.04%\n","Precision: 0.81\n","Recall: 0.68\n","F1-Score: 0.67\n","MCC: 0.50\n","Confusion Matrix:\n","[[56 61]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.48      0.64       117\n","           1       0.55      0.99      0.71        77\n","\n","    accuracy                           0.68       194\n","   macro avg       0.77      0.73      0.68       194\n","weighted avg       0.81      0.68      0.67       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 200, 'learning_rate': 1}\n","\n","AdaBoost Testing Accuracy: 50.52%\n","Precision: 0.75\n","Recall: 0.51\n","F1-Score: 0.43\n","MCC: 0.26\n","Confusion Matrix:\n","[[22 95]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.19      0.31       117\n","           1       0.44      0.99      0.61        77\n","\n","    accuracy                           0.51       194\n","   macro avg       0.70      0.59      0.46       194\n","weighted avg       0.75      0.51      0.43       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 200, 'max_samples': 0.75, 'max_features': 0.75, 'bootstrap': True}\n","\n","Bagging Testing Accuracy: 71.65%\n","Precision: 0.83\n","Recall: 0.72\n","F1-Score: 0.71\n","MCC: 0.55\n","Confusion Matrix:\n","[[63 54]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.54      0.70       117\n","           1       0.58      0.99      0.73        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.78      0.76      0.72       194\n","weighted avg       0.83      0.72      0.71       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 91.24%\n","Precision: 0.92\n","Recall: 0.91\n","F1-Score: 0.91\n","MCC: 0.83\n","Confusion Matrix:\n","[[102  15]\n"," [  2  75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.87      0.92       117\n","           1       0.83      0.97      0.90        77\n","\n","    accuracy                           0.91       194\n","   macro avg       0.91      0.92      0.91       194\n","weighted avg       0.92      0.91      0.91       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 63.92%\n","Precision: 0.80\n","Recall: 0.64\n","F1-Score: 0.62\n","MCC: 0.45\n","Confusion Matrix:\n","[[48 69]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.41      0.58       117\n","           1       0.52      0.99      0.68        77\n","\n","    accuracy                           0.64       194\n","   macro avg       0.75      0.70      0.63       194\n","weighted avg       0.80      0.64      0.62       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l2', 'max_iter': 500, 'C': np.float64(0.01)}\n","K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 3, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.1}\n","AdaBoost: {'n_estimators': 200, 'learning_rate': 1}\n","Bagging: {'n_estimators': 200, 'max_samples': 0.75, 'max_features': 0.75, 'bootstrap': True}\n","Extra Trees: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest =['NR3C2', 'BRCA2', 'AFF3', 'LGALS2', 'TMEM100', 'CDH3', 'ABCG2', 'STMN2', 'ABCA8', 'AZGP1', 'TSPAN7', 'ADH1B','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy')\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"-mNcsm1UOmIy"},"source":["# mRMR Feature Selection Tuning\n"]},{"cell_type":"markdown","metadata":{"id":"4Llii6ZcK7pw"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Rsm2PWBgKyH-","outputId":"5a92dc84-cf77-4f90-a88c-64b0837d430a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 87.63%\n","Precision: 0.90\n","Recall: 0.88\n","F1-Score: 0.88\n","MCC: 0.77\n","Confusion Matrix:\n","[[94 23]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.80      0.89       117\n","           1       0.77      0.99      0.86        77\n","\n","    accuracy                           0.88       194\n","   macro avg       0.88      0.90      0.88       194\n","weighted avg       0.90      0.88      0.88       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 73.20%\n","Precision: 0.80\n","Recall: 0.73\n","F1-Score: 0.73\n","MCC: 0.53\n","Confusion Matrix:\n","[[71 46]\n"," [ 6 71]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.61      0.73       117\n","           1       0.61      0.92      0.73        77\n","\n","    accuracy                           0.73       194\n","   macro avg       0.76      0.76      0.73       194\n","weighted avg       0.80      0.73      0.73       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n","\n","Random Forest Testing Accuracy: 76.80%\n","Precision: 0.85\n","Recall: 0.77\n","F1-Score: 0.77\n","MCC: 0.62\n","Confusion Matrix:\n","[[73 44]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.62      0.76       117\n","           1       0.63      0.99      0.77        77\n","\n","    accuracy                           0.77       194\n","   macro avg       0.81      0.81      0.77       194\n","weighted avg       0.85      0.77      0.77       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 85.05%\n","Precision: 0.89\n","Recall: 0.85\n","F1-Score: 0.85\n","MCC: 0.73\n","Confusion Matrix:\n","[[89 28]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.76      0.86       117\n","           1       0.73      0.99      0.84        77\n","\n","    accuracy                           0.85       194\n","   macro avg       0.86      0.87      0.85       194\n","weighted avg       0.89      0.85      0.85       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n","\n","AdaBoost Testing Accuracy: 66.49%\n","Precision: 0.81\n","Recall: 0.66\n","F1-Score: 0.65\n","MCC: 0.48\n","Confusion Matrix:\n","[[53 64]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.45      0.62       117\n","           1       0.54      0.99      0.70        77\n","\n","    accuracy                           0.66       194\n","   macro avg       0.76      0.72      0.66       194\n","weighted avg       0.81      0.66      0.65       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': False, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 200}\n","\n","Bagging Testing Accuracy: 63.92%\n","Precision: 0.80\n","Recall: 0.64\n","F1-Score: 0.62\n","MCC: 0.45\n","Confusion Matrix:\n","[[48 69]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.41      0.58       117\n","           1       0.52      0.99      0.68        77\n","\n","    accuracy                           0.64       194\n","   macro avg       0.75      0.70      0.63       194\n","weighted avg       0.80      0.64      0.62       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 92.27%\n","Precision: 0.93\n","Recall: 0.92\n","F1-Score: 0.92\n","MCC: 0.85\n","Confusion Matrix:\n","[[104  13]\n"," [  2  75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.89      0.93       117\n","           1       0.85      0.97      0.91        77\n","\n","    accuracy                           0.92       194\n","   macro avg       0.92      0.93      0.92       194\n","weighted avg       0.93      0.92      0.92       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'subsample': 0.8}\n","\n","XGBoost Testing Accuracy: 72.16%\n","Precision: 0.83\n","Recall: 0.72\n","F1-Score: 0.72\n","MCC: 0.55\n","Confusion Matrix:\n","[[64 53]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.55      0.70       117\n","           1       0.59      0.99      0.74        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.79      0.77      0.72       194\n","weighted avg       0.83      0.72      0.72       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n","Random Forest: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n","Bagging: {'bootstrap': False, 'max_features': 0.75, 'max_samples': 0.75, 'n_estimators': 200}\n","Extra Trees: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'subsample': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['CDH3', 'GNG7', 'GDPD3', 'MMP1', 'ABCA8', 'VSNL1', 'ADH1B', 'AZGP1', 'LGALS2', 'TMEM100', 'DPEP1',\n","                       'AFF3', 'BRCA2', 'MS4A12', 'CENPA', 'LIFR', 'CEP72', 'ABCG2', 'ABI3BP', 'COL10A1', 'MT1M', 'MAD2L1',\n","                       'STMN2', 'NFE2L3', 'SLC4A4', 'KRT23', 'PRR7', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"fbobSA-eLGwl"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"grBfkHRHLKAY","outputId":"3e3fb1c1-37d4-4bfc-9b83-a9cf64b4d489"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(1.6681005372000592)}\n","\n","Logistic Regression Testing Accuracy: 87.63%\n","Precision: 0.90\n","Recall: 0.88\n","F1-Score: 0.88\n","MCC: 0.77\n","Confusion Matrix:\n","[[94 23]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.80      0.89       117\n","           1       0.77      0.99      0.86        77\n","\n","    accuracy                           0.88       194\n","   macro avg       0.88      0.90      0.88       194\n","weighted avg       0.90      0.88      0.88       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","\n","K-Nearest Neighbors Testing Accuracy: 79.38%\n","Precision: 0.84\n","Recall: 0.79\n","F1-Score: 0.80\n","MCC: 0.63\n","Confusion Matrix:\n","[[82 35]\n"," [ 5 72]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.70      0.80       117\n","           1       0.67      0.94      0.78        77\n","\n","    accuracy                           0.79       194\n","   macro avg       0.81      0.82      0.79       194\n","weighted avg       0.84      0.79      0.80       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 77.32%\n","Precision: 0.85\n","Recall: 0.77\n","F1-Score: 0.77\n","MCC: 0.62\n","Confusion Matrix:\n","[[74 43]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.63      0.77       117\n","           1       0.64      0.99      0.78        77\n","\n","    accuracy                           0.77       194\n","   macro avg       0.81      0.81      0.77       194\n","weighted avg       0.85      0.77      0.77       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.2}\n","\n","Gradient Boosting Testing Accuracy: 79.38%\n","Precision: 0.85\n","Recall: 0.79\n","F1-Score: 0.79\n","MCC: 0.64\n","Confusion Matrix:\n","[[79 38]\n"," [ 2 75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.68      0.80       117\n","           1       0.66      0.97      0.79        77\n","\n","    accuracy                           0.79       194\n","   macro avg       0.82      0.82      0.79       194\n","weighted avg       0.85      0.79      0.79       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 100, 'learning_rate': 0.1}\n","\n","AdaBoost Testing Accuracy: 66.49%\n","Precision: 0.81\n","Recall: 0.66\n","F1-Score: 0.65\n","MCC: 0.48\n","Confusion Matrix:\n","[[53 64]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.45      0.62       117\n","           1       0.54      0.99      0.70        77\n","\n","    accuracy                           0.66       194\n","   macro avg       0.76      0.72      0.66       194\n","weighted avg       0.81      0.66      0.65       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 100, 'max_samples': 0.75, 'max_features': 0.75, 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 64.43%\n","Precision: 0.80\n","Recall: 0.64\n","F1-Score: 0.63\n","MCC: 0.45\n","Confusion Matrix:\n","[[49 68]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.42      0.59       117\n","           1       0.53      0.99      0.69        77\n","\n","    accuracy                           0.64       194\n","   macro avg       0.75      0.70      0.64       194\n","weighted avg       0.80      0.64      0.63       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None}\n","\n","Extra Trees Testing Accuracy: 96.39%\n","Precision: 0.96\n","Recall: 0.96\n","F1-Score: 0.96\n","MCC: 0.92\n","Confusion Matrix:\n","[[113   4]\n"," [  3  74]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       117\n","           1       0.95      0.96      0.95        77\n","\n","    accuracy                           0.96       194\n","   macro avg       0.96      0.96      0.96       194\n","weighted avg       0.96      0.96      0.96       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 67.01%\n","Precision: 0.81\n","Recall: 0.67\n","F1-Score: 0.66\n","MCC: 0.49\n","Confusion Matrix:\n","[[54 63]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.46      0.63       117\n","           1       0.55      0.99      0.70        77\n","\n","    accuracy                           0.67       194\n","   macro avg       0.76      0.72      0.67       194\n","weighted avg       0.81      0.67      0.66       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(1.6681005372000592)}\n","K-Nearest Neighbors: {'weights': 'distance', 'n_neighbors': 4, 'metric': 'manhattan'}\n","Random Forest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 200, 'min_samples_split': 5, 'max_depth': 5, 'learning_rate': 0.2}\n","AdaBoost: {'n_estimators': 100, 'learning_rate': 0.1}\n","Bagging: {'n_estimators': 100, 'max_samples': 0.75, 'max_features': 0.75, 'bootstrap': False}\n","Extra Trees: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None}\n","XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = ['CDH3', 'GNG7', 'GDPD3', 'MMP1', 'ABCA8', 'VSNL1', 'ADH1B', 'AZGP1', 'LGALS2', 'TMEM100', 'DPEP1',\n","                       'AFF3', 'BRCA2', 'MS4A12', 'CENPA', 'LIFR', 'CEP72', 'ABCG2', 'ABI3BP', 'COL10A1', 'MT1M', 'MAD2L1',\n","                       'STMN2', 'NFE2L3', 'SLC4A4', 'KRT23', 'PRR7', 'target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'CatBoost': {\n","        'iterations': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'depth': [4, 6, 8],\n","        'l2_leaf_reg': [3, 5, 7]\n","    },\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy')\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix2WJa7FfAgG"},"outputs":[],"source":["# GRid Search CV for 15 Dataset"]},{"cell_type":"markdown","metadata":{"id":"ZACfMYOlj2Pr"},"source":["# Elastic Net Feature Selection Tuning"]},{"cell_type":"markdown","metadata":{"id":"p6PWrX_bMkJY"},"source":["# Grid Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVGl5Ew1MoIr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1dc6d015-c501-4106-9115-cd2bb53414f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","\n","Logistic Regression Testing Accuracy: 94.33%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.88\n","Confusion Matrix:\n","[[114   3]\n"," [  8  69]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95       117\n","           1       0.96      0.90      0.93        77\n","\n","    accuracy                           0.94       194\n","   macro avg       0.95      0.94      0.94       194\n","weighted avg       0.94      0.94      0.94       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n","\n","K-Nearest Neighbors Testing Accuracy: 93.81%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.87\n","Confusion Matrix:\n","[[109   8]\n"," [  4  73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.93      0.95       117\n","           1       0.90      0.95      0.92        77\n","\n","    accuracy                           0.94       194\n","   macro avg       0.93      0.94      0.94       194\n","weighted avg       0.94      0.94      0.94       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Random Forest Testing Accuracy: 72.16%\n","Precision: 0.83\n","Recall: 0.72\n","F1-Score: 0.72\n","MCC: 0.55\n","Confusion Matrix:\n","[[64 53]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.55      0.70       117\n","           1       0.59      0.99      0.74        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.79      0.77      0.72       194\n","weighted avg       0.83      0.72      0.72       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","\n","Gradient Boosting Testing Accuracy: 63.40%\n","Precision: 0.80\n","Recall: 0.63\n","F1-Score: 0.61\n","MCC: 0.44\n","Confusion Matrix:\n","[[47 70]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.40      0.57       117\n","           1       0.52      0.99      0.68        77\n","\n","    accuracy                           0.63       194\n","   macro avg       0.75      0.69      0.63       194\n","weighted avg       0.80      0.63      0.61       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 50}\n","\n","AdaBoost Testing Accuracy: 67.53%\n","Precision: 0.80\n","Recall: 0.68\n","F1-Score: 0.67\n","MCC: 0.48\n","Confusion Matrix:\n","[[56 61]\n"," [ 2 75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.48      0.64       117\n","           1       0.55      0.97      0.70        77\n","\n","    accuracy                           0.68       194\n","   macro avg       0.76      0.73      0.67       194\n","weighted avg       0.80      0.68      0.67       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n","\n","Bagging Testing Accuracy: 65.98%\n","Precision: 0.81\n","Recall: 0.66\n","F1-Score: 0.65\n","MCC: 0.47\n","Confusion Matrix:\n","[[52 65]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.44      0.61       117\n","           1       0.54      0.99      0.70        77\n","\n","    accuracy                           0.66       194\n","   macro avg       0.76      0.72      0.65       194\n","weighted avg       0.81      0.66      0.65       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n","\n","Extra Trees Testing Accuracy: 74.23%\n","Precision: 0.84\n","Recall: 0.74\n","F1-Score: 0.74\n","MCC: 0.58\n","Confusion Matrix:\n","[[68 49]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.58      0.73       117\n","           1       0.61      0.99      0.75        77\n","\n","    accuracy                           0.74       194\n","   macro avg       0.80      0.78      0.74       194\n","weighted avg       0.84      0.74      0.74       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n","\n","XGBoost Testing Accuracy: 62.37%\n","Precision: 0.79\n","Recall: 0.62\n","F1-Score: 0.60\n","MCC: 0.43\n","Confusion Matrix:\n","[[45 72]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.38      0.55       117\n","           1       0.51      0.99      0.68        77\n","\n","    accuracy                           0.62       194\n","   macro avg       0.75      0.69      0.61       194\n","weighted avg       0.79      0.62      0.60       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n","K-Nearest Neighbors: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n","Random Forest: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n","Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 0.8}\n","AdaBoost: {'learning_rate': 1, 'n_estimators': 50}\n","Bagging: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n","Extra Trees: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n","XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier, GradientBoostingClassifier,\n","    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",")\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = [ 'NEBL.1', 'CDH3', 'LGI1', 'BRCA2', 'PTGDS', 'ASPA', 'ABCG2', 'SST', 'TNFRSF11B', 'SLC4A4', 'TAF1A', 'SPINK2', 'PTGDR', 'AFF3', 'DHRS9', 'TG', 'MMP12', 'ADH1B', 'IL23A', 'ACSL6', 'TSPAN7', 'MMP1', 'PDE9A', 'SPINK5', 'CDKN2A','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter grid for each model\n","param_grids = {\n","    'Logistic Regression': {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2', 'elasticnet'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform GridSearchCV\n","    grid_search = GridSearchCV(model, param_grids[name], cv=10, n_jobs=-1, scoring='accuracy')\n","    grid_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = grid_search.best_params_\n","    print(f\"Best parameters for {name}: {grid_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = grid_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]},{"cell_type":"markdown","metadata":{"id":"N0FQoca8MxEl"},"source":["# Random Search CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGHnKrB2L5A5","outputId":"beeef356-212e-414f-ee88-c2195b3b2514"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"]}],"source":["!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewqHYkoBMzyj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"474af6df-a080-465b-b4be-0d3e8cbeabc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tuning Logistic Regression...\n","Best parameters for Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(0.01)}\n","\n","Logistic Regression Testing Accuracy: 94.33%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.88\n","Confusion Matrix:\n","[[114   3]\n"," [  8  69]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95       117\n","           1       0.96      0.90      0.93        77\n","\n","    accuracy                           0.94       194\n","   macro avg       0.95      0.94      0.94       194\n","weighted avg       0.94      0.94      0.94       194\n","\n","------------------------------------------------------------\n","Tuning K-Nearest Neighbors...\n","Best parameters for K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 5, 'metric': 'euclidean'}\n","\n","K-Nearest Neighbors Testing Accuracy: 93.81%\n","Precision: 0.94\n","Recall: 0.94\n","F1-Score: 0.94\n","MCC: 0.87\n","Confusion Matrix:\n","[[109   8]\n"," [  4  73]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.93      0.95       117\n","           1       0.90      0.95      0.92        77\n","\n","    accuracy                           0.94       194\n","   macro avg       0.93      0.94      0.94       194\n","weighted avg       0.94      0.94      0.94       194\n","\n","------------------------------------------------------------\n","Tuning Random Forest...\n","Best parameters for Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n","\n","Random Forest Testing Accuracy: 72.16%\n","Precision: 0.83\n","Recall: 0.72\n","F1-Score: 0.72\n","MCC: 0.55\n","Confusion Matrix:\n","[[64 53]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.55      0.70       117\n","           1       0.59      0.99      0.74        77\n","\n","    accuracy                           0.72       194\n","   macro avg       0.79      0.77      0.72       194\n","weighted avg       0.83      0.72      0.72       194\n","\n","------------------------------------------------------------\n","Tuning Gradient Boosting...\n","Best parameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n","\n","Gradient Boosting Testing Accuracy: 68.56%\n","Precision: 0.81\n","Recall: 0.69\n","F1-Score: 0.68\n","MCC: 0.51\n","Confusion Matrix:\n","[[57 60]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.49      0.65       117\n","           1       0.56      0.99      0.71        77\n","\n","    accuracy                           0.69       194\n","   macro avg       0.77      0.74      0.68       194\n","weighted avg       0.81      0.69      0.68       194\n","\n","------------------------------------------------------------\n","Tuning AdaBoost...\n","Best parameters for AdaBoost: {'n_estimators': 50, 'learning_rate': 1}\n","\n","AdaBoost Testing Accuracy: 67.53%\n","Precision: 0.80\n","Recall: 0.68\n","F1-Score: 0.67\n","MCC: 0.48\n","Confusion Matrix:\n","[[56 61]\n"," [ 2 75]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.97      0.48      0.64       117\n","           1       0.55      0.97      0.70        77\n","\n","    accuracy                           0.68       194\n","   macro avg       0.76      0.73      0.67       194\n","weighted avg       0.80      0.68      0.67       194\n","\n","------------------------------------------------------------\n","Tuning Bagging...\n","Best parameters for Bagging: {'n_estimators': 200, 'max_samples': 0.5, 'max_features': 0.5, 'bootstrap': False}\n","\n","Bagging Testing Accuracy: 75.26%\n","Precision: 0.84\n","Recall: 0.75\n","F1-Score: 0.75\n","MCC: 0.59\n","Confusion Matrix:\n","[[70 47]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.60      0.74       117\n","           1       0.62      0.99      0.76        77\n","\n","    accuracy                           0.75       194\n","   macro avg       0.80      0.79      0.75       194\n","weighted avg       0.84      0.75      0.75       194\n","\n","------------------------------------------------------------\n","Tuning Extra Trees...\n","Best parameters for Extra Trees: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","\n","Extra Trees Testing Accuracy: 68.56%\n","Precision: 0.82\n","Recall: 0.69\n","F1-Score: 0.67\n","MCC: 0.52\n","Confusion Matrix:\n","[[56 61]\n"," [ 0 77]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.48      0.65       117\n","           1       0.56      1.00      0.72        77\n","\n","    accuracy                           0.69       194\n","   macro avg       0.78      0.74      0.68       194\n","weighted avg       0.82      0.69      0.67       194\n","\n","------------------------------------------------------------\n","Tuning XGBoost...\n","Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n","\n","XGBoost Testing Accuracy: 62.37%\n","Precision: 0.79\n","Recall: 0.62\n","F1-Score: 0.60\n","MCC: 0.43\n","Confusion Matrix:\n","[[45 72]\n"," [ 1 76]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.38      0.55       117\n","           1       0.51      0.99      0.68        77\n","\n","    accuracy                           0.62       194\n","   macro avg       0.75      0.69      0.61       194\n","weighted avg       0.79      0.62      0.60       194\n","\n","------------------------------------------------------------\n","\n","Final Best Parameters for All Models:\n","Logistic Regression: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'C': np.float64(0.01)}\n","K-Nearest Neighbors: {'weights': 'uniform', 'n_neighbors': 5, 'metric': 'euclidean'}\n","Random Forest: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n","Gradient Boosting: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 5, 'learning_rate': 0.1}\n","AdaBoost: {'n_estimators': 50, 'learning_rate': 1}\n","Bagging: {'n_estimators': 200, 'max_samples': 0.5, 'max_features': 0.5, 'bootstrap': False}\n","Extra Trees: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n","XGBoost: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n","from xgboost import XGBClassifier\n","\n","\n","\n","from xgboost import XGBClassifier\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# Load datasets\n","df_train = pd.read_csv(\"ML_DEG_dataset.csv\")\n","columns_of_interest = [ 'NEBL.1', 'CDH3', 'LGI1', 'BRCA2', 'PTGDS', 'ASPA', 'ABCG2', 'SST', 'TNFRSF11B', 'SLC4A4', 'TAF1A', 'SPINK2', 'PTGDR', 'AFF3', 'DHRS9', 'TG', 'MMP12', 'ADH1B', 'IL23A', 'ACSL6', 'TSPAN7', 'MMP1', 'PDE9A', 'SPINK5', 'CDKN2A','target']\n","df_train = df_train[columns_of_interest]\n","#df_train = df_train.iloc[:, 1:]\n","df_train = df_train[[col for col in df_train.columns if col != 'target'] + ['target']]\n","X_train = df_train.iloc[:, 0:-1]\n","y_train = df_train.iloc[:, -1]\n","\n","df_test = pd.read_csv(\"GSE106582_gene_expression_renamed.csv\")\n","df_test = df_test.iloc[:, 1:]\n","df_test = df_test[[col for col in df_test.columns if col != 'target'] + ['target']]\n","X_test = df_test.iloc[:, 0:-1]\n","y_test = df_test.iloc[:, -1]\n","\n","# Ensure common features in both train and test sets\n","common_cols = set(X_train.columns).intersection(X_test.columns)\n","X_train_common = X_train[list(common_cols)]\n","X_test_common = X_test[list(common_cols)]\n","\n","# Hyperparameter distributions for each model\n","param_distributions = {\n","    'Logistic Regression': {\n","        'C': np.logspace(-2, 2, 10),\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['saga'],\n","        'max_iter': [500]\n","    },\n","    'K-Nearest Neighbors': {\n","        'n_neighbors': range(3, 10),\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","    'Random Forest': {\n","        'n_estimators': range(100, 301, 100),\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'min_samples_split': [2, 5]\n","    },\n","    'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 1]\n","    },\n","    'Bagging': {\n","        'n_estimators': [50, 100, 200],\n","        'max_samples': [0.5, 0.75, 1.0],\n","        'max_features': [0.5, 0.75, 1.0],\n","        'bootstrap': [True, False]\n","    },\n","    'Extra Trees': {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [None, 10, 20],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'max_features': ['sqrt', 'log2']\n","    },\n","\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0]\n","    }\n","}\n","\n","# Define models\n","models = {\n","    'Logistic Regression': LogisticRegression(),\n","    'K-Nearest Neighbors': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    'Bagging': BaggingClassifier(),\n","    'Extra Trees': ExtraTreesClassifier(),\n","    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","}\n","\n","# Dictionary to store best parameters\n","best_params = {}\n","\n","# Tune and evaluate models\n","for name, model in models.items():\n","    print(f\"Tuning {name}...\")\n","\n","    # Perform RandomizedSearchCV\n","    random_search = RandomizedSearchCV(model, param_distributions[name], n_iter=20, cv=10, n_jobs=-1, scoring='accuracy')\n","    random_search.fit(X_train_common, y_train)\n","\n","    # Store best parameters\n","    best_params[name] = random_search.best_params_\n","    print(f\"Best parameters for {name}: {random_search.best_params_}\\n\")\n","\n","    # Train model with best parameters\n","    best_model = random_search.best_estimator_\n","    best_model.fit(X_train_common, y_train)\n","\n","    # Predict on test set\n","    predictions = best_model.predict(X_test_common)\n","\n","    # Compute metrics\n","    test_accuracy = accuracy_score(y_test, predictions)\n","    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)\n","    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)\n","    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)\n","    mcc = matthews_corrcoef(y_test, predictions)\n","\n","    print(f\"{name} Testing Accuracy: {test_accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1-Score: {f1:.2f}\")\n","    print(f\"MCC: {mcc:.2f}\")\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_test, predictions))\n","    print(\"Classification Report:\")\n","    print(classification_report(y_test, predictions))\n","    print(\"-\" * 60)\n","\n","print(\"\\nFinal Best Parameters for All Models:\")\n","for model, params in best_params.items():\n","    print(f\"{model}: {params}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}